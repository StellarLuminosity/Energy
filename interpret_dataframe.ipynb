{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd7e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a5649a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_ROOTS = [\n",
    "    Path(\"logs\"),\n",
    "    Path(\"trillium-logs\"),\n",
    "    Path(\"runpod2_logs\"),\n",
    "]\n",
    "ROOT_CLUSTER = {\n",
    "    \"logs\": \"killarney\",\n",
    "    \"trillium-logs\": \"trillium\",\n",
    "    \"runpod2_logs\": \"runpod\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3936c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codecarbon Helper\n",
    "\n",
    "def load_codecarbon_logs(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CodeCarbon emissions.csv from each root into a single DataFrame.\n",
    "\n",
    "    Returns columns including:\n",
    "        root, cluster, project_name, experiment_id,\n",
    "        duration, cpu_energy, gpu_energy, ram_energy, energy_consumed, emissions, ...\n",
    "    \"\"\"\n",
    "    cc_rows = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cc_dir = root / \"codecarbon\"\n",
    "        if not cc_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Prefer the main emissions.csv; ignore .bak variants here\n",
    "        cc_path = cc_dir / \"emissions.csv\"\n",
    "        if not cc_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(cc_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read CodeCarbon CSV at {cc_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"root\"] = str(root)\n",
    "        df[\"cluster\"] = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        cc_rows.append(df)\n",
    "\n",
    "    if not cc_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cc_df = pd.concat(cc_rows, ignore_index=True)\n",
    "\n",
    "    # Normalize names we use often\n",
    "    cc_df.rename(\n",
    "        columns={\n",
    "            \"energy_consumed\": \"energy_consumed_kwh\",\n",
    "            \"cpu_energy\": \"cpu_energy_kwh\",\n",
    "            \"gpu_energy\": \"gpu_energy_kwh\",\n",
    "            \"ram_energy\": \"ram_energy_kwh\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return cc_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage Metrics Normalization\n",
    "\n",
    "STAGE_DEFAULTS: Dict[str, Any] = {\n",
    "    # identity / meta\n",
    "    \"root\": None,\n",
    "    \"cluster\": None,\n",
    "    \"stage_dir\": None,\n",
    "    \"experiment_id\": None,\n",
    "    \"experiment_name\": None,\n",
    "    \"stage_id\": None,\n",
    "    \"stage_name\": None,\n",
    "    \"source\": None,  # \"summary\", \"stage_json\", \"snapshot\", \"codecarbon_only\"\n",
    "\n",
    "    # snapshot info\n",
    "    \"is_snapshot\": False,\n",
    "    \"snapshot_step\": None,\n",
    "    \"snapshot_type\": None,\n",
    "    \"snapshot_time\": None,\n",
    "\n",
    "    # config metadata\n",
    "    \"total_energy_policy\": None,\n",
    "    \"pipeline\": None,\n",
    "    \"student_size\": None,\n",
    "    \"dataset_choice\": None,\n",
    "    \"kd_temperature\": None,\n",
    "    \"kd_alpha\": None,\n",
    "    \"sft_max_new_tokens\": None,\n",
    "\n",
    "    # timing / tokens\n",
    "    \"start_time\": None,\n",
    "    \"end_time\": None,\n",
    "    \"duration_seconds\": None,\n",
    "    \"tokens_processed\": None,\n",
    "    \"tokens_per_second\": None,\n",
    "\n",
    "    # GPU metrics\n",
    "    \"gpu_energy_joules\": None,\n",
    "    \"gpu_avg_power_watts\": None,\n",
    "    \"gpu_peak_power_watts\": None,\n",
    "    \"nvml_poll_interval_ms\": None,\n",
    "\n",
    "    # CPU + total\n",
    "    \"cpu_energy_joules\": None,\n",
    "    \"total_energy_joules\": None,\n",
    "    \"total_energy_kwh\": None,\n",
    "    \"joules_per_token\": None,\n",
    "    \"kwh_total\": None,\n",
    "\n",
    "    # CodeCarbon normalized\n",
    "    \"total_codecarbon_energy_kwh\": None,\n",
    "    \"codecarbon_emissions_kg\": None,\n",
    "    \"codecarbon_cpu_energy_kwh\": None,\n",
    "    \"codecarbon_gpu_energy_kwh\": None,\n",
    "    \"codecarbon_ram_energy_kwh\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_stage_metrics_dict(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize a StageMetrics-like dict (from stage JSON or experiment_summary)\n",
    "    into the canonical keys in STAGE_DEFAULTS (no root/cluster/stage_dir/source).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # Basic identifiers\n",
    "    out[\"stage_id\"] = raw.get(\"stage_id\")\n",
    "    out[\"stage_name\"] = raw.get(\"stage_name\")\n",
    "\n",
    "    # Timing / tokens\n",
    "    out[\"start_time\"] = raw.get(\"start_time\")\n",
    "    out[\"end_time\"] = raw.get(\"end_time\")\n",
    "    out[\"duration_seconds\"] = raw.get(\"duration_seconds\")\n",
    "    out[\"tokens_processed\"] = raw.get(\"tokens_processed\")\n",
    "    out[\"tokens_per_second\"] = raw.get(\"tokens_per_second\")\n",
    "\n",
    "    # GPU\n",
    "    out[\"gpu_energy_joules\"] = raw.get(\"gpu_energy_joules\")\n",
    "    out[\"gpu_avg_power_watts\"] = raw.get(\"gpu_avg_power_watts\")\n",
    "    out[\"gpu_peak_power_watts\"] = raw.get(\"gpu_peak_power_watts\")\n",
    "    out[\"nvml_poll_interval_ms\"] = raw.get(\"nvml_poll_interval_ms\")\n",
    "\n",
    "    # CPU\n",
    "    out[\"cpu_energy_joules\"] = raw.get(\"cpu_energy_joules\")\n",
    "\n",
    "    # CodeCarbon variants:\n",
    "    # new-style: total_codecarbon_energy_kwh\n",
    "    # old-style:  codecarbon_energy_kwh\n",
    "    cc_total = raw.get(\"total_codecarbon_energy_kwh\", None)\n",
    "    if cc_total is None:\n",
    "        cc_total = raw.get(\"codecarbon_energy_kwh\", None)\n",
    "    out[\"total_codecarbon_energy_kwh\"] = cc_total\n",
    "\n",
    "    out[\"codecarbon_emissions_kg\"] = raw.get(\"codecarbon_emissions_kg\")\n",
    "    out[\"codecarbon_cpu_energy_kwh\"] = raw.get(\"codecarbon_cpu_energy_kwh\")\n",
    "    out[\"codecarbon_gpu_energy_kwh\"] = raw.get(\"codecarbon_gpu_energy_kwh\")\n",
    "    out[\"codecarbon_ram_energy_kwh\"] = raw.get(\"codecarbon_ram_energy_kwh\")\n",
    "\n",
    "    # Totals / derived\n",
    "    out[\"total_energy_joules\"] = raw.get(\"total_energy_joules\")\n",
    "    out[\"total_energy_kwh\"] = raw.get(\"total_energy_kwh\")\n",
    "    out[\"joules_per_token\"] = raw.get(\"joules_per_token\")\n",
    "    out[\"kwh_total\"] = raw.get(\"kwh_total\")\n",
    "\n",
    "    # Snapshot info (may or may not be present)\n",
    "    out[\"is_snapshot\"] = bool(raw.get(\"snapshot\", False))\n",
    "    out[\"snapshot_step\"] = raw.get(\"snapshot_step\")\n",
    "    out[\"snapshot_type\"] = raw.get(\"snapshot_type\")\n",
    "    out[\"snapshot_time\"] = raw.get(\"snapshot_time\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80eee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Metadata extraction\n",
    "\n",
    "def _infer_pipeline_and_student(exp_name: str) -> (Optional[str], Optional[str]):\n",
    "    s = exp_name.lower()\n",
    "    pipeline = None\n",
    "    if s.startswith(\"kd_\"):\n",
    "        pipeline = \"kd\"\n",
    "    elif s.startswith(\"sft_\"):\n",
    "        pipeline = \"sft\"\n",
    "    elif \"true\" in s:\n",
    "        pipeline = \"true_sft\"\n",
    "\n",
    "    student_size = None\n",
    "    if \"to_1b\" in s:\n",
    "        student_size = \"1B\"\n",
    "    elif \"to_7b\" in s:\n",
    "        student_size = \"7B\"\n",
    "    elif \"to_13b\" in s or \"13b\" in s:\n",
    "        student_size = \"13B\"\n",
    "\n",
    "    return pipeline, student_size\n",
    "\n",
    "\n",
    "def load_config_meta(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan all config_*.json files and extract per-(root, stage_dir, stage_name) metadata:\n",
    "        experiment_name, total_energy_policy, pipeline, student_size, kd_temperature, kd_alpha,\n",
    "        sft_max_new_tokens, dataset_choice, etc.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        for cfg_path in root.rglob(\"config_*.json\"):\n",
    "            try:\n",
    "                with open(cfg_path) as f:\n",
    "                    cfg = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read config at {cfg_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            stage_name = cfg.get(\"stage_name\")\n",
    "            stage_id = cfg.get(\"stage_id\")\n",
    "\n",
    "            config = cfg.get(\"config\", {})\n",
    "            exp_cfg = config.get(\"experiment\", {})\n",
    "            data_cfg = config.get(\"data\", {})\n",
    "            train_cfg = config.get(\"training\", {})\n",
    "            kd_cfg = config.get(\"kd\", config.get(\"distillation\", {}))  # handle naming\n",
    "            energy_cfg = config.get(\"energy\", {})\n",
    "\n",
    "            exp_name = exp_cfg.get(\"name\", stage_name)\n",
    "            pipeline, student_size = _infer_pipeline_and_student(exp_name)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"root\": str(root),\n",
    "                    \"cluster\": cluster,\n",
    "                    \"stage_dir\": str(cfg_path.parent),\n",
    "                    \"stage_name\": stage_name,\n",
    "                    \"stage_id\": stage_id,\n",
    "                    \"experiment_name\": exp_name,\n",
    "                    \"total_energy_policy\": energy_cfg.get(\"total_energy_policy\"),\n",
    "                    \"pipeline\": pipeline,\n",
    "                    \"student_size\": student_size,\n",
    "                    \"dataset_choice\": data_cfg.get(\"dataset_choice\"),\n",
    "                    \"kd_temperature\": kd_cfg.get(\"temperature\"),\n",
    "                    \"kd_alpha\": kd_cfg.get(\"alpha\"),\n",
    "                    \"sft_max_new_tokens\": train_cfg.get(\"max_new_tokens\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage folder -> records\n",
    "\n",
    "def _collect_from_experiment_summary(\n",
    "    summary_path: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Given an experiment_summary.json, return a list of normalized stage records (source='summary').\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    exp_id = summary.get(\"experiment_id\")\n",
    "    exp_name = summary.get(\"experiment_name\")\n",
    "    stages = summary.get(\"stages\", {})\n",
    "\n",
    "    for stage_name, raw in stages.items():\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        base[\"stage_dir\"] = str(summary_path.parent)\n",
    "        base[\"experiment_id\"] = exp_id\n",
    "        base[\"experiment_name\"] = exp_name\n",
    "        base[\"source\"] = \"summary\"\n",
    "\n",
    "        # Normalize metrics\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # Attach config meta if available (match by stage_dir + stage_name)\n",
    "        m = cfg_meta[\n",
    "            (cfg_meta[\"root\"] == str(root))\n",
    "            & (cfg_meta[\"stage_dir\"] == str(summary_path.parent))\n",
    "            & (cfg_meta[\"stage_name\"] == stage_name)\n",
    "        ]\n",
    "        if not m.empty:\n",
    "            meta_row = m.iloc[0].to_dict()\n",
    "            for k in [\n",
    "                \"total_energy_policy\",\n",
    "                \"pipeline\",\n",
    "                \"student_size\",\n",
    "                \"dataset_choice\",\n",
    "                \"kd_temperature\",\n",
    "                \"kd_alpha\",\n",
    "                \"sft_max_new_tokens\",\n",
    "            ]:\n",
    "                base[k] = meta_row.get(k)\n",
    "\n",
    "        records.append(base)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def _is_stage_metrics_json(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: JSON files that look like StageMetrics but are not config/env/summary.\n",
    "    Includes snapshots.\n",
    "    \"\"\"\n",
    "    name = path.name\n",
    "    if not name.endswith(\".json\"):\n",
    "        return False\n",
    "    if name.startswith(\"config_\") or name.startswith(\"environment_\"):\n",
    "        return False\n",
    "    if name == \"experiment_summary.json\":\n",
    "        return False\n",
    "    # This will match stage.json and stage__step_*.json (snapshots)\n",
    "    return True\n",
    "\n",
    "\n",
    "def _collect_stage_jsons_in_dir(\n",
    "    stage_dir: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect stage records from standalone stage JSON files in a stage directory\n",
    "    (finals + snapshots), using source='stage_json' or 'snapshot'.\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Try to find associated experiment_name and config meta for this dir\n",
    "    m_dir = cfg_meta[\n",
    "        (cfg_meta[\"root\"] == str(root)) & (cfg_meta[\"stage_dir\"] == str(stage_dir))\n",
    "    ]\n",
    "    cfg_row = m_dir.iloc[0].to_dict() if not m_dir.empty else {}\n",
    "\n",
    "    for path in stage_dir.glob(\"*.json\"):\n",
    "        if not _is_stage_metrics_json(path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                raw = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read stage JSON at {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        base[\"stage_dir\"] = str(stage_dir)\n",
    "        base[\"experiment_name\"] = cfg_row.get(\"experiment_name\")\n",
    "        base[\"source\"] = \"snapshot\" if raw.get(\"snapshot\") else \"stage_json\"\n",
    "\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # Config meta\n",
    "        for k in [\n",
    "            \"total_energy_policy\",\n",
    "            \"pipeline\",\n",
    "            \"student_size\",\n",
    "            \"dataset_choice\",\n",
    "            \"kd_temperature\",\n",
    "            \"kd_alpha\",\n",
    "            \"sft_max_new_tokens\",\n",
    "        ]:\n",
    "            base[k] = cfg_row.get(k)\n",
    "\n",
    "        records.append(base)\n",
    "\n",
    "    return records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-stage Dataframe\n",
    "\n",
    "def build_stage_dataframe(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main entry point:\n",
    "      - loads config metadata,\n",
    "      - walks all log roots,\n",
    "      - collects StageMetrics from experiment_summary.json and individual stage JSONs,\n",
    "      - returns one big DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    cfg_meta = load_config_meta(log_roots)\n",
    "    cc_df = load_codecarbon_logs(log_roots)  # not yet used as fallback, but available\n",
    "\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        # 1) experiment_summary.json files (per stage or sometimes per pipeline)\n",
    "        for summary_path in root.rglob(\"experiment_summary.json\"):\n",
    "            # Skip copies written into individual stage dirs: .../stages/<stage>/experiment_summary.json\n",
    "            parent = summary_path.parent\n",
    "            if parent.parent.name == \"stages\":\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(summary_path) as f:\n",
    "                    summary = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read experiment_summary at {summary_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if \"stages\" in summary:\n",
    "                all_records.extend(\n",
    "                    _collect_from_experiment_summary(summary_path, root, cluster, cfg_meta)\n",
    "                )\n",
    "            else:\n",
    "                # Some summaries might be in an older/global format; skip or handle specially.\n",
    "                pass\n",
    "\n",
    "        # 2) Standalone stage directories: often under root/stages/*, but also\n",
    "        #    top-level experiment dirs like trillium-logs/sft_32b_to_1b_tulu_3500\n",
    "        for stage_dir in root.rglob(\"*\"):\n",
    "            if not stage_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Heuristic: a \"stage dir\" is one that contains some StageMetrics JSON\n",
    "            has_stage_json = any(_is_stage_metrics_json(p) for p in stage_dir.glob(\"*.json\"))\n",
    "            if not has_stage_json:\n",
    "                continue\n",
    "\n",
    "            # We already handled those covered by experiment_summary.json â€“ but that's okay.\n",
    "            # The downstream logic can deduplicate or prefer summary over stage_json if needed.\n",
    "            records = _collect_stage_jsons_in_dir(stage_dir, root, cluster, cfg_meta)\n",
    "            all_records.extend(records)\n",
    "\n",
    "    if not all_records:\n",
    "        return pd.DataFrame(columns=STAGE_DEFAULTS.keys())\n",
    "\n",
    "    stage_df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Optional: deduplicate (e.g., you might want to drop stage_json records\n",
    "    # that correspond exactly to summary records). For now, keep everything\n",
    "    # and let later analysis decide which to use.\n",
    "    return stage_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_stage_dataframe(LOG_ROOTS)\n",
    "print(df.head())\n",
    "df.to_csv(\"stage_metrics_standardized.csv\", index=False)\n",
    "print(f\"Saved stage_metrics_standardized.csv with {len(df)} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
