{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdc500",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LOG_ROOTS = [\n",
    "    Path(\"off_logs\")\n",
    "]\n",
    "ROOT_CLUSTER = {\n",
    "    \"off_logs\": \"off_logs\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Codecarbon Helper\n",
    "\n",
    "def load_codecarbon_logs(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CodeCarbon emissions.csv from each root into a single DataFrame.\n",
    "\n",
    "    Returns columns including:\n",
    "        root, cluster, project_name, experiment_id,\n",
    "        duration, cpu_energy, gpu_energy, ram_energy, energy_consumed, emissions, ...\n",
    "    \"\"\"\n",
    "    cc_rows = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cc_dir = root / \"codecarbon\"\n",
    "        if not cc_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Prefer the main emissions.csv; ignore .bak variants here\n",
    "        cc_path = cc_dir / \"emissions.csv\"\n",
    "        if not cc_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(cc_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read CodeCarbon CSV at {cc_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"root\"] = str(root)\n",
    "        df[\"cluster\"] = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        cc_rows.append(df)\n",
    "\n",
    "    if not cc_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cc_df = pd.concat(cc_rows, ignore_index=True)\n",
    "\n",
    "    # Normalize names we use often\n",
    "    cc_df.rename(\n",
    "        columns={\n",
    "            \"energy_consumed\": \"energy_consumed_kwh\",\n",
    "            \"cpu_energy\": \"cpu_energy_kwh\",\n",
    "            \"gpu_energy\": \"gpu_energy_kwh\",\n",
    "            \"ram_energy\": \"ram_energy_kwh\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return cc_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c57fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stage Metrics Normalization\n",
    "\n",
    "STAGE_DEFAULTS: Dict[str, Any] = {\n",
    "    # identity / meta\n",
    "    \"root\": None,\n",
    "    \"cluster\": None,\n",
    "    \"stage_dir\": None,\n",
    "    \"experiment_id\": None,\n",
    "    \"experiment_name\": None,\n",
    "    \"stage_id\": None,\n",
    "    \"stage_name\": None,\n",
    "    \"source\": None,  # \"summary\", \"stage_json\", \"snapshot\", \"codecarbon_only\"\n",
    "\n",
    "    # snapshot info\n",
    "    \"is_snapshot\": False,\n",
    "    \"snapshot_step\": None,\n",
    "    \"snapshot_type\": None,\n",
    "    \"snapshot_time\": None,\n",
    "\n",
    "    # config metadata\n",
    "    \"total_energy_policy\": None,\n",
    "    \"pipeline\": None,\n",
    "    \"student_size\": None,\n",
    "    \"dataset_choice\": None,\n",
    "    \"kd_temperature\": None,\n",
    "    \"kd_alpha\": None,\n",
    "    \"sft_max_new_tokens\": None,\n",
    "\n",
    "    # timing / tokens\n",
    "    \"start_time\": None,\n",
    "    \"end_time\": None,\n",
    "    \"duration_seconds\": None,\n",
    "    \"tokens_processed\": None,\n",
    "    \"tokens_per_second\": None,\n",
    "\n",
    "    # GPU metrics\n",
    "    \"gpu_energy_joules\": None,\n",
    "    \"gpu_avg_power_watts\": None,\n",
    "    \"gpu_peak_power_watts\": None,\n",
    "    \"nvml_poll_interval_ms\": None,\n",
    "\n",
    "    # CPU + total\n",
    "    \"cpu_energy_joules\": None,\n",
    "    \"total_energy_joules\": None,\n",
    "    \"total_energy_kwh\": None,\n",
    "    \"joules_per_token\": None,\n",
    "    \"kwh_total\": None,\n",
    "\n",
    "    # CodeCarbon normalized\n",
    "    \"total_codecarbon_energy_kwh\": None,\n",
    "    \"codecarbon_emissions_kg\": None,\n",
    "    \"codecarbon_cpu_energy_kwh\": None,\n",
    "    \"codecarbon_gpu_energy_kwh\": None,\n",
    "    \"codecarbon_ram_energy_kwh\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_stage_metrics_dict(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize a StageMetrics-like dict (from stage JSON or experiment_summary)\n",
    "    into the canonical keys in STAGE_DEFAULTS (no root/cluster/stage_dir/source).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # Basic identifiers\n",
    "    out[\"stage_id\"] = raw.get(\"stage_id\")\n",
    "    out[\"stage_name\"] = raw.get(\"stage_name\")\n",
    "\n",
    "    # Timing / tokens\n",
    "    out[\"start_time\"] = raw.get(\"start_time\")\n",
    "    out[\"end_time\"] = raw.get(\"end_time\")\n",
    "    out[\"duration_seconds\"] = raw.get(\"duration_seconds\")\n",
    "    out[\"tokens_processed\"] = raw.get(\"tokens_processed\")\n",
    "    out[\"tokens_per_second\"] = raw.get(\"tokens_per_second\")\n",
    "\n",
    "    # GPU\n",
    "    out[\"gpu_energy_joules\"] = raw.get(\"gpu_energy_joules\")\n",
    "    out[\"gpu_avg_power_watts\"] = raw.get(\"gpu_avg_power_watts\")\n",
    "    out[\"gpu_peak_power_watts\"] = raw.get(\"gpu_peak_power_watts\")\n",
    "    out[\"nvml_poll_interval_ms\"] = raw.get(\"nvml_poll_interval_ms\")\n",
    "\n",
    "    # CPU\n",
    "    out[\"cpu_energy_joules\"] = raw.get(\"cpu_energy_joules\")\n",
    "\n",
    "    # CodeCarbon variants:\n",
    "    # new-style: total_codecarbon_energy_kwh\n",
    "    # old-style:  codecarbon_energy_kwh\n",
    "    cc_total = raw.get(\"total_codecarbon_energy_kwh\", None)\n",
    "    if cc_total is None:\n",
    "        cc_total = raw.get(\"codecarbon_energy_kwh\", None)\n",
    "    out[\"total_codecarbon_energy_kwh\"] = cc_total\n",
    "\n",
    "    out[\"codecarbon_emissions_kg\"] = raw.get(\"codecarbon_emissions_kg\")\n",
    "    out[\"codecarbon_cpu_energy_kwh\"] = raw.get(\"codecarbon_cpu_energy_kwh\")\n",
    "    out[\"codecarbon_gpu_energy_kwh\"] = raw.get(\"codecarbon_gpu_energy_kwh\")\n",
    "    out[\"codecarbon_ram_energy_kwh\"] = raw.get(\"codecarbon_ram_energy_kwh\")\n",
    "\n",
    "    # Totals / derived\n",
    "    out[\"total_energy_joules\"] = raw.get(\"total_energy_joules\")\n",
    "    out[\"total_energy_kwh\"] = raw.get(\"total_energy_kwh\")\n",
    "    out[\"joules_per_token\"] = raw.get(\"joules_per_token\")\n",
    "    out[\"kwh_total\"] = raw.get(\"kwh_total\")\n",
    "\n",
    "    # Snapshot info (may or may not be present)\n",
    "    out[\"is_snapshot\"] = bool(raw.get(\"snapshot\", False))\n",
    "    out[\"snapshot_step\"] = raw.get(\"snapshot_step\")\n",
    "    out[\"snapshot_type\"] = raw.get(\"snapshot_type\")\n",
    "    out[\"snapshot_time\"] = raw.get(\"snapshot_time\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079f514",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "# Config Metadata extraction\n",
    "\n",
    "def _infer_pipeline_and_student(exp_name: str) -> (Optional[str], Optional[str]):\n",
    "    s = exp_name.lower()\n",
    "    pipeline = None\n",
    "    if s.startswith(\"kd_\"):\n",
    "        pipeline = \"kd\"\n",
    "    elif s.startswith(\"sft_\"):\n",
    "        pipeline = \"sft\"\n",
    "    elif \"true\" in s:\n",
    "        pipeline = \"true_sft\"\n",
    "\n",
    "    student_size = None\n",
    "    if \"to_1b\" in s:\n",
    "        student_size = \"1B\"\n",
    "    elif \"to_7b\" in s:\n",
    "        student_size = \"7B\"\n",
    "    elif \"to_13b\" in s or \"13b\" in s:\n",
    "        student_size = \"13B\"\n",
    "\n",
    "    return pipeline, student_size\n",
    "\n",
    "\n",
    "def load_config_meta(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan all config_*.json files and extract per-(root, stage_dir, stage_name) metadata:\n",
    "        experiment_name, total_energy_policy, pipeline, student_size, kd_temperature, kd_alpha,\n",
    "        sft_max_new_tokens, dataset_choice, etc.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        for cfg_path in root.rglob(\"config_*.json\"):\n",
    "            try:\n",
    "                with open(cfg_path) as f:\n",
    "                    cfg = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read config at {cfg_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            stage_name = cfg.get(\"stage_name\")\n",
    "            stage_id = cfg.get(\"stage_id\")\n",
    "\n",
    "            config = cfg.get(\"config\", {})\n",
    "            exp_cfg = config.get(\"experiment\", {})\n",
    "            data_cfg = config.get(\"data\", {})\n",
    "            train_cfg = config.get(\"training\", {})\n",
    "            kd_cfg = config.get(\"kd\", config.get(\"distillation\", {}))  # handle naming\n",
    "            energy_cfg = config.get(\"energy\", {})\n",
    "\n",
    "            exp_name = exp_cfg.get(\"name\", stage_name)\n",
    "            pipeline, student_size = _infer_pipeline_and_student(exp_name)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"root\": str(root),\n",
    "                    \"cluster\": cluster,\n",
    "                    \"stage_dir\": str(cfg_path.parent),\n",
    "                    \"stage_name\": stage_name,\n",
    "                    \"stage_id\": stage_id,\n",
    "                    \"experiment_name\": exp_name,\n",
    "                    \"total_energy_policy\": energy_cfg.get(\"total_energy_policy\"),\n",
    "                    \"pipeline\": pipeline,\n",
    "                    \"student_size\": student_size,\n",
    "                    \"dataset_choice\": data_cfg.get(\"dataset_choice\"),\n",
    "                    \"kd_temperature\": kd_cfg.get(\"temperature\"),\n",
    "                    \"kd_alpha\": kd_cfg.get(\"alpha\"),\n",
    "                    \"sft_max_new_tokens\": train_cfg.get(\"max_new_tokens\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf84520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stage folder -> records\n",
    "\n",
    "def _collect_from_experiment_summary(\n",
    "    summary_path: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Given an experiment_summary.json, return a list of normalized stage records (source='summary').\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    exp_id = summary.get(\"experiment_id\")\n",
    "    exp_name = summary.get(\"experiment_name\")\n",
    "    stages = summary.get(\"stages\", {})\n",
    "\n",
    "    for stage_name, raw in stages.items():\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        # Default: parent of the summary (e.g., run_dir); overridden if config meta is found\n",
    "        base[\"stage_dir\"] = str(summary_path.parent)\n",
    "        base[\"experiment_id\"] = exp_id\n",
    "        base[\"experiment_name\"] = exp_name\n",
    "        base[\"source\"] = \"summary\"\n",
    "\n",
    "        # Normalize metrics\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # Attach config meta if available.\n",
    "        # Match by root + stage_name, then prefer the config's stage_dir.\n",
    "        m = cfg_meta[\n",
    "            (cfg_meta[\"root\"] == str(root))\n",
    "            & (cfg_meta[\"stage_name\"] == stage_name)\n",
    "        ]\n",
    "        if not m.empty:\n",
    "            meta_row = m.iloc[0].to_dict()\n",
    "\n",
    "            # Prefer the config's notion of the stage_dir (actual stage folder)\n",
    "            stage_dir_cfg = meta_row.get(\"stage_dir\")\n",
    "            if stage_dir_cfg:\n",
    "                base[\"stage_dir\"] = stage_dir_cfg\n",
    "\n",
    "            # Optionally override stage_id if missing\n",
    "            if base.get(\"stage_id\") is None and meta_row.get(\"stage_id\"):\n",
    "                base[\"stage_id\"] = meta_row[\"stage_id\"]\n",
    "\n",
    "            for k in [\n",
    "                \"total_energy_policy\",\n",
    "                \"pipeline\",\n",
    "                \"student_size\",\n",
    "                \"dataset_choice\",\n",
    "                \"kd_temperature\",\n",
    "                \"kd_alpha\",\n",
    "                \"sft_max_new_tokens\",\n",
    "            ]:\n",
    "                base[k] = meta_row.get(k)\n",
    "\n",
    "        records.append(base)\n",
    "\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def _is_stage_metrics_json(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: JSON files that look like StageMetrics but are not config/env/summary.\n",
    "    Includes snapshots.\n",
    "    \"\"\"\n",
    "    name = path.name\n",
    "    if not name.endswith(\".json\"):\n",
    "        return False\n",
    "    if name.startswith(\"config_\") or name.startswith(\"environment_\"):\n",
    "        return False\n",
    "    if name == \"experiment_summary.json\":\n",
    "        return False\n",
    "    # This will match stage.json and stage__step_*.json (snapshots)\n",
    "    return True\n",
    "\n",
    "\n",
    "def _collect_stage_jsons_in_dir(\n",
    "    stage_dir: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect stage records from standalone stage JSON files in a stage directory,\n",
    "    aggregate snapshots, and return ONE row per logical stage.\n",
    "\n",
    "    Rules:\n",
    "      - If a final stage JSON exists (source='stage_json', not a snapshot),\n",
    "        use that as the base row.\n",
    "      - If only snapshots exist, pick the latest snapshot (by snapshot_step, then end_time).\n",
    "      - For stages with both final and snapshots, final wins; we can still\n",
    "        use the last snapshot to fill missing fields if needed.\n",
    "    \"\"\"\n",
    "    # Match config for this directory (pipeline, student_size, etc.)\n",
    "    m_dir = cfg_meta[\n",
    "        (cfg_meta[\"root\"] == str(root)) & (cfg_meta[\"stage_dir\"] == str(stage_dir))\n",
    "    ]\n",
    "    cfg_row = m_dir.iloc[0].to_dict() if not m_dir.empty else {}\n",
    "\n",
    "    stage_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for path in stage_dir.glob(\"*.json\"):\n",
    "        if not _is_stage_metrics_json(path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                raw = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read stage JSON at {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Skip JSONs that aren't dicts (or single-element list of dict)\n",
    "        if isinstance(raw, list):\n",
    "            if len(raw) == 1 and isinstance(raw[0], dict):\n",
    "                raw = raw[0]\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[INFO] Skipping JSON at {path} \"\n",
    "                    f\"(top-level list, not a StageMetrics dict)\"\n",
    "                )\n",
    "                continue\n",
    "        elif not isinstance(raw, dict):\n",
    "            print(\n",
    "                f\"[INFO] Skipping JSON at {path} \"\n",
    "                f\"(top-level {type(raw).__name__}, expected dict)\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        base[\"stage_dir\"] = str(stage_dir)\n",
    "        base[\"experiment_name\"] = cfg_row.get(\"experiment_name\")\n",
    "        base[\"source\"] = \"snapshot\" if raw.get(\"snapshot\") else \"stage_json\"\n",
    "\n",
    "        # Normalize StageMetrics-style dict into our standard fields\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # If JSON didn't carry stage_name, fall back to folder name\n",
    "        if not base.get(\"stage_name\"):\n",
    "            base[\"stage_name\"] = stage_dir.name\n",
    "\n",
    "        # Attach config meta\n",
    "        for k in [\n",
    "            \"total_energy_policy\",\n",
    "            \"pipeline\",\n",
    "            \"student_size\",\n",
    "            \"dataset_choice\",\n",
    "            \"kd_temperature\",\n",
    "            \"kd_alpha\",\n",
    "            \"sft_max_new_tokens\",\n",
    "        ]:\n",
    "            base[k] = cfg_row.get(k)\n",
    "\n",
    "        stage_records.append(base)\n",
    "\n",
    "    if not stage_records:\n",
    "        return []\n",
    "\n",
    "    # --- Aggregate to ONE row per logical stage in this directory ---\n",
    "\n",
    "    by_stage: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for rec in stage_records:\n",
    "        key = rec.get(\"stage_id\") or rec[\"stage_name\"]\n",
    "        by_stage.setdefault(key, []).append(rec)\n",
    "\n",
    "    aggregated: List[Dict[str, Any]] = []\n",
    "\n",
    "    for key, recs in by_stage.items():\n",
    "        finals = [\n",
    "            r\n",
    "            for r in recs\n",
    "            if r.get(\"source\") != \"snapshot\" and not r.get(\"is_snapshot\", False)\n",
    "        ]\n",
    "        snapshots = [r for r in recs if r.get(\"source\") == \"snapshot\"]\n",
    "\n",
    "        if finals:\n",
    "            # Prefer the final metrics JSON; if multiple, take the one with the latest end_time.\n",
    "            best = max(finals, key=lambda r: (r.get(\"end_time\") or 0.0))\n",
    "\n",
    "            # Optional: use the latest snapshot as a fallback for missing fields.\n",
    "            if snapshots:\n",
    "                snaps_sorted = sorted(\n",
    "                    snapshots,\n",
    "                    key=lambda r: (\n",
    "                        r.get(\"snapshot_step\") if r.get(\"snapshot_step\") is not None else -1,\n",
    "                        r.get(\"end_time\") or 0.0,\n",
    "                    ),\n",
    "                )\n",
    "                last_snap = snaps_sorted[-1]\n",
    "                for field in STAGE_DEFAULTS.keys():\n",
    "                    if best.get(field) in (None, 0) and last_snap.get(field) not in (None, 0):\n",
    "                        best[field] = last_snap[field]\n",
    "\n",
    "            aggregated.append(best)\n",
    "        else:\n",
    "            # No final file: only snapshots. Pick the latest snapshot as the representative row.\n",
    "            snaps_sorted = sorted(\n",
    "                recs,\n",
    "                key=lambda r: (\n",
    "                    r.get(\"snapshot_step\") if r.get(\"snapshot_step\") is not None else -1,\n",
    "                    r.get(\"end_time\") or 0.0,\n",
    "                ),\n",
    "            )\n",
    "            aggregated.append(snaps_sorted[-1])\n",
    "\n",
    "    return aggregated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bb72f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Full-stage Dataframe\n",
    "\n",
    "def build_stage_dataframe(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main entry point:\n",
    "      - loads config metadata,\n",
    "      - walks all log roots,\n",
    "      - collects StageMetrics from experiment_summary.json and individual stage JSONs,\n",
    "      - returns one big DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    cfg_meta = load_config_meta(log_roots)\n",
    "    cc_df = load_codecarbon_logs(log_roots)  # not yet used as fallback, but available\n",
    "\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        # 1) experiment_summary.json files (per run)\n",
    "        for summary_path in root.rglob(\"experiment_summary.json\"):\n",
    "            # Skip copies written into individual stage dirs:\n",
    "            # .../<root>/stages/<stage>/experiment_summary.json\n",
    "            parent = summary_path.parent\n",
    "            if parent.parent.name == \"stages\":\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(summary_path) as f:\n",
    "                    summary = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read experiment_summary at {summary_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if \"stages\" in summary:\n",
    "                all_records.extend(\n",
    "                    _collect_from_experiment_summary(summary_path, root, cluster, cfg_meta)\n",
    "                )\n",
    "            else:\n",
    "                # Some summaries might be in an older/global format; skip or handle specially.\n",
    "                pass\n",
    "\n",
    "\n",
    "        # 2) Standalone stage directories: often under root/stages/*, but also\n",
    "        for stage_dir in root.rglob(\"*\"):\n",
    "            if not stage_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Skip the container folder itself (we only want its children)\n",
    "            if stage_dir == root / \"stages\":\n",
    "                continue\n",
    "\n",
    "            # Heuristic: a \"stage dir\" is one that contains some StageMetrics JSON\n",
    "            has_stage_json = any(_is_stage_metrics_json(p) for p in stage_dir.glob(\"*.json\"))\n",
    "            if not has_stage_json:\n",
    "                continue\n",
    "\n",
    "            records = _collect_stage_jsons_in_dir(stage_dir, root, cluster, cfg_meta)\n",
    "            all_records.extend(records)\n",
    "\n",
    "\n",
    "    if not all_records:\n",
    "        return pd.DataFrame(columns=STAGE_DEFAULTS.keys())\n",
    "\n",
    "    stage_df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Optional: deduplicate (e.g., you might want to drop stage_json records\n",
    "    # that correspond exactly to summary records). For now, keep everything\n",
    "    # and let later analysis decide which to use.\n",
    "    return stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64849248",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_stage_dataframe_for_path(path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience helper to build a standardized stage DataFrame for a specific\n",
    "    log root or stage directory.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs\")\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    - build_stage_dataframe_for_path(\"/abs/path/to/runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    \"\"\"\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # If they passed a specific stage dir under .../stages/<stage_name>\n",
    "    if path.is_dir() and path.name != \"stages\" and path.parent.name == \"stages\":\n",
    "        # /.../<log_root>/stages/<stage_name>\n",
    "        # For /project/.../Energy/runpod2_logs/stages/sft_32b_to_1b_math_nosft\n",
    "        # we want log_root = /project/.../Energy/runpod2_logs\n",
    "        log_root = path.parent.parent  # == path.parents[1]\n",
    "        filter_prefix = str(path)\n",
    "    elif path.is_dir() and path.name == \"stages\":\n",
    "        # They pointed at the stages/ directory: restrict to that subtree\n",
    "        log_root = path.parent\n",
    "        filter_prefix = str(path)\n",
    "    else:\n",
    "        # Treat as a log root\n",
    "        log_root = path\n",
    "        filter_prefix = str(log_root)\n",
    "\n",
    "    df = build_stage_dataframe([log_root])\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # If they gave a root, no extra filtering\n",
    "    if filter_prefix == str(log_root):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    # Otherwise restrict to that specific stage subtree\n",
    "    stage_dirs = df[\"stage_dir\"].astype(str)\n",
    "    mask = stage_dirs.str.startswith(filter_prefix)\n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Choose what you want to process\n",
    "# - Leave `paths` empty to use default LOG_ROOTS\n",
    "# - Or set it to one or more specific paths, e.g. a single stage dir\n",
    "paths = [\n",
    "    Path(\"off_logs\"), \n",
    "]\n",
    "output = \"stage_metrics.csv\"\n",
    "\n",
    "if paths:\n",
    "    dfs = [build_stage_dataframe_for_path(p) for p in paths]\n",
    "    df = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]\n",
    "else:\n",
    "    df = build_stage_dataframe(LOG_ROOTS)\n",
    "\n",
    "output_path = Path(output)\n",
    "file_exists = output_path.exists()\n",
    "\n",
    "display(df.head())\n",
    "df.to_csv(\n",
    "    output_path,\n",
    "    mode=\"a\" if file_exists else \"w\",   # append if exists, else write\n",
    "    header=not file_exists,            # write header only if new file\n",
    "    index=False,\n",
    ")\n",
    "print(f\"Saved {output} with {len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473ae99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Safe display() for both notebooks and plain python execution\n",
    "try:\n",
    "    from IPython.display import display  # type: ignore\n",
    "except Exception:  # pragma: no cover\n",
    "    def display(x):\n",
    "        print(x)\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the aggregated stage metrics CSV\n",
    "if \"output_path\" in globals():\n",
    "    stage_metrics_path = output_path\n",
    "else:\n",
    "    stage_metrics_path = Path(\"stage_metrics.csv\")\n",
    "\n",
    "stage_df_raw = pd.read_csv(stage_metrics_path)\n",
    "print(f\"\\n=== Loaded aggregated stage metrics ===\")\n",
    "print(f\"Path: {stage_metrics_path}\")\n",
    "print(f\"Rows: {len(stage_df_raw)}\")\n",
    "print(f\"Columns: {len(stage_df_raw.columns)}\")\n",
    "print(\"First few columns:\", list(stage_df_raw.columns[:10]))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Ensure numeric dtypes for core metric columns\n",
    "# -------------------------------------------------------------------------\n",
    "numeric_cols = [\n",
    "    \"duration_seconds\",\n",
    "    \"tokens_processed\",\n",
    "    \"tokens_per_second\",\n",
    "    \"gpu_energy_joules\",\n",
    "    \"gpu_avg_power_watts\",\n",
    "    \"gpu_peak_power_watts\",\n",
    "    \"nvml_poll_interval_ms\",\n",
    "    \"cpu_energy_joules\",\n",
    "    \"total_energy_joules\",\n",
    "    \"total_energy_kwh\",\n",
    "    \"joules_per_token\",\n",
    "    \"kwh_total\",\n",
    "    \"total_codecarbon_energy_kwh\",\n",
    "    \"codecarbon_emissions_kg\",\n",
    "    \"codecarbon_cpu_energy_kwh\",\n",
    "    \"codecarbon_gpu_energy_kwh\",\n",
    "    \"codecarbon_ram_energy_kwh\",\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in stage_df_raw.columns:\n",
    "        stage_df_raw[col] = pd.to_numeric(stage_df_raw[col], errors=\"coerce\")\n",
    "\n",
    "stage_df_all = stage_df_raw.copy()\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Ensure we have a stage_name column (fallback from stage_dir if needed)\n",
    "# -------------------------------------------------------------------------\n",
    "if \"stage_name\" not in stage_df_all.columns:\n",
    "    if \"stage_dir\" in stage_df_all.columns:\n",
    "        # Use the last path component of stage_dir as a synthetic stage_name\n",
    "        stage_df_all[\"stage_name\"] = (\n",
    "            stage_df_all[\"stage_dir\"]\n",
    "            .astype(str)\n",
    "            .apply(lambda p: Path(p).name)\n",
    "        )\n",
    "        print(\"[INFO] stage_name column was missing; synthesized from stage_dir.\")\n",
    "    else:\n",
    "        # Absolute fallback: create an all-NaN column\n",
    "        stage_df_all[\"stage_name\"] = np.nan\n",
    "        print(\"[WARN] stage_name and stage_dir both missing; stage_name set to NaN.\")\n",
    "\n",
    "print(\"\\n=== Non-null counts for numeric columns ===\")\n",
    "existing_numeric_cols = [c for c in numeric_cols if c in stage_df_all.columns]\n",
    "if existing_numeric_cols:\n",
    "    print(stage_df_all[existing_numeric_cols].notna().sum().sort_values())\n",
    "else:\n",
    "    print(\"No numeric_cols found in stage_df_all.columns – skipping numeric summary.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Canonical energy / throughput columns\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Best-effort total energy in kWh\n",
    "energy_kwh = None\n",
    "energy_sources_used = []\n",
    "\n",
    "if \"total_energy_kwh\" in stage_df_all.columns:\n",
    "    energy_kwh = stage_df_all[\"total_energy_kwh\"].copy()\n",
    "    energy_sources_used.append(\"total_energy_kwh\")\n",
    "\n",
    "if \"total_codecarbon_energy_kwh\" in stage_df_all.columns:\n",
    "    if energy_kwh is None:\n",
    "        energy_kwh = stage_df_all[\"total_codecarbon_energy_kwh\"].copy()\n",
    "    else:\n",
    "        missing_mask = energy_kwh.isna() & stage_df_all[\"total_codecarbon_energy_kwh\"].notna()\n",
    "        energy_kwh = energy_kwh.fillna(stage_df_all[\"total_codecarbon_energy_kwh\"])\n",
    "        if missing_mask.any():\n",
    "            print(\n",
    "                f\"[INFO] Filled energy_kwh from total_codecarbon_energy_kwh \"\n",
    "                f\"for {missing_mask.sum()} rows\"\n",
    "            )\n",
    "    energy_sources_used.append(\"total_codecarbon_energy_kwh\")\n",
    "\n",
    "if \"kwh_total\" in stage_df_all.columns:\n",
    "    if energy_kwh is None:\n",
    "        energy_kwh = stage_df_all[\"kwh_total\"].copy()\n",
    "    else:\n",
    "        missing_mask = energy_kwh.isna() & stage_df_all[\"kwh_total\"].notna()\n",
    "        energy_kwh = energy_kwh.fillna(stage_df_all[\"kwh_total\"])\n",
    "        if missing_mask.any():\n",
    "            print(\n",
    "                f\"[INFO] Filled energy_kwh from kwh_total \"\n",
    "                f\"for {missing_mask.sum()} rows\"\n",
    "            )\n",
    "    energy_sources_used.append(\"kwh_total\")\n",
    "\n",
    "if energy_kwh is None:\n",
    "    stage_df_all[\"energy_kwh\"] = np.nan\n",
    "    print(\"[WARN] Could not construct energy_kwh from any known columns.\")\n",
    "else:\n",
    "    stage_df_all[\"energy_kwh\"] = energy_kwh\n",
    "\n",
    "print(\"\\n=== energy_kwh summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"energy_kwh\"].notna().sum())\n",
    "print(\"Rows with NaN energy_kwh:\", stage_df_all[\"energy_kwh\"].isna().sum())\n",
    "if energy_sources_used:\n",
    "    print(\"Sources considered for energy_kwh:\", \", \".join(energy_sources_used))\n",
    "\n",
    "# GPU energy in kWh: prefer direct Joules, fall back to CodeCarbon\n",
    "stage_df_all[\"gpu_energy_kwh\"] = np.nan\n",
    "if \"gpu_energy_joules\" in stage_df_all.columns:\n",
    "    mask_gpu_j = stage_df_all[\"gpu_energy_joules\"].notna()\n",
    "    stage_df_all.loc[mask_gpu_j, \"gpu_energy_kwh\"] = (\n",
    "        stage_df_all.loc[mask_gpu_j, \"gpu_energy_joules\"] / 3.6e6\n",
    "    )\n",
    "    print(\n",
    "        f\"[INFO] Converted gpu_energy_joules -> gpu_energy_kwh \"\n",
    "        f\"for {mask_gpu_j.sum()} rows\"\n",
    "    )\n",
    "\n",
    "if \"codecarbon_gpu_energy_kwh\" in stage_df_all.columns:\n",
    "    mask_fill = stage_df_all[\"gpu_energy_kwh\"].isna() & stage_df_all[\"codecarbon_gpu_energy_kwh\"].notna()\n",
    "    stage_df_all.loc[mask_fill, \"gpu_energy_kwh\"] = stage_df_all.loc[\n",
    "        mask_fill, \"codecarbon_gpu_energy_kwh\"\n",
    "    ]\n",
    "    print(\n",
    "        f\"[INFO] Filled gpu_energy_kwh from codecarbon_gpu_energy_kwh \"\n",
    "        f\"for {mask_fill.sum()} rows\"\n",
    "    )\n",
    "\n",
    "# CPU energy in kWh: prefer direct Joules, fall back to CodeCarbon\n",
    "stage_df_all[\"cpu_energy_kwh\"] = np.nan\n",
    "if \"cpu_energy_joules\" in stage_df_all.columns:\n",
    "    mask_cpu_j = stage_df_all[\"cpu_energy_joules\"].notna()\n",
    "    stage_df_all.loc[mask_cpu_j, \"cpu_energy_kwh\"] = (\n",
    "        stage_df_all.loc[mask_cpu_j, \"cpu_energy_joules\"] / 3.6e6\n",
    "    )\n",
    "    print(\n",
    "        f\"[INFO] Converted cpu_energy_joules -> cpu_energy_kwh \"\n",
    "        f\"for {mask_cpu_j.sum()} rows\"\n",
    "    )\n",
    "\n",
    "if \"codecarbon_cpu_energy_kwh\" in stage_df_all.columns:\n",
    "    mask_fill = stage_df_all[\"cpu_energy_kwh\"].isna() & stage_df_all[\"codecarbon_cpu_energy_kwh\"].notna()\n",
    "    stage_df_all.loc[mask_fill, \"cpu_energy_kwh\"] = stage_df_all.loc[\n",
    "        mask_fill, \"codecarbon_cpu_energy_kwh\"\n",
    "    ]\n",
    "    print(\n",
    "        f\"[INFO] Filled cpu_energy_kwh from codecarbon_cpu_energy_kwh \"\n",
    "        f\"for {mask_fill.sum()} rows\"\n",
    "    )\n",
    "\n",
    "# Joules per token: prefer precomputed, else total_energy_joules / tokens_processed\n",
    "if \"joules_per_token\" in stage_df_all.columns:\n",
    "    stage_df_all[\"energy_j_per_token\"] = stage_df_all[\"joules_per_token\"]\n",
    "else:\n",
    "    stage_df_all[\"energy_j_per_token\"] = np.nan\n",
    "\n",
    "mask_need_jpt = stage_df_all[\"energy_j_per_token\"].isna()\n",
    "if \"total_energy_joules\" in stage_df_all.columns and \"tokens_processed\" in stage_df_all.columns:\n",
    "    denom = stage_df_all[\"tokens_processed\"].replace({0: np.nan})\n",
    "    jpt_mask = mask_need_jpt & stage_df_all[\"total_energy_joules\"].notna() & denom.notna()\n",
    "    stage_df_all.loc[jpt_mask, \"energy_j_per_token\"] = (\n",
    "        stage_df_all.loc[jpt_mask, \"total_energy_joules\"] / denom[jpt_mask]\n",
    "    )\n",
    "    if jpt_mask.any():\n",
    "        print(\n",
    "            f\"[INFO] Computed energy_j_per_token as total_energy_joules/tokens_processed \"\n",
    "            f\"for {jpt_mask.sum()} rows\"\n",
    "        )\n",
    "\n",
    "print(\"\\n=== energy_j_per_token summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"energy_j_per_token\"].notna().sum())\n",
    "\n",
    "# Tokens per second: prefer precomputed, else tokens_processed / duration_seconds\n",
    "if \"tokens_per_second\" in stage_df_all.columns:\n",
    "    stage_df_all[\"tokens_per_sec\"] = stage_df_all[\"tokens_per_second\"]\n",
    "else:\n",
    "    stage_df_all[\"tokens_per_sec\"] = np.nan\n",
    "\n",
    "mask_need_tps = stage_df_all[\"tokens_per_sec\"].isna()\n",
    "if \"duration_seconds\" in stage_df_all.columns and \"tokens_processed\" in stage_df_all.columns:\n",
    "    dur = stage_df_all[\"duration_seconds\"].replace({0: np.nan})\n",
    "    tps_mask = mask_need_tps & dur.notna() & stage_df_all[\"tokens_processed\"].notna()\n",
    "    stage_df_all.loc[tps_mask, \"tokens_per_sec\"] = (\n",
    "        stage_df_all.loc[tps_mask, \"tokens_processed\"] / dur[tps_mask]\n",
    "    )\n",
    "    if tps_mask.any():\n",
    "        print(\n",
    "            f\"[INFO] Computed tokens_per_sec as tokens_processed/duration_seconds \"\n",
    "            f\"for {tps_mask.sum()} rows\"\n",
    "        )\n",
    "\n",
    "print(\"\\n=== tokens_per_sec summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"tokens_per_sec\"].notna().sum())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Filtered view: drop rows that will mess up per-token metrics\n",
    "# -------------------------------------------------------------------------\n",
    "if \"tokens_processed\" in stage_df_all.columns:\n",
    "    mask_valid_tokens = (\n",
    "        stage_df_all[\"tokens_processed\"].notna()\n",
    "        & (stage_df_all[\"tokens_processed\"] > 0)\n",
    "    )\n",
    "    stage_df_clean = stage_df_all[mask_valid_tokens].copy()\n",
    "else:\n",
    "    stage_df_clean = stage_df_all.copy()\n",
    "\n",
    "print(\"\\n=== Clean vs all rows ===\")\n",
    "print(f\"stage_df_all:   {len(stage_df_all)} rows\")\n",
    "print(\n",
    "    f\"stage_df_clean: {len(stage_df_clean)} rows \"\n",
    "    \"(tokens_processed > 0 where available)\"\n",
    ")\n",
    "\n",
    "display(stage_df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e1a59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Tag stages, runs, and cells (Step 2)\n",
    "\n",
    "def _normalize_str(series: pd.Series, to_lower: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Take an arbitrary Series, convert to string, strip whitespace,\n",
    "    optionally lowercase, and turn empty strings into NaN.\n",
    "    \"\"\"\n",
    "    # Convert everything to pandas \"string\" dtype (handles None/NaN cleanly)\n",
    "    s = series.astype(\"string\")\n",
    "\n",
    "    # Strip whitespace\n",
    "    s = s.str.strip()\n",
    "\n",
    "    # Optional lowercase\n",
    "    if to_lower:\n",
    "        s = s.str.lower()\n",
    "\n",
    "    # Turn empty strings into NaN\n",
    "    s = s.replace({\"\": pd.NA})\n",
    "\n",
    "    return s\n",
    "\n",
    "# Normalize key identifier columns for easier grouping\n",
    "for col in [\"pipeline\", \"student_size\", \"dataset_choice\", \"stage_name\", \"experiment_name\", \"source\"]:\n",
    "    if col in stage_df_all.columns:\n",
    "        # pipeline & dataset_choice we want consistently lowercased\n",
    "        to_lower = col in [\"pipeline\", \"dataset_choice\", \"stage_name\", \"source\"]\n",
    "        stage_df_all[col] = _normalize_str(stage_df_all[col], to_lower=to_lower)\n",
    "\n",
    "print(\"\\n=== Identifier column samples ===\")\n",
    "for col in [\"pipeline\", \"student_size\", \"dataset_choice\", \"stage_name\"]:\n",
    "    if col in stage_df_all.columns:\n",
    "        print(f\"{col}: {stage_df_all[col].dropna().unique()[:5]}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Stage role inference\n",
    "# -------------------------------------------------------------------------\n",
    "def infer_stage_role(row: pd.Series) -> str:\n",
    "    \"\"\"Heuristic stage categorization used for teacher/student/eval breakdowns.\n",
    "\n",
    "    Notes:\n",
    "    - Teacher-side work often shows up as logit/logprob caching or synthetic generation.\n",
    "    - Some pipelines emit 'summary' or 'snapshot' rows; prefer filtering those at analysis\n",
    "      time (see `make_analysis_df`) to avoid double counting.\n",
    "    \"\"\"\n",
    "    name = str(row.get(\"stage_name\", \"\") or \"\").lower()\n",
    "    pipeline = str(row.get(\"pipeline\", \"\") or \"\").lower()\n",
    "    source = str(row.get(\"source\", \"\") or \"\").lower()\n",
    "\n",
    "    # Snapshot/checkpoint rows (often cumulative) — treat separately\n",
    "    if source in {\"snapshot\"} or bool(row.get(\"is_snapshot\", False)):\n",
    "        return \"snapshot\"\n",
    "\n",
    "    # Synthetic / teacher-side work\n",
    "    if \"synthetic\" in name:\n",
    "        if any(k in name for k in [\"gen\", \"generation\", \"create\", \"produce\"]):\n",
    "            return \"teacher_generation\"\n",
    "        return \"teacher_processing\"\n",
    "\n",
    "    # Teacher forward/caching stages (common in KD/logit/logprob caching)\n",
    "    if any(k in name for k in [\"logit\", \"logprob\", \"teacher_forward\", \"cache_logits\", \"cache_logprobs\"]):\n",
    "        return \"teacher_processing\"\n",
    "    if \"cache\" in name and any(k in name for k in [\"kd\", \"distill\", \"teacher\"]):\n",
    "        return \"teacher_processing\"\n",
    "\n",
    "    # Dataset preprocessing\n",
    "    if any(k in name for k in [\"preprocess\", \"tokenize\", \"pack\", \"shard\"]):\n",
    "        return \"data_preprocess\"\n",
    "\n",
    "    # Evaluation stages\n",
    "    if any(k in name for k in [\"eval\", \"gsm8k\", \"mmlu\", \"alpaca\", \"alpacaeval\", \"ifeval\", \"mt-bench\", \"mtbench\"]):\n",
    "        return \"evaluation\"\n",
    "\n",
    "    # Main training pipelines\n",
    "    if pipeline in {\"sft\", \"kd\", \"dpo\"}:\n",
    "        if source == \"summary\":\n",
    "            return \"train_summary\"\n",
    "        return \"student_train\"\n",
    "\n",
    "    # Fallbacks\n",
    "    if source == \"summary\":\n",
    "        return \"summary_only\"\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "stage_df_all[\"stage_role\"] = stage_df_all.apply(infer_stage_role, axis=1)\n",
    "\n",
    "print(\"\\n=== stage_role value counts ===\")\n",
    "print(stage_df_all[\"stage_role\"].value_counts())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Run ID: group stages belonging to the same logical experiment\n",
    "# -------------------------------------------------------------------------\n",
    "if \"experiment_name\" in stage_df_all.columns:\n",
    "    if \"stage_name\" in stage_df_all.columns:\n",
    "        stage_df_all[\"run_id\"] = stage_df_all[\"experiment_name\"].fillna(\n",
    "            stage_df_all[\"stage_name\"]\n",
    "        )\n",
    "    else:\n",
    "        stage_df_all[\"run_id\"] = stage_df_all[\"experiment_name\"]\n",
    "else:\n",
    "    if \"stage_name\" in stage_df_all.columns:\n",
    "        stage_df_all[\"run_id\"] = stage_df_all[\"stage_name\"]\n",
    "    elif \"stage_dir\" in stage_df_all.columns:\n",
    "        stage_df_all[\"run_id\"] = (\n",
    "            stage_df_all[\"stage_dir\"]\n",
    "            .astype(str)\n",
    "            .apply(lambda p: Path(p).name)\n",
    "        )\n",
    "    else:\n",
    "        # Last-resort: just use the row index as a string\n",
    "        stage_df_all[\"run_id\"] = stage_df_all.index.astype(str)\n",
    "\n",
    "print(\"\\nUnique run_id count:\", stage_df_all[\"run_id\"].nunique())\n",
    "\n",
    "\n",
    "print(\"\\nUnique run_id count:\", stage_df_all[\"run_id\"].nunique())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3×3 grid cell ID: pipeline × student_size × dataset_choice\n",
    "# -------------------------------------------------------------------------\n",
    "required_for_cell = [\"pipeline\", \"student_size\", \"dataset_choice\"]\n",
    "for col in required_for_cell:\n",
    "    if col in stage_df_all.columns:\n",
    "        stage_df_all[col] = stage_df_all[col].replace(\"\", np.nan)\n",
    "\n",
    "if all(col in stage_df_all.columns for col in required_for_cell):\n",
    "    missing_any = stage_df_all[required_for_cell].isna().any(axis=1)\n",
    "    stage_df_all[\"cell_id\"] = np.where(\n",
    "        missing_any,\n",
    "        np.nan,\n",
    "        (\n",
    "            stage_df_all[\"pipeline\"].str.lower()\n",
    "            + \"_\"\n",
    "            + stage_df_all[\"student_size\"].astype(str)\n",
    "            + \"_\"\n",
    "            + stage_df_all[\"dataset_choice\"].str.lower()\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    stage_df_all[\"cell_id\"] = np.nan\n",
    "\n",
    "print(\"\\n=== cell_id summary ===\")\n",
    "print(\"Non-null cell_id rows:\", stage_df_all[\"cell_id\"].notna().sum())\n",
    "print(\"Unique cell_ids:\", stage_df_all[\"cell_id\"].dropna().nunique())\n",
    "print(stage_df_all[\"cell_id\"].dropna().unique())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Keep stage_df_clean in sync with new columns\n",
    "# -------------------------------------------------------------------------\n",
    "if \"tokens_processed\" in stage_df_all.columns:\n",
    "    valid = stage_df_all[\"tokens_processed\"].notna() & (stage_df_all[\"tokens_processed\"] > 0)\n",
    "    stage_df_clean = stage_df_all[valid].copy()\n",
    "else:\n",
    "    stage_df_clean = stage_df_all.copy()\n",
    "\n",
    "print(\"\\n=== Preview with tagging columns ===\")\n",
    "display(\n",
    "    stage_df_all[\n",
    "        [\n",
    "            \"stage_name\",\n",
    "            \"pipeline\",\n",
    "            \"student_size\",\n",
    "            \"dataset_choice\",\n",
    "            \"stage_role\",\n",
    "            \"run_id\",\n",
    "            \"cell_id\",\n",
    "            \"energy_kwh\",\n",
    "            \"energy_j_per_token\",\n",
    "            \"tokens_per_sec\",\n",
    "        ]\n",
    "    ].head(15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70783a93",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Aggregate pipeline-level metrics (Step 3)\n",
    "\n",
    "print(\"\\n\\n=== Step 3: Aggregating pipeline-level (cell) metrics ===\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Effective energy in Joules for aggregation\n",
    "# -------------------------------------------------------------------------\n",
    "if \"total_energy_joules\" in stage_df_all.columns:\n",
    "    stage_df_all[\"energy_joules_eff\"] = stage_df_all[\"total_energy_joules\"]\n",
    "    # Fill missing with kWh-based estimate if available\n",
    "    if \"energy_kwh\" in stage_df_all.columns:\n",
    "        mask_fill_j = stage_df_all[\"energy_joules_eff\"].isna() & stage_df_all[\"energy_kwh\"].notna()\n",
    "        stage_df_all.loc[mask_fill_j, \"energy_joules_eff\"] = (\n",
    "            stage_df_all.loc[mask_fill_j, \"energy_kwh\"] * 3.6e6\n",
    "        )\n",
    "        if mask_fill_j.any():\n",
    "            print(\n",
    "                f\"[INFO] Filled energy_joules_eff from energy_kwh for \"\n",
    "                f\"{mask_fill_j.sum()} rows\"\n",
    "            )\n",
    "else:\n",
    "    if \"energy_kwh\" in stage_df_all.columns:\n",
    "        stage_df_all[\"energy_joules_eff\"] = stage_df_all[\"energy_kwh\"] * 3.6e6\n",
    "        print(\"[INFO] Constructed energy_joules_eff from energy_kwh for all rows.\")\n",
    "    else:\n",
    "        stage_df_all[\"energy_joules_eff\"] = np.nan\n",
    "        print(\"[WARN] No total_energy_joules or energy_kwh; energy_joules_eff is NaN.\")\n",
    "\n",
    "print(\"\\n=== energy_joules_eff summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"energy_joules_eff\"].notna().sum())\n",
    "print(\"Rows with NaN energy_joules_eff:\", stage_df_all[\"energy_joules_eff\"].isna().sum())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Stage-role groupings for aggregation\n",
    "# -------------------------------------------------------------------------\n",
    "STUDENT_ROLES = {\"student_train\", \"train_summary\"}\n",
    "TEACHER_ROLES = {\"teacher_generation\", \"teacher_processing\", \"data_preprocess\"}\n",
    "EVAL_ROLES = {\"evaluation\"}\n",
    "\n",
    "def _safe_div(num, denom):\n",
    "    \"\"\"Division with protection against zero / NaN denom.\"\"\"\n",
    "    if denom is None or pd.isna(denom) or denom == 0:\n",
    "        return np.nan\n",
    "    return num / denom\n",
    "\n",
    "def aggregate_cell_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate stage-level metrics into pipeline-level (cell-level) metrics.\n",
    "\n",
    "    Grouping keys:\n",
    "        pipeline, student_size, dataset_choice, cell_id, run_id\n",
    "\n",
    "    For each group, we compute:\n",
    "        - student_tokens / teacher_tokens\n",
    "        - student_duration_s / teacher_duration_s\n",
    "        - student_kwh / teacher_kwh / eval_kwh / other_kwh / total_kwh_all\n",
    "        - GPU/CPU kWh for student stages\n",
    "        - total / student / teacher energy in Joules\n",
    "        - tokens_per_sec_student\n",
    "        - energy_j_per_token_total\n",
    "        - energy_j_per_token_student\n",
    "    \"\"\"\n",
    "    group_cols = [\"pipeline\", \"student_size\", \"dataset_choice\", \"cell_id\", \"run_id\"]\n",
    "    group_cols = [c for c in group_cols if c in df.columns]\n",
    "\n",
    "    print(\"\\n[DEBUG] Aggregating with group columns:\", group_cols)\n",
    "    grouped = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for key, rows in grouped:\n",
    "        # key can be a scalar or tuple depending on number of group_cols\n",
    "        if not isinstance(key, tuple):\n",
    "            key = (key,)\n",
    "        record = dict(zip(group_cols, key))\n",
    "\n",
    "        # Split by stage_role\n",
    "        student_rows = rows[rows[\"stage_role\"].isin(STUDENT_ROLES)]\n",
    "        teacher_rows = rows[rows[\"stage_role\"].isin(TEACHER_ROLES)]\n",
    "        eval_rows = rows[rows[\"stage_role\"].isin(EVAL_ROLES)]\n",
    "        other_rows = rows[\n",
    "            ~rows[\"stage_role\"].isin(STUDENT_ROLES | TEACHER_ROLES | EVAL_ROLES)\n",
    "        ]\n",
    "\n",
    "        # Tokens\n",
    "        student_tokens = (\n",
    "            student_rows[\"tokens_processed\"].sum()\n",
    "            if \"tokens_processed\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "        teacher_tokens = (\n",
    "            teacher_rows[\"tokens_processed\"].sum()\n",
    "            if \"tokens_processed\" in teacher_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        # Durations\n",
    "        student_dur = (\n",
    "            student_rows[\"duration_seconds\"].sum()\n",
    "            if \"duration_seconds\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "        teacher_dur = (\n",
    "            teacher_rows[\"duration_seconds\"].sum()\n",
    "            if \"duration_seconds\" in teacher_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        # Energy in kWh\n",
    "        student_kwh = student_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in student_rows.columns else np.nan\n",
    "        teacher_kwh = teacher_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in teacher_rows.columns else np.nan\n",
    "        eval_kwh = eval_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in eval_rows.columns else np.nan\n",
    "        other_kwh = other_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in other_rows.columns else np.nan\n",
    "        total_kwh_all = rows[\"energy_kwh\"].sum() if \"energy_kwh\" in rows.columns else np.nan\n",
    "\n",
    "        # Energy in Joules (effective)\n",
    "        student_j = student_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in student_rows.columns else np.nan\n",
    "        teacher_j = teacher_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in teacher_rows.columns else np.nan\n",
    "        eval_j = eval_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in eval_rows.columns else np.nan\n",
    "        other_j = other_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in other_rows.columns else np.nan\n",
    "        total_j_all = rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in rows.columns else np.nan\n",
    "\n",
    "        # GPU / CPU energy (student-only for now; you can expand to teacher/eval later)\n",
    "        student_gpu_kwh = (\n",
    "            student_rows[\"gpu_energy_kwh\"].sum()\n",
    "            if \"gpu_energy_kwh\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "        student_cpu_kwh = (\n",
    "            student_rows[\"cpu_energy_kwh\"].sum()\n",
    "            if \"cpu_energy_kwh\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        # Tokens per second (student)\n",
    "        tokens_per_sec_student = _safe_div(student_tokens, student_dur)\n",
    "\n",
    "        # J/token:\n",
    "        #  - total: teacher + student + eval + other, divided by student tokens\n",
    "        #  - student-only: student energy divided by student tokens\n",
    "        energy_j_per_token_total = _safe_div(total_j_all, student_tokens)\n",
    "        energy_j_per_token_student = _safe_div(student_j, student_tokens)\n",
    "\n",
    "        record.update(\n",
    "            dict(\n",
    "                student_tokens=student_tokens,\n",
    "                teacher_tokens=teacher_tokens,\n",
    "                student_duration_s=student_dur,\n",
    "                teacher_duration_s=teacher_dur,\n",
    "                student_kwh=student_kwh,\n",
    "                teacher_kwh=teacher_kwh,\n",
    "                eval_kwh=eval_kwh,\n",
    "                other_kwh=other_kwh,\n",
    "                total_kwh_all=total_kwh_all,\n",
    "                student_energy_joules=student_j,\n",
    "                teacher_energy_joules=teacher_j,\n",
    "                eval_energy_joules=eval_j,\n",
    "                other_energy_joules=other_j,\n",
    "                total_energy_joules_all=total_j_all,\n",
    "                student_gpu_kwh=student_gpu_kwh,\n",
    "                student_cpu_kwh=student_cpu_kwh,\n",
    "                tokens_per_sec_student=tokens_per_sec_student,\n",
    "                energy_j_per_token_total=energy_j_per_token_total,\n",
    "                energy_j_per_token_student=energy_j_per_token_student,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    agg_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    print(f\"[INFO] Aggregated into {len(agg_df)} rows (one per {group_cols} group).\")\n",
    "    return agg_df\n",
    "\n",
    "if \"stage_role\" not in stage_df_all.columns:\n",
    "    stage_df_all[\"stage_role\"] = stage_df_all.apply(infer_stage_role, axis=1)\n",
    "\n",
    "cell_metrics_df = aggregate_cell_metrics(stage_df_all)\n",
    "\n",
    "print(\"\\n=== cell_metrics_df basic summary ===\")\n",
    "print(\"Rows:\", len(cell_metrics_df))\n",
    "print(\"Distinct cells (cell_id):\", cell_metrics_df[\"cell_id\"].dropna().nunique() if \"cell_id\" in cell_metrics_df.columns else \"N/A\")\n",
    "print(\"Distinct run_id:\", cell_metrics_df[\"run_id\"].nunique() if \"run_id\" in cell_metrics_df.columns else \"N/A\")\n",
    "\n",
    "print(\"\\n=== Sample of aggregated pipeline-level metrics ===\")\n",
    "cols_to_show = [\n",
    "    \"pipeline\",\n",
    "    \"student_size\",\n",
    "    \"dataset_choice\",\n",
    "    \"cell_id\",\n",
    "    \"run_id\",\n",
    "    \"student_tokens\",\n",
    "    \"student_duration_s\",\n",
    "    \"total_kwh_all\",\n",
    "    \"student_kwh\",\n",
    "    \"teacher_kwh\",\n",
    "    \"eval_kwh\",\n",
    "    \"tokens_per_sec_student\",\n",
    "    \"energy_j_per_token_total\",\n",
    "    \"energy_j_per_token_student\",\n",
    "]\n",
    "cols_to_show = [c for c in cols_to_show if c in cell_metrics_df.columns]\n",
    "display(cell_metrics_df[cols_to_show].head(15))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Optional hook: evaluation metrics (to be added later)\n",
    "# -------------------------------------------------------------------------\n",
    "print(\n",
    "    \"\\n[NOTE] Evaluation metrics (GSM8K, MMLU, AlpacaEval, etc.) \"\n",
    "    \"will be merged into cell_metrics_df in a later step once we \"\n",
    "    \"have a structured eval_metrics CSV.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b3d00",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Helpers and core 3×3-style grid (Step 3b)\n",
    "\n",
    "print(\"\\n\\n=== Step 3b: Helpers and core 3×3-style grid ===\")\n",
    "\n",
    "def filter_cells(\n",
    "    df: pd.DataFrame,\n",
    "    pipeline: str | list[str] | None = None,\n",
    "    student_size: str | list[str] | None = None,\n",
    "    dataset_choice: str | list[str] | None = None,\n",
    "    run_id: str | list[str] | None = None,\n",
    "    cell_id: str | list[str] | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Convenience filter for aggregated (run-level or cell-level) DataFrames.\n",
    "\n",
    "    - `pipeline` and `dataset_choice` are matched case-insensitively.\n",
    "    - `student_size`, `run_id`, `cell_id` are matched as strings.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def _norm(val, *, lower=False):\n",
    "        if val is None:\n",
    "            return None\n",
    "        if isinstance(val, (list, tuple, set)):\n",
    "            vals = list(val)\n",
    "        else:\n",
    "            vals = [val]\n",
    "        vals = [str(v) for v in vals]\n",
    "        return [v.lower() for v in vals] if lower else vals\n",
    "\n",
    "    pipeline = _norm(pipeline, lower=True)\n",
    "    dataset_choice = _norm(dataset_choice, lower=True)\n",
    "    student_size = _norm(student_size)\n",
    "    run_id = _norm(run_id)\n",
    "    cell_id = _norm(cell_id)\n",
    "\n",
    "    if pipeline is not None and \"pipeline\" in out.columns:\n",
    "        out = out[out[\"pipeline\"].astype(str).str.lower().isin(pipeline)]\n",
    "    if student_size is not None and \"student_size\" in out.columns:\n",
    "        out = out[out[\"student_size\"].astype(str).isin(student_size)]\n",
    "    if dataset_choice is not None and \"dataset_choice\" in out.columns:\n",
    "        out = out[out[\"dataset_choice\"].astype(str).str.lower().isin(dataset_choice)]\n",
    "    if run_id is not None and \"run_id\" in out.columns:\n",
    "        out = out[out[\"run_id\"].astype(str).isin(run_id)]\n",
    "    if cell_id is not None and \"cell_id\" in out.columns:\n",
    "        out = out[out[\"cell_id\"].astype(str).isin(cell_id)]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[filter_cells] -> {len(out)} rows (from {len(df)} input rows)\")\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def build_core_grid(\n",
    "    df: pd.DataFrame,\n",
    "    primary_dataset: str | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate run-level cell metrics into a single row per\n",
    "    (pipeline, student_size, dataset_choice).\n",
    "\n",
    "    This is the basis for the 3×3 grid table:\n",
    "        pipelines × student sizes (× dataset)\n",
    "\n",
    "    Metrics:\n",
    "        - total_student_tokens\n",
    "        - total_student_duration_s\n",
    "        - total_kwh_all\n",
    "        - student_kwh, teacher_kwh, eval_kwh, other_kwh\n",
    "        - tokens_per_sec_student_agg\n",
    "        - energy_j_per_token_total_agg\n",
    "        - energy_j_per_token_student_agg\n",
    "        - summed student/teacher/eval Joules\n",
    "    \"\"\"\n",
    "    work_df = df.copy()\n",
    "\n",
    "    # Optionally restrict to a primary dataset, e.g. \"tulu\"\n",
    "    if primary_dataset is not None and \"dataset_choice\" in work_df.columns:\n",
    "        before = len(work_df)\n",
    "        work_df = work_df[work_df[\"dataset_choice\"] == primary_dataset]\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[build_core_grid] Filtered to dataset_choice == '{primary_dataset}': \"\n",
    "                f\"{before} -> {len(work_df)} rows\"\n",
    "            )\n",
    "\n",
    "    # Group by pipeline × student_size × dataset_choice\n",
    "    group_cols = [\"pipeline\", \"student_size\"]\n",
    "    if \"dataset_choice\" in work_df.columns:\n",
    "        group_cols.append(\"dataset_choice\")\n",
    "\n",
    "    group_cols = [c for c in group_cols if c in work_df.columns]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[build_core_grid] Grouping by:\", group_cols)\n",
    "\n",
    "    grouped = work_df.groupby(group_cols, dropna=False)\n",
    "\n",
    "    grid_records = []\n",
    "\n",
    "    for key, rows in grouped:\n",
    "        if not isinstance(key, tuple):\n",
    "            key = (key,)\n",
    "        rec = dict(zip(group_cols, key))\n",
    "\n",
    "        # Aggregate tokens and duration (summing across runs)\n",
    "        student_tokens_total = rows[\"student_tokens\"].sum()\n",
    "        student_duration_total = rows[\"student_duration_s\"].sum()\n",
    "\n",
    "        # Aggregate kWh\n",
    "        student_kwh_total = rows[\"student_kwh\"].sum()\n",
    "        teacher_kwh_total = rows[\"teacher_kwh\"].sum()\n",
    "        eval_kwh_total = rows[\"eval_kwh\"].sum()\n",
    "        other_kwh_total = rows[\"other_kwh\"].sum()\n",
    "        total_kwh_all = rows[\"total_kwh_all\"].sum()\n",
    "\n",
    "        # Aggregate Joules\n",
    "        student_j_total = rows[\"student_energy_joules\"].sum()\n",
    "        teacher_j_total = rows[\"teacher_energy_joules\"].sum()\n",
    "        eval_j_total = rows[\"eval_energy_joules\"].sum()\n",
    "        other_j_total = rows[\"other_energy_joules\"].sum()\n",
    "        total_j_all = rows[\"total_energy_joules_all\"].sum()\n",
    "\n",
    "        # Aggregate GPU/CPU energy for student stages\n",
    "        student_gpu_kwh_total = rows[\"student_gpu_kwh\"].sum()\n",
    "        student_cpu_kwh_total = rows[\"student_cpu_kwh\"].sum()\n",
    "\n",
    "        # Recompute tokens/sec and J/token at this aggregated level\n",
    "        tokens_per_sec_student_agg = (\n",
    "            student_tokens_total / student_duration_total\n",
    "            if (pd.notna(student_duration_total) and student_duration_total > 0)\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        energy_j_per_token_total_agg = (\n",
    "            total_j_all / student_tokens_total\n",
    "            if (pd.notna(student_tokens_total) and student_tokens_total > 0)\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        energy_j_per_token_student_agg = (\n",
    "            student_j_total / student_tokens_total\n",
    "            if (pd.notna(student_tokens_total) and student_tokens_total > 0)\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        rec.update(\n",
    "            dict(\n",
    "                runs_in_cell=len(rows),\n",
    "                total_student_tokens=student_tokens_total,\n",
    "                total_student_duration_s=student_duration_total,\n",
    "                total_kwh_all=total_kwh_all,\n",
    "                student_kwh=student_kwh_total,\n",
    "                teacher_kwh=teacher_kwh_total,\n",
    "                eval_kwh=eval_kwh_total,\n",
    "                other_kwh=other_kwh_total,\n",
    "                total_energy_joules_all=total_j_all,\n",
    "                student_energy_joules=student_j_total,\n",
    "                teacher_energy_joules=teacher_j_total,\n",
    "                eval_energy_joules=eval_j_total,\n",
    "                other_energy_joules=other_j_total,\n",
    "                student_gpu_kwh=student_gpu_kwh_total,\n",
    "                student_cpu_kwh=student_cpu_kwh_total,\n",
    "                tokens_per_sec_student=tokens_per_sec_student_agg,\n",
    "                energy_j_per_token_total=energy_j_per_token_total_agg,\n",
    "                energy_j_per_token_student=energy_j_per_token_student_agg,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        grid_records.append(rec)\n",
    "\n",
    "    grid_df = pd.DataFrame.from_records(grid_records)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"[build_core_grid] Created grid_df with {len(grid_df)} rows \"\n",
    "            f\"(from {len(work_df)} input rows)\"\n",
    "        )\n",
    "        if \"pipeline\" in grid_df.columns:\n",
    "            print(\"  Pipelines:\", grid_df[\"pipeline\"].dropna().unique())\n",
    "        if \"student_size\" in grid_df.columns:\n",
    "            print(\"  Student sizes:\", grid_df[\"student_size\"].dropna().unique())\n",
    "        if \"dataset_choice\" in grid_df.columns:\n",
    "            print(\"  Datasets:\", grid_df[\"dataset_choice\"].dropna().unique())\n",
    "\n",
    "    return grid_df\n",
    "\n",
    "\n",
    "# You can change this to your primary headline dataset, e.g. \"tulu\"\n",
    "PRIMARY_DATASET_FOR_CORE_GRID = None  # e.g. \"tulu\"\n",
    "\n",
    "core_grid_df = build_core_grid(\n",
    "    cell_metrics_df,\n",
    "    primary_dataset=PRIMARY_DATASET_FOR_CORE_GRID,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== core_grid_df preview (for 3×3-style table) ===\")\n",
    "cols_to_show = [\n",
    "    \"pipeline\",\n",
    "    \"student_size\",\n",
    "    \"dataset_choice\",\n",
    "    \"runs_in_cell\",\n",
    "    \"total_student_tokens\",\n",
    "    \"total_student_duration_s\",\n",
    "    \"total_kwh_all\",\n",
    "    \"student_kwh\",\n",
    "    \"teacher_kwh\",\n",
    "    \"tokens_per_sec_student\",\n",
    "    \"energy_j_per_token_total\",\n",
    "    \"energy_j_per_token_student\",\n",
    "]\n",
    "cols_to_show = [c for c in cols_to_show if c in core_grid_df.columns]\n",
    "display(core_grid_df[cols_to_show].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Utility: summarize arbitrary stage names (by name pattern)\n",
    "\n",
    "print(\"\\n\\n=== Utility: summarize selected stage_names ===\")\n",
    "\n",
    "def summarize_stages_by_name(\n",
    "    df: pd.DataFrame,\n",
    "    stage_name_patterns: list[str],\n",
    "    use_contains: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sum energy + tokens for stages whose `stage_name` matches\n",
    "    any of the provided patterns.\n",
    "\n",
    "    Args:\n",
    "        df: stage-level DataFrame (e.g., stage_df_all or stage_df_clean)\n",
    "        stage_name_patterns: list of patterns, e.g.\n",
    "            [\"logit_caching\", \"kd_32b_to_7b\", \"eval_gsm8k\"]\n",
    "        use_contains: if True, pattern is substring; if False, exact match.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame grouped by stage_name with:\n",
    "            - tokens_processed_sum\n",
    "            - duration_seconds_sum\n",
    "            - energy_kwh_sum\n",
    "            - energy_joules_eff_sum\n",
    "            - tokens_per_sec (recomputed)\n",
    "            - energy_j_per_token (recomputed)\n",
    "    \"\"\"\n",
    "    if \"stage_name\" not in df.columns:\n",
    "        raise ValueError(\"summarize_stages_by_name: df must have a 'stage_name' column.\")\n",
    "\n",
    "    patterns = list(stage_name_patterns)\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "\n",
    "    for p in patterns:\n",
    "        if use_contains:\n",
    "            mask |= df[\"stage_name\"].str.contains(p, na=False)\n",
    "        else:\n",
    "            mask |= (df[\"stage_name\"] == p)\n",
    "\n",
    "    subset = df[mask].copy()\n",
    "    print(\n",
    "        f\"[summarize_stages_by_name] Selected {len(subset)} rows \"\n",
    "        f\"matching patterns: {patterns}\"\n",
    "    )\n",
    "\n",
    "    if subset.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    group = subset.groupby(\"stage_name\", dropna=False)\n",
    "    rows = []\n",
    "\n",
    "    for name, g in group:\n",
    "        tokens_sum = g[\"tokens_processed\"].sum() if \"tokens_processed\" in g.columns else np.nan\n",
    "        dur_sum = g[\"duration_seconds\"].sum() if \"duration_seconds\" in g.columns else np.nan\n",
    "        kwh_sum = g[\"energy_kwh\"].sum() if \"energy_kwh\" in g.columns else np.nan\n",
    "        j_sum = g[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in g.columns else np.nan\n",
    "\n",
    "        tokens_per_sec = (tokens_sum / dur_sum) if (pd.notna(dur_sum) and dur_sum > 0) else np.nan\n",
    "        energy_j_per_token = (j_sum / tokens_sum) if (pd.notna(tokens_sum) and tokens_sum > 0) else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            dict(\n",
    "                stage_name=name,\n",
    "                rows_included=len(g),\n",
    "                tokens_processed_sum=tokens_sum,\n",
    "                duration_seconds_sum=dur_sum,\n",
    "                energy_kwh_sum=kwh_sum,\n",
    "                energy_joules_eff_sum=j_sum,\n",
    "                tokens_per_sec=tokens_per_sec,\n",
    "                energy_j_per_token=energy_j_per_token,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"stage_name\").reset_index(drop=True)\n",
    "    print(\"[summarize_stages_by_name] Summary:\")\n",
    "    display(out)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c88601",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage (commented out):\n",
    "# summarize_stages_by_name(stage_df_all, [\"logit_caching\", \"kd_32b_to_7b\", \"eval\"], use_contains=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7ff53",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Stage-wise breakdown & teacher vs student vs eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n\\n=== Stage-wise breakdown helpers ===\")\n",
    "\n",
    "def stage_breakdown_for_cell(\n",
    "    stage_df: pd.DataFrame,\n",
    "    pipeline: str | None = None,\n",
    "    student_size: str | None = None,\n",
    "    dataset_choice: str | None = None,\n",
    "    run_id: str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute stage-wise energy breakdown (kWh and fraction) for a given\n",
    "    (pipeline, student_size, dataset_choice, run_id) subset.\n",
    "\n",
    "    Returns a small DataFrame indexed by stage_role + stage_name.\n",
    "    \"\"\"\n",
    "    df = stage_df.copy()\n",
    "\n",
    "    if pipeline is not None and \"pipeline\" in df.columns:\n",
    "        df = df[df[\"pipeline\"] == str(pipeline).lower()]\n",
    "    if student_size is not None and \"student_size\" in df.columns:\n",
    "        df = df[df[\"student_size\"] == str(student_size)]\n",
    "    if dataset_choice is not None and \"dataset_choice\" in df.columns:\n",
    "        df = df[df[\"dataset_choice\"] == str(dataset_choice).lower()]\n",
    "    if run_id is not None and \"run_id\" in df.columns:\n",
    "        df = df[df[\"run_id\"] == run_id]\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"[stage_breakdown_for_cell] No rows match the specified filters.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    group_cols = [\"stage_role\"]\n",
    "    if \"stage_name\" in df.columns:\n",
    "        group_cols.append(\"stage_name\")\n",
    "\n",
    "    grouped = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "    rows = []\n",
    "    for (role, name, *rest), g in grouped:\n",
    "        role = role\n",
    "        name = name\n",
    "        kwh = g[\"energy_kwh\"].sum() if \"energy_kwh\" in g.columns else np.nan\n",
    "        j = g[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in g.columns else np.nan\n",
    "\n",
    "        rows.append(\n",
    "            dict(\n",
    "                stage_role=role,\n",
    "                stage_name=name,\n",
    "                energy_kwh=kwh,\n",
    "                energy_joules_eff=j,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    total_kwh = out[\"energy_kwh\"].sum()\n",
    "    out[\"fraction_of_total_kwh\"] = (\n",
    "        out[\"energy_kwh\"] / total_kwh if (pd.notna(total_kwh) and total_kwh > 0) else np.nan\n",
    "    )\n",
    "    out = out.sort_values(\"energy_kwh\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        \"[stage_breakdown_for_cell] Breakdown for \"\n",
    "        f\"pipeline={pipeline}, student_size={student_size}, \"\n",
    "        f\"dataset_choice={dataset_choice}, run_id={run_id}\"\n",
    "    )\n",
    "    print(f\"Total energy_kwh: {total_kwh:.4f}\")\n",
    "    display(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_teacher_student_eval_bar(\n",
    "    df: pd.DataFrame,\n",
    "    title: str = \"Teacher vs Student vs Eval Energy (kWh)\",\n",
    "    figsize=(8, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a small DataFrame with columns:\n",
    "        label, student_kwh, teacher_kwh, eval_kwh, other_kwh\n",
    "    produce a stacked bar chart.\n",
    "    \"\"\"\n",
    "    required = [\"label\", \"student_kwh\", \"teacher_kwh\", \"eval_kwh\", \"other_kwh\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[plot_teacher_student_eval_bar] Missing columns: {missing}\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(df))\n",
    "    width = 0.6\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    bottom = np.zeros(len(df))\n",
    "    for col, legend_label in [\n",
    "        (\"teacher_kwh\", \"Teacher\"),\n",
    "        (\"student_kwh\", \"Student\"),\n",
    "        (\"eval_kwh\", \"Eval\"),\n",
    "        (\"other_kwh\", \"Other\"),\n",
    "    ]:\n",
    "        vals = df[col].fillna(0.0).to_numpy()\n",
    "        ax.bar(x, vals, width, bottom=bottom, label=legend_label)\n",
    "        bottom += vals\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df[\"label\"], rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Energy (kWh)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def build_teacher_student_eval_comparison_from_core(\n",
    "    core_df: pd.DataFrame,\n",
    "    selector: pd.DataFrame | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a compact table for teacher vs student vs eval energy from core_grid_df.\n",
    "\n",
    "    Optionally pass a pre-filtered selector DataFrame (subset of core_grid_df).\n",
    "    \"\"\"\n",
    "    work = selector if selector is not None else core_df\n",
    "    if work.empty:\n",
    "        print(\"[build_teacher_student_eval_comparison_from_core] Empty input.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for _, row in work.iterrows():\n",
    "        label_parts = []\n",
    "        if \"pipeline\" in row:\n",
    "            label_parts.append(str(row[\"pipeline\"]))\n",
    "        if \"student_size\" in row:\n",
    "            label_parts.append(str(row[\"student_size\"]))\n",
    "        if \"dataset_choice\" in row and pd.notna(row[\"dataset_choice\"]):\n",
    "            label_parts.append(str(row[\"dataset_choice\"]))\n",
    "        label = \" \".join(label_parts)\n",
    "\n",
    "        rows.append(\n",
    "            dict(\n",
    "                label=label,\n",
    "                student_kwh=row.get(\"student_kwh\", np.nan),\n",
    "                teacher_kwh=row.get(\"teacher_kwh\", np.nan),\n",
    "                eval_kwh=row.get(\"eval_kwh\", np.nan),\n",
    "                other_kwh=row.get(\"other_kwh\", np.nan),\n",
    "                total_kwh=row.get(\"total_kwh_all\", np.nan),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    print(\"[build_teacher_student_eval_comparison_from_core] Table:\")\n",
    "    display(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_gpu_cpu_share_for_core(core_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute GPU vs CPU energy share for student stages in core_grid_df.\n",
    "    \"\"\"\n",
    "    df = core_df.copy()\n",
    "    if not {\"student_gpu_kwh\", \"student_cpu_kwh\"}.issubset(df.columns):\n",
    "        print(\"[compute_gpu_cpu_share_for_core] Required columns missing.\")\n",
    "        return df\n",
    "\n",
    "    total_student = df[\"student_gpu_kwh\"].fillna(0.0) + df[\"student_cpu_kwh\"].fillna(0.0)\n",
    "    df[\"student_gpu_share\"] = np.where(\n",
    "        total_student > 0,\n",
    "        df[\"student_gpu_kwh\"].fillna(0.0) / total_student,\n",
    "        np.nan,\n",
    "    )\n",
    "    df[\"student_cpu_share\"] = np.where(\n",
    "        total_student > 0,\n",
    "        df[\"student_cpu_kwh\"].fillna(0.0) / total_student,\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "    print(\"\\n[compute_gpu_cpu_share_for_core] GPU/CPU share:\")\n",
    "    cols = [\n",
    "        \"pipeline\",\n",
    "        \"student_size\",\n",
    "        \"dataset_choice\",\n",
    "        \"student_gpu_kwh\",\n",
    "        \"student_cpu_kwh\",\n",
    "        \"student_gpu_share\",\n",
    "        \"student_cpu_share\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    display(df[cols].head(20))\n",
    "    return df\n",
    "\n",
    "# Example usage (commented out):\n",
    "# comparison_table = build_teacher_student_eval_comparison_from_core(core_grid_df)\n",
    "# plot_teacher_student_eval_bar(comparison_table)\n",
    "# core_grid_with_gpu_cpu = compute_gpu_cpu_share_for_core(core_grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903378a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Pareto frontier utilities (energy vs quality)\n",
    "\n",
    "print(\"\\n\\n=== Pareto frontier helpers ===\")\n",
    "\n",
    "def mark_pareto_frontier(\n",
    "    df: pd.DataFrame,\n",
    "    energy_col: str,\n",
    "    quality_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a boolean column 'pareto_optimal' to df indicating which rows are\n",
    "    on the Pareto frontier: no other row has BOTH lower (or equal) energy\n",
    "    AND higher (or equal) quality, with at least one strict inequality.\n",
    "\n",
    "    Returns a copy of df with the new column.\n",
    "    \"\"\"\n",
    "    if energy_col not in df.columns or quality_col not in df.columns:\n",
    "        print(f\"[mark_pareto_frontier] Missing columns {energy_col} or {quality_col}\")\n",
    "        return df.copy()\n",
    "\n",
    "    work = df[[energy_col, quality_col]].to_numpy()\n",
    "    n = work.shape[0]\n",
    "    pareto = np.ones(n, dtype=bool)\n",
    "\n",
    "    for i in range(n):\n",
    "        if not pareto[i]:\n",
    "            continue\n",
    "        ei, qi = work[i]\n",
    "        if pd.isna(ei) or pd.isna(qi):\n",
    "            pareto[i] = False\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            ej, qj = work[j]\n",
    "            if pd.isna(ej) or pd.isna(qj):\n",
    "                continue\n",
    "            # Check if j dominates i: lower or equal energy and higher or equal quality,\n",
    "            # with at least one strict inequality\n",
    "            if (ej <= ei) and (qj >= qi) and ((ej < ei) or (qj > qi)):\n",
    "                pareto[i] = False\n",
    "                break\n",
    "\n",
    "    df_out = df.copy()\n",
    "    df_out[\"pareto_optimal\"] = pareto\n",
    "    print(\n",
    "        f\"[mark_pareto_frontier] Marked {pareto.sum()} Pareto-optimal points \"\n",
    "        f\"out of {n}.\"\n",
    "    )\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def plot_energy_quality_pareto(\n",
    "    df: pd.DataFrame,\n",
    "    energy_col: str,\n",
    "    quality_col: str,\n",
    "    pipeline_col: str = \"pipeline\",\n",
    "    student_size_col: str = \"student_size\",\n",
    "    title: str | None = None,\n",
    "    figsize=(7, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter plot of energy vs quality with Pareto frontier highlighting.\n",
    "\n",
    "    Expects df to already have 'pareto_optimal' column (from mark_pareto_frontier).\n",
    "    \"\"\"\n",
    "    required = [energy_col, quality_col]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[plot_energy_quality_pareto] Missing columns: {missing}\")\n",
    "        return\n",
    "\n",
    "    if \"pareto_optimal\" not in df.columns:\n",
    "        print(\"[plot_energy_quality_pareto] 'pareto_optimal' column missing; call mark_pareto_frontier first.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Non-Pareto points\n",
    "    mask_pareto = df[\"pareto_optimal\"].fillna(False)\n",
    "    non_pareto = df[~mask_pareto]\n",
    "    pareto = df[mask_pareto]\n",
    "\n",
    "    # Encode pipeline as marker and student_size as color via simple mapping\n",
    "    pipeline_vals = non_pareto[pipeline_col].dropna().unique() if pipeline_col in df.columns else []\n",
    "    student_vals = non_pareto[student_size_col].dropna().unique() if student_size_col in df.columns else []\n",
    "\n",
    "    marker_map = {p: m for p, m in zip(pipeline_vals, [\"o\", \"s\", \"D\", \"^\", \"v\", \"P\"])}\n",
    "    color_map = {s: i for i, s in enumerate(student_vals)}\n",
    "\n",
    "    def _scatter(subset, edgecolors=None, linewidths=0.5, alpha=0.8, zorder=2):\n",
    "        for _, row in subset.iterrows():\n",
    "            e = row[energy_col]\n",
    "            q = row[quality_col]\n",
    "            if pd.isna(e) or pd.isna(q):\n",
    "                continue\n",
    "            marker = marker_map.get(row.get(pipeline_col), \"o\")\n",
    "            color_idx = color_map.get(row.get(student_size_col), 0)\n",
    "            ax.scatter(\n",
    "                e,\n",
    "                q,\n",
    "                marker=marker,\n",
    "                s=40,\n",
    "                edgecolors=edgecolors,\n",
    "                linewidths=linewidths,\n",
    "                alpha=alpha,\n",
    "                zorder=zorder,\n",
    "            )\n",
    "\n",
    "    # Plot non-Pareto\n",
    "    _scatter(non_pareto, edgecolors=None, linewidths=0.5, alpha=0.5, zorder=1)\n",
    "\n",
    "    # Plot Pareto with edge highlight\n",
    "    _scatter(pareto, edgecolors=\"black\", linewidths=1.2, alpha=0.9, zorder=3)\n",
    "\n",
    "    ax.set_xlabel(energy_col)\n",
    "    ax.set_ylabel(quality_col)\n",
    "    ax.set_title(title or f\"Energy vs Quality ({energy_col} vs {quality_col})\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (after you have eval metrics merged in):\n",
    "# gsm_df = mark_pareto_frontier(core_grid_df, energy_col=\"total_kwh_all\", quality_col=\"gsm8k_acc\")\n",
    "# plot_energy_quality_pareto(gsm_df, \"total_kwh_all\", \"gsm8k_acc\", title=\"GSM8K: Energy vs Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367e962",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Evaluation metrics merge + LaTeX / CSV exports\n",
    "\n",
    "print(\"\\n\\n=== Eval metrics + export helpers ===\")\n",
    "\n",
    "def merge_eval_metrics(\n",
    "    base_df: pd.DataFrame,\n",
    "    eval_df: pd.DataFrame,\n",
    "    on: list[str] | str = \"run_id\",\n",
    "    suffixes=(\"\", \"_eval\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge evaluation metrics into base_df (cell_metrics_df or core_grid_df).\n",
    "\n",
    "    `eval_df` should contain columns like:\n",
    "        run_id, gsm8k_acc, mmlu_acc, alpacaeval_winrate, ifeval_score, ...\n",
    "\n",
    "    Args:\n",
    "        base_df: DataFrame with at least the key columns.\n",
    "        eval_df: DataFrame with metrics.\n",
    "        on: column name or list of column names to merge on.\n",
    "    \"\"\"\n",
    "    merged = base_df.merge(eval_df, how=\"left\", on=on, suffixes=suffixes)\n",
    "    print(\n",
    "        f\"[merge_eval_metrics] Merged eval metrics: base {len(base_df)} rows, \"\n",
    "        f\"eval {len(eval_df)} rows -> merged {len(merged)} rows\"\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "\n",
    "def export_table_to_latex_and_csv(\n",
    "    df: pd.DataFrame,\n",
    "    cols: list[str],\n",
    "    latex_path: str,\n",
    "    csv_path: str | None = None,\n",
    "    float_format: str = \"%.3f\",\n",
    "    index: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Export a DataFrame slice to LaTeX and optionally CSV.\n",
    "\n",
    "    Args:\n",
    "        df: the full DataFrame.\n",
    "        cols: columns to include in the table.\n",
    "        latex_path: path to .tex file.\n",
    "        csv_path: optional path to .csv file.\n",
    "    \"\"\"\n",
    "    subset = df[cols].copy()\n",
    "    print(f\"[export_table_to_latex_and_csv] Exporting {len(subset)} rows to {latex_path}\")\n",
    "    subset.to_latex(\n",
    "        latex_path,\n",
    "        float_format=float_format,\n",
    "        index=index,\n",
    "        escape=True,\n",
    "    )\n",
    "    if csv_path is not None:\n",
    "        print(f\"[export_table_to_latex_and_csv] Exporting CSV to {csv_path}\")\n",
    "        subset.to_csv(csv_path, index=index)\n",
    "\n",
    "    display(subset.head(20))\n",
    "\n",
    "\n",
    "# Example usage for your 3×3 headline table (adjust cols as needed):\n",
    "# headline_cols = [\n",
    "#     \"pipeline\",\n",
    "#     \"student_size\",\n",
    "#     \"dataset_choice\",\n",
    "#     \"total_student_tokens\",\n",
    "#     \"total_kwh_all\",\n",
    "#     \"energy_j_per_token_total\",\n",
    "#     \"tokens_per_sec_student\",\n",
    "#     \"gsm8k_acc\",\n",
    "#     \"mmlu_acc\",\n",
    "#     \"alpacaeval_winrate\",\n",
    "# ]\n",
    "# headline_cols = [c for c in headline_cols if c in core_grid_df.columns]\n",
    "# export_table_to_latex_and_csv(\n",
    "#     core_grid_df,\n",
    "#     cols=headline_cols,\n",
    "#     latex_path=\"tables/core_3x3_grid.tex\",\n",
    "#     csv_path=\"tables/core_3x3_grid.csv\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Manual overrides (for correcting / adding results)\n",
    "\n",
    "print(\"\\n\\n=== Manual overrides helper ===\")\n",
    "\n",
    "def apply_overrides(\n",
    "    base_df: pd.DataFrame,\n",
    "    overrides_df: pd.DataFrame,\n",
    "    key_cols: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply overrides to base_df using key_cols as the identifier.\n",
    "\n",
    "    For each row in overrides_df:\n",
    "        - find matching row(s) in base_df by key_cols\n",
    "        - overwrite non-null values in overrides_df into base_df\n",
    "\n",
    "    If an override row does not match any base row, it is appended.\n",
    "    \"\"\"\n",
    "    base = base_df.copy()\n",
    "    ov = overrides_df.copy()\n",
    "\n",
    "    for col in key_cols:\n",
    "        if col not in base.columns:\n",
    "            raise ValueError(f\"apply_overrides: key column '{col}' missing from base_df.\")\n",
    "        if col not in ov.columns:\n",
    "            raise ValueError(f\"apply_overrides: key column '{col}' missing from overrides_df.\")\n",
    "\n",
    "    base[\"_override_key\"] = base[key_cols].astype(str).agg(\"||\".join, axis=1)\n",
    "    ov[\"_override_key\"] = ov[key_cols].astype(str).agg(\"||\".join, axis=1)\n",
    "\n",
    "    override_keys = set(ov[\"_override_key\"].unique())\n",
    "    base_keys = set(base[\"_override_key\"].unique())\n",
    "\n",
    "    to_update = override_keys & base_keys\n",
    "    to_add = override_keys - base_keys\n",
    "\n",
    "    print(\n",
    "        f\"[apply_overrides] {len(to_update)} rows will be updated, \"\n",
    "        f\"{len(to_add)} rows will be appended.\"\n",
    "    )\n",
    "\n",
    "    # Update existing rows\n",
    "    for key in to_update:\n",
    "        base_mask = base[\"_override_key\"] == key\n",
    "        ov_row = ov[ov[\"_override_key\"] == key].iloc[0]\n",
    "\n",
    "        for col in ov.columns:\n",
    "            if col in (\"_override_key\",) + tuple(key_cols):\n",
    "                continue\n",
    "            val = ov_row[col]\n",
    "            if pd.notna(val):\n",
    "                base.loc[base_mask, col] = val\n",
    "\n",
    "    # Append new rows\n",
    "    new_rows = []\n",
    "    for key in to_add:\n",
    "        ov_row = ov[ov[\"_override_key\"] == key].iloc[0].to_dict()\n",
    "        # Ensure all columns exist in base\n",
    "        for col in base.columns:\n",
    "            ov_row.setdefault(col, np.nan)\n",
    "        new_rows.append(ov_row)\n",
    "\n",
    "    if new_rows:\n",
    "        base = pd.concat([base, pd.DataFrame(new_rows)[base.columns]], ignore_index=True)\n",
    "\n",
    "    # Cleanup\n",
    "    base = base.drop(columns=[\"_override_key\"], errors=\"ignore\")\n",
    "\n",
    "    print(\"[apply_overrides] Overrides applied. New shape:\", base.shape)\n",
    "    return base\n",
    "\n",
    "# Example usage:\n",
    "# overrides_df = pd.DataFrame([\n",
    "#     {\n",
    "#         \"pipeline\": \"kd\",\n",
    "#         \"student_size\": \"7b\",\n",
    "#         \"dataset_choice\": \"tulu\",\n",
    "#         \"total_kwh_all\": 123.456,  # corrected value\n",
    "#         \"gsm8k_acc\": 0.745,       # manually entered\n",
    "#     },\n",
    "# ])\n",
    "# core_grid_corrected = apply_overrides(core_grid_df, overrides_df, key_cols=[\"pipeline\", \"student_size\", \"dataset_choice\"])\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Interface utilities: flexible grouping by run/stage names\n",
    "# -------------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "def load_stage_metrics_csvs(csv_paths: list[str | Path]) -> pd.DataFrame:\n",
    "    \"\"\"Load and concatenate one or more stage_metrics.csv files.\n",
    "\n",
    "    Adds a `metrics_csv_path` column for provenance.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for p in csv_paths:\n",
    "        p = Path(p)\n",
    "        df = pd.read_csv(p)\n",
    "        df[\"metrics_csv_path\"] = str(p)\n",
    "        frames.append(df)\n",
    "    return pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def prepare_stage_df(stage_df_raw: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Create canonical columns + tags used by downstream analysis.\n",
    "\n",
    "    Returns:\n",
    "        stage_df_all: with canonical columns and identifiers\n",
    "        stage_df_clean: minimal subset (tokens_processed > 0)\n",
    "    \"\"\"\n",
    "    df = stage_df_raw.copy()\n",
    "\n",
    "    # Coerce numerics (best-effort)\n",
    "    numeric_cols = [\n",
    "        \"duration_seconds\",\n",
    "        \"tokens_processed\",\n",
    "        \"tokens_per_second\",\n",
    "        \"total_energy_kwh\",\n",
    "        \"total_codecarbon_energy_kwh\",\n",
    "        \"gpu_energy_joules\",\n",
    "        \"cpu_energy_joules\",\n",
    "        \"total_energy_joules\",\n",
    "        \"joules_per_token\",\n",
    "        \"codecarbon_gpu_energy_kwh\",\n",
    "        \"codecarbon_cpu_energy_kwh\",\n",
    "    ]\n",
    "    for c in numeric_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Normalize identifiers\n",
    "    for c in [\"pipeline\", \"dataset_choice\", \"source\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).replace({\"\": np.nan}).str.lower()\n",
    "\n",
    "    # Canonical energy_kwh\n",
    "    energy_kwh = None\n",
    "    for c in [\"total_energy_kwh\", \"total_codecarbon_energy_kwh\", \"kwh_total\"]:\n",
    "        if c in df.columns:\n",
    "            if energy_kwh is None:\n",
    "                energy_kwh = df[c].copy()\n",
    "            else:\n",
    "                energy_kwh = energy_kwh.fillna(df[c])\n",
    "    df[\"energy_kwh\"] = energy_kwh if energy_kwh is not None else np.nan\n",
    "\n",
    "    # Canonical GPU/CPU kWh\n",
    "    df[\"gpu_energy_kwh\"] = np.nan\n",
    "    df[\"cpu_energy_kwh\"] = np.nan\n",
    "    if \"gpu_energy_joules\" in df.columns:\n",
    "        m = df[\"gpu_energy_joules\"].notna()\n",
    "        df.loc[m, \"gpu_energy_kwh\"] = df.loc[m, \"gpu_energy_joules\"] / 3.6e6\n",
    "    if \"cpu_energy_joules\" in df.columns:\n",
    "        m = df[\"cpu_energy_joules\"].notna()\n",
    "        df.loc[m, \"cpu_energy_kwh\"] = df.loc[m, \"cpu_energy_joules\"] / 3.6e6\n",
    "    if \"codecarbon_gpu_energy_kwh\" in df.columns:\n",
    "        df[\"gpu_energy_kwh\"] = df[\"gpu_energy_kwh\"].fillna(df[\"codecarbon_gpu_energy_kwh\"])\n",
    "    if \"codecarbon_cpu_energy_kwh\" in df.columns:\n",
    "        df[\"cpu_energy_kwh\"] = df[\"cpu_energy_kwh\"].fillna(df[\"codecarbon_cpu_energy_kwh\"])\n",
    "\n",
    "    # Canonical total energy in Joules\n",
    "    df[\"energy_joules_eff\"] = np.nan\n",
    "    if \"total_energy_joules\" in df.columns:\n",
    "        df[\"energy_joules_eff\"] = df[\"total_energy_joules\"]\n",
    "    m = df[\"energy_joules_eff\"].isna() & df[\"energy_kwh\"].notna()\n",
    "    df.loc[m, \"energy_joules_eff\"] = df.loc[m, \"energy_kwh\"] * 3.6e6\n",
    "\n",
    "    # tokens/sec and J/token (recomputed when possible)\n",
    "    df[\"tokens_per_sec\"] = np.nan\n",
    "    if \"tokens_per_second\" in df.columns:\n",
    "        df[\"tokens_per_sec\"] = df[\"tokens_per_second\"]\n",
    "    m = (\n",
    "        df[\"tokens_per_sec\"].isna()\n",
    "        & df[\"tokens_processed\"].notna()\n",
    "        & df[\"duration_seconds\"].notna()\n",
    "        & (df[\"duration_seconds\"] > 0)\n",
    "    )\n",
    "    df.loc[m, \"tokens_per_sec\"] = df.loc[m, \"tokens_processed\"] / df.loc[m, \"duration_seconds\"]\n",
    "\n",
    "    df[\"energy_j_per_token\"] = np.nan\n",
    "    if \"joules_per_token\" in df.columns:\n",
    "        df[\"energy_j_per_token\"] = df[\"joules_per_token\"]\n",
    "    m = (\n",
    "        df[\"energy_j_per_token\"].isna()\n",
    "        & df[\"energy_joules_eff\"].notna()\n",
    "        & df[\"tokens_processed\"].notna()\n",
    "        & (df[\"tokens_processed\"] > 0)\n",
    "    )\n",
    "    df.loc[m, \"energy_j_per_token\"] = df.loc[m, \"energy_joules_eff\"] / df.loc[m, \"tokens_processed\"]\n",
    "\n",
    "    # stage_role / run_id / cell_id\n",
    "    df[\"stage_role\"] = df.apply(infer_stage_role, axis=1)\n",
    "\n",
    "    if \"experiment_name\" in df.columns:\n",
    "        df[\"run_id\"] = df[\"experiment_name\"].fillna(df.get(\"stage_name\"))\n",
    "    else:\n",
    "        df[\"run_id\"] = df.get(\"stage_name\")\n",
    "\n",
    "    if all(c in df.columns for c in [\"pipeline\", \"student_size\", \"dataset_choice\"]):\n",
    "        missing_any = df[[\"pipeline\", \"student_size\", \"dataset_choice\"]].isna().any(axis=1)\n",
    "        df[\"cell_id\"] = np.where(\n",
    "            missing_any,\n",
    "            np.nan,\n",
    "            df[\"pipeline\"].astype(str)\n",
    "            + \"_\"\n",
    "            + df[\"student_size\"].astype(str)\n",
    "            + \"_\"\n",
    "            + df[\"dataset_choice\"].astype(str),\n",
    "        )\n",
    "    else:\n",
    "        df[\"cell_id\"] = np.nan\n",
    "\n",
    "    if \"tokens_processed\" in df.columns:\n",
    "        stage_df_clean = df[df[\"tokens_processed\"].notna() & (df[\"tokens_processed\"] > 0)].copy()\n",
    "    else:\n",
    "        stage_df_clean = df.copy()\n",
    "\n",
    "    return df, stage_df_clean\n",
    "\n",
    "\n",
    "def make_analysis_df(\n",
    "    stage_df_all: pd.DataFrame,\n",
    "    *,\n",
    "    require_tokens: bool = True,\n",
    "    exclude_sources: set[str] | None = None,\n",
    "    exclude_snapshots: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return a cleaned stage-level DF suitable for aggregation/plots.\n",
    "\n",
    "    Conservative defaults to reduce double-counting:\n",
    "    - exclude snapshot rows\n",
    "    - exclude source=='summary' rows\n",
    "\n",
    "    You can relax these constraints if you rely on summaries as fallback.\n",
    "    \"\"\"\n",
    "    df = stage_df_all.copy()\n",
    "\n",
    "    if exclude_sources is None:\n",
    "        exclude_sources = {\"summary\"}\n",
    "\n",
    "    if \"source\" in df.columns and exclude_sources:\n",
    "        df = df[~df[\"source\"].astype(str).str.lower().isin({s.lower() for s in exclude_sources})]\n",
    "\n",
    "    if exclude_snapshots:\n",
    "        if \"is_snapshot\" in df.columns:\n",
    "            df = df[~df[\"is_snapshot\"].fillna(False).astype(bool)]\n",
    "        if \"source\" in df.columns:\n",
    "            df = df[df[\"source\"].astype(str).str.lower() != \"snapshot\"]\n",
    "\n",
    "    if require_tokens and \"tokens_processed\" in df.columns:\n",
    "        df = df[df[\"tokens_processed\"].notna() & (df[\"tokens_processed\"] > 0)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_stage_role_overrides(\n",
    "    stage_df: pd.DataFrame,\n",
    "    overrides: dict[str, str],\n",
    "    *,\n",
    "    match: str = \"exact\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Override `stage_role` for specific stages.\n",
    "\n",
    "    Args:\n",
    "        overrides: mapping {pattern_or_stage_name -> new_stage_role}\n",
    "        match: \"exact\" | \"contains\" | \"regex\"\n",
    "    \"\"\"\n",
    "    df = stage_df.copy()\n",
    "    if \"stage_role\" not in df.columns or \"stage_name\" not in df.columns:\n",
    "        raise ValueError(\"apply_stage_role_overrides requires 'stage_name' and 'stage_role' columns.\")\n",
    "\n",
    "    for pat, new_role in overrides.items():\n",
    "        if match == \"exact\":\n",
    "            m = df[\"stage_name\"].astype(str) == str(pat)\n",
    "        elif match == \"contains\":\n",
    "            m = df[\"stage_name\"].astype(str).str.contains(str(pat), case=False, regex=False)\n",
    "        elif match == \"regex\":\n",
    "            m = df[\"stage_name\"].astype(str).str.contains(str(pat), case=False, regex=True)\n",
    "        else:\n",
    "            raise ValueError(\"match must be one of: exact|contains|regex\")\n",
    "        df.loc[m, \"stage_role\"] = str(new_role)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _match_stage_names(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    stage_names: list[str] | None = None,\n",
    "    stage_name_contains: list[str] | None = None,\n",
    "    stage_name_regex: list[str] | None = None,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Return a boolean mask selecting rows by stage_name matching rules.\"\"\"\n",
    "    if \"stage_name\" not in df.columns:\n",
    "        raise ValueError(\"stage_df is missing required column 'stage_name'.\")\n",
    "\n",
    "    name_series = df[\"stage_name\"].astype(str)\n",
    "\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "\n",
    "    if stage_names:\n",
    "        mask |= name_series.isin(stage_names)\n",
    "\n",
    "    if stage_name_contains:\n",
    "        for pat in stage_name_contains:\n",
    "            mask |= name_series.str.contains(str(pat), case=False, regex=False)\n",
    "\n",
    "    if stage_name_regex:\n",
    "        for pat in stage_name_regex:\n",
    "            mask |= name_series.str.contains(pat, case=False, regex=True)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def build_custom_cell_metrics(\n",
    "    stage_df: pd.DataFrame,\n",
    "    cell_specs: dict[str, dict],\n",
    "    *,\n",
    "    include_unmatched: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Build a run-level `cell_metrics_df` from user-defined stage-name groupings.\"\"\"\n",
    "    df = stage_df.copy()\n",
    "\n",
    "    required_cols = {\"stage_role\", \"energy_kwh\", \"energy_joules_eff\", \"tokens_processed\", \"duration_seconds\"}\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            \"build_custom_cell_metrics: stage_df is missing required columns: \"\n",
    "            + \", \".join(missing)\n",
    "        )\n",
    "\n",
    "    rows_for_agg = []\n",
    "    used_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "    for run_id, spec in cell_specs.items():\n",
    "        pipeline = str(spec.get(\"pipeline\", \"\")).lower()\n",
    "        student_size = str(spec.get(\"student_size\", \"\"))\n",
    "        dataset_choice = str(spec.get(\"dataset_choice\", \"\")).lower()\n",
    "        cell_id = spec.get(\"cell_id\") or f\"{pipeline}_{student_size}_{dataset_choice}\"\n",
    "\n",
    "        mask = _match_stage_names(\n",
    "            df,\n",
    "            stage_names=spec.get(\"stage_names\"),\n",
    "            stage_name_contains=spec.get(\"stage_name_contains\"),\n",
    "            stage_name_regex=spec.get(\"stage_name_regex\"),\n",
    "        )\n",
    "\n",
    "        sub = df[mask].copy()\n",
    "        used_mask |= mask\n",
    "\n",
    "        exclude_roles = spec.get(\"exclude_stage_roles\")\n",
    "        if exclude_roles and \"stage_role\" in sub.columns:\n",
    "            exclude_roles = {str(r) for r in exclude_roles}\n",
    "            sub = sub[~sub[\"stage_role\"].astype(str).isin(exclude_roles)]\n",
    "\n",
    "        sub[\"pipeline\"] = pipeline if pipeline else sub.get(\"pipeline\", np.nan)\n",
    "        sub[\"student_size\"] = student_size if student_size else sub.get(\"student_size\", np.nan)\n",
    "        sub[\"dataset_choice\"] = dataset_choice if dataset_choice else sub.get(\"dataset_choice\", np.nan)\n",
    "        sub[\"cell_id\"] = cell_id\n",
    "        sub[\"run_id\"] = str(run_id)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[build_custom_cell_metrics] run_id='{run_id}' -> \"\n",
    "                f\"{len(sub)} stage rows (cell_id='{cell_id}')\"\n",
    "            )\n",
    "            if len(sub) == 0:\n",
    "                print(\"  [WARN] No stages matched. Check patterns / stage_name values.\")\n",
    "\n",
    "        rows_for_agg.append(sub)\n",
    "\n",
    "    if not rows_for_agg:\n",
    "        raise ValueError(\"build_custom_cell_metrics: no specs produced any rows.\")\n",
    "\n",
    "    concat = pd.concat(rows_for_agg, axis=0, ignore_index=True)\n",
    "\n",
    "    cell_metrics_df = aggregate_cell_metrics(concat)\n",
    "\n",
    "    if include_unmatched:\n",
    "        unmatched = df[~used_mask].copy()\n",
    "        if len(unmatched) > 0:\n",
    "            unmatched[\"pipeline\"] = \"unassigned\"\n",
    "            unmatched[\"student_size\"] = \"NA\"\n",
    "            unmatched[\"dataset_choice\"] = \"NA\"\n",
    "            unmatched[\"cell_id\"] = \"unassigned\"\n",
    "            unmatched[\"run_id\"] = \"unassigned\"\n",
    "            cell_metrics_df_un = aggregate_cell_metrics(unmatched)\n",
    "            cell_metrics_df = pd.concat([cell_metrics_df, cell_metrics_df_un], ignore_index=True)\n",
    "\n",
    "    return cell_metrics_df\n",
    "\n",
    "\n",
    "def build_core_grid_from_specs(\n",
    "    stage_df: pd.DataFrame,\n",
    "    cell_specs: dict[str, dict],\n",
    "    *,\n",
    "    primary_dataset: str | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Convenience wrapper: specs -> cell_metrics_df -> core_grid_df.\"\"\"\n",
    "    cell_metrics_df = build_custom_cell_metrics(stage_df, cell_specs, verbose=verbose)\n",
    "    core_grid_df = build_core_grid(cell_metrics_df, primary_dataset=primary_dataset, verbose=verbose)\n",
    "    return cell_metrics_df, core_grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8533f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage_df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a602f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = \"tulu\"\n",
    "# \n",
    "# grid = core_grid_df[core_grid_df[\"dataset_choice\"].str.lower() == ds].copy()\n",
    "# kwh_grid = grid.pivot(\n",
    "#     index=\"pipeline\",\n",
    "#     columns=\"student_size\",\n",
    "#     values=\"total_kwh_all\",\n",
    "# )\n",
    "# print(kwh_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_runs = [\n",
    "#     \"tulu_synthetic_gen\",\n",
    "#     \"sft_32b_to_1b_tulu_nosft\",\n",
    "# ]\n",
    "# \n",
    "# subset_cells = cell_metrics_df[cell_metrics_df[\"run_id\"].isin(selected_runs)].copy()\n",
    "# subset_grid = build_core_grid(subset_cells, primary_dataset=None)\n",
    "# \n",
    "# subset_grid[[\n",
    "#     \"pipeline\", \"student_size\", \"dataset_choice\",\n",
    "#     \"total_kwh_all\", \"energy_j_per_token_total_agg\"\n",
    "# ]]\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
