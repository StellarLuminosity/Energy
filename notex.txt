
Results to Report - last test run of prerun.py:
--- Idle Baseline Results ---
  Average Power: 35.03 W
  Min Power: 34.04 W
  Max Power: 76.81 W
  Stable: False (std 4.93W > 1.00W; max-mean 41.79W > 10.00W)
  Std Dev: 4.93 W
  Samples: 601

Directories to clear / check before runs:
- For energy logs: /home/klambert/projects/aip-craffel/klambert/Energy/logs
- For Tulu core datset: /scratch/klambert/dataset/tulu-3-sft-mixture-preprocessed

KD:
- For teacher logprobs: /scratch/klambert/dataset/logprob_cache/teacher_logprobs
- Env metadata, checkpoints, and model: /scratch/klambert/model_log/energy_experiments/kd_32b_to_1b

SFT:
- Synthetic dataset path: /scratch/klambert/dataset/synthetic_32b_to_1b
- Env metadata, checkpoints, and model: /scratch/klambert/model_log/energy_experiments/sft_32b_to_1b
- Change dataset choice in the sft and base configs before launching scirpts!

!! unadd configs folder from the .gitignore
!! Move the energy logs from the trillium cluster into the home directory !!

Runs Checklist:
  - [X] Generate **KD teacher logits** on the TULU-3 SFT mixture (full dataset, fixed seq len).
  - [X] Generate **synthetic SFT datasets** with OLMo-2-32B-SFT:
    - [X] Instruction/general synthetic set (TULU-style prompts).
    - [X] Math synthetic set (e.g., OpenR1-Math-220k style).
    - [X] Coding synthetic set (e.g., Codeforces-CoTS style).
  - [X] Generate and cache teacher logits for TULU dataset from 32B-SFT model

- [X] **Core 3 × 3 grid (pipelines × student sizes)**  
  For each student ∈ {1B, 7B, 13B}, with fixed training budget, seq len 1024, same optimizer/schedule:
  - [X] Train **Baseline SFT** (no teacher) on TULU-3 SFT mixture.
  - [X] Train **KD Proper** using cached teacher logits on TULU-3.
    - [X] 1B
    - [X] 7B
    - [X] 13B
  - [X] Train synthetic SFT using TULU dataset
    - [X] 1B
    - [X] 7B
    - [X] 13B
  - [X] Train synthetic SFT on different datasets:
    - [X] 1B on Codeforces
    - [X] 1B on Math

- [ ] KD hyperparameter sensitivity (on 1B student)
  - [X] Temeperature Runs:
    - [X] τ = 1.0
    - [X] τ = 2.0 (launched on Killarney)
    - [X] τ = 4.0 
  - [X] Alpha Runs:
    - [ ] α = 0.3
    - [ ] α = 0.5
    - [ ] α = 0.8 (transferring to Trillium)

- [ ] Synthetic SFT hyperparameter sensitivity (on 1B student)
  - [ ] Vary max_new_tokens {256, 512, 1024} when generating synthetic data:
    - 256
        - [X] synthetic data generation
        - [X] train student on this dataset (launched Trillium)
    - 512 
        - [X] synthetic data generation
        - [X] train student on this dataset
    - 1024
        - [X] synthetic data generation
        - [X] train student on this dataset
  - [X] Vary dataset size: 
    - [X] 3.5k student training
    - [X] 7k student training

- [] True runs
  - [] SFT 1B
  - [] SFT 7B
  - [] SFT 13B

maybe also answer the question where you vary the dataset size! Maybe KD doesn't need such a large dataset after al!


- [ ] Evaluation suite for all final checkpoints
  - [ ] For each trained student in the core 3 × 3 grid (and selected sensitivity runs), run **full evaluation** with energy logging on:
    - [ ] AlpacaEval 2
    - [ ] IFEval
    - [ ] MT-Bench-101
    - [ ] GSM8K
    - [ ] MMLU
  - [ ] Save benchmark scores + evaluation energy to a unified results table for Pareto plotting.


