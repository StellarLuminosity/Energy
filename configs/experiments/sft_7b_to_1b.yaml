# Data Distillation (SFT): Teacher-Generated Synthetic Data
# Teacher generates responses, student learns via supervised fine-tuning

# Pipeline type
pipeline: "sft"

# Experiment identification
experiment:
  name: "sft_olmo2_7b_to_1b"
  description: "Data distillation: 7B teacher generates data, 1B student learns via SFT"

# Models
model:
  teacher: "allenai/OLMo-2-1124-7B-SFT"
  student: "allenai/OLMo-2-0425-1B-SFT"
  student_vocab_size: 100352

# SFT-specific settings
synthetic_data:
  # Teacher generation settings (FIXED for fair comparison)
  generation:
    temperature: 0.7
    top_p: 0.9
    max_new_tokens: 512
    decoding_strategy: "sampling"  # sampling vs greedy
  
  # Dataset for prompts
  prompt_dataset: "allenai/tulu-3-sft-mixture"
  num_samples: 50000  # Number of synthetic examples to generate
  
  # Filtering (quality control)
  filtering:
    enabled: true
    min_length: 10  # Minimum response length in tokens
    max_length: 1024
    # Additional filters can be added (e.g., perplexity threshold)
  
  # Paths
  synthetic_dataset_path: "/scratch/klambert/dataset/synthetic_7b_to_1b"
  use_existing: false  # If true, skip generation and use existing dataset

# Output
output:
  output_dir: "/scratch/klambert/model_log/energy_experiments/sft_7b_to_1b"

# W&B override
wandb:
  run_name: "sft_7b_to_1b_temp0.7_50k"

