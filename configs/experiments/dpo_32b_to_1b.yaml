# Preference Distillation (DPO): Teacher as Judge â†’ Student Policy
# Teacher judges preference pairs, student optimized with DPO

# Pipeline type
pipeline: "dpo"

# Experiment identification
experiment:
  name: "dpo_olmo2_7b_to_1b"
  description: "Preference distillation: 7B judge labels pairs, 1B learns via DPO"

# Models
model:
  teacher: "allenai/OLMo-2-0325-32B-SFT"   # used for generation of labels
  reference: "allenai/OLMo-2-0325-32B-SFT"
  policy: "allenai/OLMo-2-0425-1B-SFT"
  judge: "allenai/OLMo-2-0325-32B-SFT"
  
  student_vocab_size: 100352

# DPO-specific settings
dpo:
  # DPO hyperparameters
  beta: 0.1  # Temperature parameter for DPO loss
  # Teacher generation settings for preference pairs
  judge_labeling:
    temperature: 0.7
    top_p: 0.9
    max_new_tokens: 256
    scoring_method: "likelihood"

# Output
output:
  output_dir: "/scratch/klambert/model_log/energy_experiments/dpo_7b_to_1b"

# W&B override
wandb:
  run_name: "dpo_7b_to_1b_beta0.1_ultrafeedback"
