# Base Configuration for Distillation Experiments
# These settings are FIXED across all experiments for valid comparison

# Experiment metadata
experiment:
  seed: 42
  debug_mode: false
  debug_max_steps: 40

# Dataset configuration
data:
  dataset_choice: "tulu"  # One of: tulu, codeforces, math
  dataset_name: "allenai/tulu-3-sft-mixture"
  dataset_path: "/path/to/datasets/tulu-3-sft-mixture-preprocessed"
  tokenizer_name: "allenai/OLMo-2-1124-7B-SFT"
  max_sequence_length: 1024
  pad_token_id: 100277  # OLMo-2 pad token
  datasets:
    tulu:
      dataset_name: "allenai/tulu-3-sft-mixture"
      dataset_path: "/path/to/datasets/tulu-3-sft-mixture-preprocessed"
    codeforces:
      dataset_name: "open-r1/codeforces-cots"
      dataset_subset: "solutions"
      dataset_path: "/path/to/datasets/open-r1-codeforces-preprocessed"
    math:
      dataset_name: "open-r1/OpenR1-Math-220k"
      dataset_split: "train"
      dataset_path: "/path/to/datasets/openr1-math-220k-preprocessed"

# Synthetic data generation defaults
synthetic_data:
  # Dataset for prompts
  num_samples: 20000  # Max Number of dataset examples to process
  max_gen_examples: 7000 # Number of synthetic examples to generate

# Training configuration
training:
  token_budget: 10_000_000_000  # 10B tokens
  
  # Batch configuration
  batch_size: 4
  eval_batch_size: 1
  gradient_accumulation_steps: 16
  
  # Optimizer
  optimizer: "adafactor"
  learning_rate: 5.0e-5
  max_grad_norm: 1.0
  
  # LR schedule
  num_warmup_steps: 100
  schedule_type: "cosine"  # cosine with warmup
  
  # Precision
  dtype: "bfloat16"
  
  # Training control
  num_epochs: 20
  num_training_steps: 0  # 0 = use epochs instead
  
  # Checkpointing
  save_steps: 200
  eval_steps: 100

# Energy tracking configuration
energy:
  enabled: true
  # NVML polling interval for GPU power
  nvml_poll_interval_ms: 500
  # Track CPU energy (via RAPL if available)
  track_cpu: true
  # CodeCarbon settings
  country_iso_code: "CAN"
  offline_mode: true
  rapl_root: "/sys/class/powercap/intel-rapl"
  total_energy_policy: "measured" # allowed: "measured" (default), "codecarbon", "gpu_only"

# Weights & Biases logging
wandb:
  enabled: true
  project: "distillation-energy-benchmark"
  entity: null  # Set in experiment config or via env var
  run_name: null  # Auto-generated if null
  log_interval: 10

# Hardware assertions (optional - for validating consistent hardware)
hardware:
  # If specified, will check that experiments run on matching hardware
  assert_gpu_name: null  # e.g., "NVIDIA L40S"
  assert_gpu_count: null  # e.g., 2
  assert_cpu_brand: null  # e.g., "Intel(R) Xeon(R)"

benchmark:
  # Root folder for all benchmark runs.
  output_dir: "/path/to/model_log/olmo_benchmarks"
  # Default model to evaluate. This can be either:
  #  - a Hugging Face id: "allenai/OLMo-2-1124-7B-SFT"
  #  - or a local HF-format directory path with config.json, model.safetensors, etc.
  model: "/path/to/model_log/energy_experiments/kd_32b_to_13b/checkpoints/checkpoint_epoch0_step13200.pt" # "allenai/OLMo-2-0325-32B-SFT"
  model_type: 'allenai/OLMo-2-1124-13B-SFT'
  subfolder_name: "olmo_benchmark_13b_trained_adafactor"
  mt_bench_101:
    data_path: "benchmarks/mt_bench_101/mtbench101.jsonl"
  tasks:
  - "core_9mcqa::olmes"
  # - "mmlu:mc::olmes"
  # - "olmo_2_generative::olmes"
  # - "olmo_2_heldout::olmes"

output:
  run_dir: "./logs"
