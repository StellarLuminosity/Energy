# Base Configuration for Distillation Experiments
# These settings are FIXED across all experiments for valid comparison

# Experiment metadata
experiment:
  seed: 42
  debug_mode: false
  debug_max_steps: 40

# Dataset configuration
data:
  dataset_name: "allenai/tulu-3-sft-mixture"
  # For KD: path to preprocessed data with teacher logits
  dataset_path: "/scratch/klambert/dataset/logprob_cache/teacher_logprobs" # if logits are cached
  tokenizer_name: "allenai/OLMo-2-1124-7B-SFT"
  max_sequence_length: 1024
  pad_token_id: 100277  # OLMo-2 pad token

# Training configuration
training:
  token_budget: 10_000_000_000  # 10B tokens (adjust as needed)
  
  # Batch configuration
  batch_size: 4
  eval_batch_size: 2
  gradient_accumulation_steps: 16
  
  # Optimizer
  optimizer: "adamw"
  learning_rate: 5.0e-5
  max_grad_norm: 1.0
  
  # LR schedule
  num_warmup_steps: 100
  schedule_type: "cosine"  # cosine with warmup
  
  # Precision
  mixed_precision: true
  dtype: "bfloat16"
  
  # Training control
  num_epochs: 2
  num_training_steps: 0  # 0 = use epochs instead
  
  # Checkpointing
  save_steps: 200
  eval_steps: 100
  resume_from_checkpoint: false

# Energy tracking configuration
energy:
  enabled: true
  # NVML polling interval for GPU power
  nvml_poll_interval_ms: 500
  # Track CPU energy (via experiment-impact-tracker)
  track_cpu: true
  # CodeCarbon settings
  country_iso_code: "USA"
  offline_mode: true

# Weights & Biases logging
wandb:
  enabled: true
  project: "distillation-energy-benchmark"
  entity: null  # Set in experiment config or via env var
  run_name: null  # Auto-generated if null
  log_interval: 10

# Hardware assertions (optional - for validating consistent hardware)
hardware:
  # If specified, will check that experiments run on matching hardware
  assert_gpu_name: null  # e.g., "NVIDIA L40S"
  assert_gpu_count: null  # e.g., 2
  assert_cpu_brand: null  # e.g., "Intel(R) Xeon(R)"

