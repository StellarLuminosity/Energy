{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5649a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_ROOTS = [\n",
    "    Path(\"logs\"),\n",
    "    Path(\"trillium-logs\"),\n",
    "    Path(\"runpod2_logs\"),\n",
    "]\n",
    "ROOT_CLUSTER = {\n",
    "    \"logs\": \"killarney\",\n",
    "    \"trillium-logs\": \"trillium\",\n",
    "    \"runpod2_logs\": \"runpod\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3936c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codecarbon Helper\n",
    "\n",
    "def load_codecarbon_logs(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CodeCarbon emissions.csv from each root into a single DataFrame.\n",
    "\n",
    "    Returns columns including:\n",
    "        root, cluster, project_name, experiment_id,\n",
    "        duration, cpu_energy, gpu_energy, ram_energy, energy_consumed, emissions, ...\n",
    "    \"\"\"\n",
    "    cc_rows = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cc_dir = root / \"codecarbon\"\n",
    "        if not cc_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Prefer the main emissions.csv; ignore .bak variants here\n",
    "        cc_path = cc_dir / \"emissions.csv\"\n",
    "        if not cc_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(cc_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read CodeCarbon CSV at {cc_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"root\"] = str(root)\n",
    "        df[\"cluster\"] = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        cc_rows.append(df)\n",
    "\n",
    "    if not cc_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cc_df = pd.concat(cc_rows, ignore_index=True)\n",
    "\n",
    "    # Normalize names we use often\n",
    "    cc_df.rename(\n",
    "        columns={\n",
    "            \"energy_consumed\": \"energy_consumed_kwh\",\n",
    "            \"cpu_energy\": \"cpu_energy_kwh\",\n",
    "            \"gpu_energy\": \"gpu_energy_kwh\",\n",
    "            \"ram_energy\": \"ram_energy_kwh\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return cc_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage Metrics Normalization\n",
    "\n",
    "STAGE_DEFAULTS: Dict[str, Any] = {\n",
    "    # identity / meta\n",
    "    \"root\": None,\n",
    "    \"cluster\": None,\n",
    "    \"stage_dir\": None,\n",
    "    \"experiment_id\": None,\n",
    "    \"experiment_name\": None,\n",
    "    \"stage_id\": None,\n",
    "    \"stage_name\": None,\n",
    "    \"source\": None,  # \"summary\", \"stage_json\", \"snapshot\", \"codecarbon_only\"\n",
    "\n",
    "    # snapshot info\n",
    "    \"is_snapshot\": False,\n",
    "    \"snapshot_step\": None,\n",
    "    \"snapshot_type\": None,\n",
    "    \"snapshot_time\": None,\n",
    "\n",
    "    # config metadata\n",
    "    \"total_energy_policy\": None,\n",
    "    \"pipeline\": None,\n",
    "    \"student_size\": None,\n",
    "    \"dataset_choice\": None,\n",
    "    \"kd_temperature\": None,\n",
    "    \"kd_alpha\": None,\n",
    "    \"sft_max_new_tokens\": None,\n",
    "\n",
    "    # timing / tokens\n",
    "    \"start_time\": None,\n",
    "    \"end_time\": None,\n",
    "    \"duration_seconds\": None,\n",
    "    \"tokens_processed\": None,\n",
    "    \"tokens_per_second\": None,\n",
    "\n",
    "    # GPU metrics\n",
    "    \"gpu_energy_joules\": None,\n",
    "    \"gpu_avg_power_watts\": None,\n",
    "    \"gpu_peak_power_watts\": None,\n",
    "    \"nvml_poll_interval_ms\": None,\n",
    "\n",
    "    # CPU + total\n",
    "    \"cpu_energy_joules\": None,\n",
    "    \"total_energy_joules\": None,\n",
    "    \"total_energy_kwh\": None,\n",
    "    \"joules_per_token\": None,\n",
    "    \"kwh_total\": None,\n",
    "\n",
    "    # CodeCarbon normalized\n",
    "    \"total_codecarbon_energy_kwh\": None,\n",
    "    \"codecarbon_emissions_kg\": None,\n",
    "    \"codecarbon_cpu_energy_kwh\": None,\n",
    "    \"codecarbon_gpu_energy_kwh\": None,\n",
    "    \"codecarbon_ram_energy_kwh\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_stage_metrics_dict(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize a StageMetrics-like dict (from stage JSON or experiment_summary)\n",
    "    into the canonical keys in STAGE_DEFAULTS (no root/cluster/stage_dir/source).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # Basic identifiers\n",
    "    out[\"stage_id\"] = raw.get(\"stage_id\")\n",
    "    out[\"stage_name\"] = raw.get(\"stage_name\")\n",
    "\n",
    "    # Timing / tokens\n",
    "    out[\"start_time\"] = raw.get(\"start_time\")\n",
    "    out[\"end_time\"] = raw.get(\"end_time\")\n",
    "    out[\"duration_seconds\"] = raw.get(\"duration_seconds\")\n",
    "    out[\"tokens_processed\"] = raw.get(\"tokens_processed\")\n",
    "    out[\"tokens_per_second\"] = raw.get(\"tokens_per_second\")\n",
    "\n",
    "    # GPU\n",
    "    out[\"gpu_energy_joules\"] = raw.get(\"gpu_energy_joules\")\n",
    "    out[\"gpu_avg_power_watts\"] = raw.get(\"gpu_avg_power_watts\")\n",
    "    out[\"gpu_peak_power_watts\"] = raw.get(\"gpu_peak_power_watts\")\n",
    "    out[\"nvml_poll_interval_ms\"] = raw.get(\"nvml_poll_interval_ms\")\n",
    "\n",
    "    # CPU\n",
    "    out[\"cpu_energy_joules\"] = raw.get(\"cpu_energy_joules\")\n",
    "\n",
    "    # CodeCarbon variants:\n",
    "    # new-style: total_codecarbon_energy_kwh\n",
    "    # old-style:  codecarbon_energy_kwh\n",
    "    cc_total = raw.get(\"total_codecarbon_energy_kwh\", None)\n",
    "    if cc_total is None:\n",
    "        cc_total = raw.get(\"codecarbon_energy_kwh\", None)\n",
    "    out[\"total_codecarbon_energy_kwh\"] = cc_total\n",
    "\n",
    "    out[\"codecarbon_emissions_kg\"] = raw.get(\"codecarbon_emissions_kg\")\n",
    "    out[\"codecarbon_cpu_energy_kwh\"] = raw.get(\"codecarbon_cpu_energy_kwh\")\n",
    "    out[\"codecarbon_gpu_energy_kwh\"] = raw.get(\"codecarbon_gpu_energy_kwh\")\n",
    "    out[\"codecarbon_ram_energy_kwh\"] = raw.get(\"codecarbon_ram_energy_kwh\")\n",
    "\n",
    "    # Totals / derived\n",
    "    out[\"total_energy_joules\"] = raw.get(\"total_energy_joules\")\n",
    "    out[\"total_energy_kwh\"] = raw.get(\"total_energy_kwh\")\n",
    "    out[\"joules_per_token\"] = raw.get(\"joules_per_token\")\n",
    "    out[\"kwh_total\"] = raw.get(\"kwh_total\")\n",
    "\n",
    "    # Snapshot info (may or may not be present)\n",
    "    out[\"is_snapshot\"] = bool(raw.get(\"snapshot\", False))\n",
    "    out[\"snapshot_step\"] = raw.get(\"snapshot_step\")\n",
    "    out[\"snapshot_type\"] = raw.get(\"snapshot_type\")\n",
    "    out[\"snapshot_time\"] = raw.get(\"snapshot_time\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80eee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Metadata extraction\n",
    "\n",
    "def _infer_pipeline_and_student(exp_name: str) -> (Optional[str], Optional[str]):\n",
    "    s = exp_name.lower()\n",
    "    pipeline = None\n",
    "    if s.startswith(\"kd_\"):\n",
    "        pipeline = \"kd\"\n",
    "    elif s.startswith(\"sft_\"):\n",
    "        pipeline = \"sft\"\n",
    "    elif \"true\" in s:\n",
    "        pipeline = \"true_sft\"\n",
    "\n",
    "    student_size = None\n",
    "    if \"to_1b\" in s:\n",
    "        student_size = \"1B\"\n",
    "    elif \"to_7b\" in s:\n",
    "        student_size = \"7B\"\n",
    "    elif \"to_13b\" in s or \"13b\" in s:\n",
    "        student_size = \"13B\"\n",
    "\n",
    "    return pipeline, student_size\n",
    "\n",
    "\n",
    "def load_config_meta(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan all config_*.json files and extract per-(root, stage_dir, stage_name) metadata:\n",
    "        experiment_name, total_energy_policy, pipeline, student_size, kd_temperature, kd_alpha,\n",
    "        sft_max_new_tokens, dataset_choice, etc.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        for cfg_path in root.rglob(\"config_*.json\"):\n",
    "            try:\n",
    "                with open(cfg_path) as f:\n",
    "                    cfg = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read config at {cfg_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            stage_name = cfg.get(\"stage_name\")\n",
    "            stage_id = cfg.get(\"stage_id\")\n",
    "\n",
    "            config = cfg.get(\"config\", {})\n",
    "            exp_cfg = config.get(\"experiment\", {})\n",
    "            data_cfg = config.get(\"data\", {})\n",
    "            train_cfg = config.get(\"training\", {})\n",
    "            kd_cfg = config.get(\"kd\", config.get(\"distillation\", {}))  # handle naming\n",
    "            energy_cfg = config.get(\"energy\", {})\n",
    "\n",
    "            exp_name = exp_cfg.get(\"name\", stage_name)\n",
    "            pipeline, student_size = _infer_pipeline_and_student(exp_name)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"root\": str(root),\n",
    "                    \"cluster\": cluster,\n",
    "                    \"stage_dir\": str(cfg_path.parent),\n",
    "                    \"stage_name\": stage_name,\n",
    "                    \"stage_id\": stage_id,\n",
    "                    \"experiment_name\": exp_name,\n",
    "                    \"total_energy_policy\": energy_cfg.get(\"total_energy_policy\"),\n",
    "                    \"pipeline\": pipeline,\n",
    "                    \"student_size\": student_size,\n",
    "                    \"dataset_choice\": data_cfg.get(\"dataset_choice\"),\n",
    "                    \"kd_temperature\": kd_cfg.get(\"temperature\"),\n",
    "                    \"kd_alpha\": kd_cfg.get(\"alpha\"),\n",
    "                    \"sft_max_new_tokens\": train_cfg.get(\"max_new_tokens\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage folder -> records\n",
    "\n",
    "def _collect_from_experiment_summary(\n",
    "    summary_path: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Given an experiment_summary.json, return a list of normalized stage records (source='summary').\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    exp_id = summary.get(\"experiment_id\")\n",
    "    exp_name = summary.get(\"experiment_name\")\n",
    "    stages = summary.get(\"stages\", {})\n",
    "\n",
    "    for stage_name, raw in stages.items():\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        # Default: parent of the summary (e.g., run_dir); overridden if config meta is found\n",
    "        base[\"stage_dir\"] = str(summary_path.parent)\n",
    "        base[\"experiment_id\"] = exp_id\n",
    "        base[\"experiment_name\"] = exp_name\n",
    "        base[\"source\"] = \"summary\"\n",
    "\n",
    "        # Normalize metrics\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # Attach config meta if available.\n",
    "        # Match by root + stage_name, then prefer the config's stage_dir.\n",
    "        m = cfg_meta[\n",
    "            (cfg_meta[\"root\"] == str(root))\n",
    "            & (cfg_meta[\"stage_name\"] == stage_name)\n",
    "        ]\n",
    "        if not m.empty:\n",
    "            meta_row = m.iloc[0].to_dict()\n",
    "\n",
    "            # Prefer the config's notion of the stage_dir (actual stage folder)\n",
    "            stage_dir_cfg = meta_row.get(\"stage_dir\")\n",
    "            if stage_dir_cfg:\n",
    "                base[\"stage_dir\"] = stage_dir_cfg\n",
    "\n",
    "            # Optionally override stage_id if missing\n",
    "            if base.get(\"stage_id\") is None and meta_row.get(\"stage_id\"):\n",
    "                base[\"stage_id\"] = meta_row[\"stage_id\"]\n",
    "\n",
    "            for k in [\n",
    "                \"total_energy_policy\",\n",
    "                \"pipeline\",\n",
    "                \"student_size\",\n",
    "                \"dataset_choice\",\n",
    "                \"kd_temperature\",\n",
    "                \"kd_alpha\",\n",
    "                \"sft_max_new_tokens\",\n",
    "            ]:\n",
    "                base[k] = meta_row.get(k)\n",
    "\n",
    "        records.append(base)\n",
    "\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def _is_stage_metrics_json(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: JSON files that look like StageMetrics but are not config/env/summary.\n",
    "    Includes snapshots.\n",
    "    \"\"\"\n",
    "    name = path.name\n",
    "    if not name.endswith(\".json\"):\n",
    "        return False\n",
    "    if name.startswith(\"config_\") or name.startswith(\"environment_\"):\n",
    "        return False\n",
    "    if name == \"experiment_summary.json\":\n",
    "        return False\n",
    "    # This will match stage.json and stage__step_*.json (snapshots)\n",
    "    return True\n",
    "\n",
    "\n",
    "def _collect_stage_jsons_in_dir(\n",
    "    stage_dir: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect stage records from standalone stage JSON files in a stage directory,\n",
    "    aggregate snapshots, and return ONE row per logical stage.\n",
    "\n",
    "    Rules:\n",
    "      - If a final stage JSON exists (source='stage_json', not a snapshot),\n",
    "        use that as the base row.\n",
    "      - If only snapshots exist, pick the latest snapshot (by snapshot_step, then end_time).\n",
    "      - For stages with both final and snapshots, final wins; we can still\n",
    "        use the last snapshot to fill missing fields if needed.\n",
    "    \"\"\"\n",
    "    # Match config for this directory (pipeline, student_size, etc.)\n",
    "    m_dir = cfg_meta[\n",
    "        (cfg_meta[\"root\"] == str(root)) & (cfg_meta[\"stage_dir\"] == str(stage_dir))\n",
    "    ]\n",
    "    cfg_row = m_dir.iloc[0].to_dict() if not m_dir.empty else {}\n",
    "\n",
    "    stage_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for path in stage_dir.glob(\"*.json\"):\n",
    "        if not _is_stage_metrics_json(path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                raw = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read stage JSON at {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Skip JSONs that aren't dicts (or single-element list of dict)\n",
    "        if isinstance(raw, list):\n",
    "            if len(raw) == 1 and isinstance(raw[0], dict):\n",
    "                raw = raw[0]\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[INFO] Skipping JSON at {path} \"\n",
    "                    f\"(top-level list, not a StageMetrics dict)\"\n",
    "                )\n",
    "                continue\n",
    "        elif not isinstance(raw, dict):\n",
    "            print(\n",
    "                f\"[INFO] Skipping JSON at {path} \"\n",
    "                f\"(top-level {type(raw).__name__}, expected dict)\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        base[\"stage_dir\"] = str(stage_dir)\n",
    "        base[\"experiment_name\"] = cfg_row.get(\"experiment_name\")\n",
    "        base[\"source\"] = \"snapshot\" if raw.get(\"snapshot\") else \"stage_json\"\n",
    "\n",
    "        # Normalize StageMetrics-style dict into our standard fields\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # If JSON didn't carry stage_name, fall back to folder name\n",
    "        if not base.get(\"stage_name\"):\n",
    "            base[\"stage_name\"] = stage_dir.name\n",
    "\n",
    "        # Attach config meta\n",
    "        for k in [\n",
    "            \"total_energy_policy\",\n",
    "            \"pipeline\",\n",
    "            \"student_size\",\n",
    "            \"dataset_choice\",\n",
    "            \"kd_temperature\",\n",
    "            \"kd_alpha\",\n",
    "            \"sft_max_new_tokens\",\n",
    "        ]:\n",
    "            base[k] = cfg_row.get(k)\n",
    "\n",
    "        stage_records.append(base)\n",
    "\n",
    "    if not stage_records:\n",
    "        return []\n",
    "\n",
    "    # --- Aggregate to ONE row per logical stage in this directory ---\n",
    "\n",
    "    by_stage: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for rec in stage_records:\n",
    "        key = rec.get(\"stage_id\") or rec[\"stage_name\"]\n",
    "        by_stage.setdefault(key, []).append(rec)\n",
    "\n",
    "    aggregated: List[Dict[str, Any]] = []\n",
    "\n",
    "    for key, recs in by_stage.items():\n",
    "        finals = [\n",
    "            r\n",
    "            for r in recs\n",
    "            if r.get(\"source\") != \"snapshot\" and not r.get(\"is_snapshot\", False)\n",
    "        ]\n",
    "        snapshots = [r for r in recs if r.get(\"source\") == \"snapshot\"]\n",
    "\n",
    "        if finals:\n",
    "            # Prefer the final metrics JSON; if multiple, take the one with the latest end_time.\n",
    "            best = max(finals, key=lambda r: (r.get(\"end_time\") or 0.0))\n",
    "\n",
    "            # Optional: use the latest snapshot as a fallback for missing fields.\n",
    "            if snapshots:\n",
    "                snaps_sorted = sorted(\n",
    "                    snapshots,\n",
    "                    key=lambda r: (\n",
    "                        r.get(\"snapshot_step\") if r.get(\"snapshot_step\") is not None else -1,\n",
    "                        r.get(\"end_time\") or 0.0,\n",
    "                    ),\n",
    "                )\n",
    "                last_snap = snaps_sorted[-1]\n",
    "                for field in STAGE_DEFAULTS.keys():\n",
    "                    if best.get(field) in (None, 0) and last_snap.get(field) not in (None, 0):\n",
    "                        best[field] = last_snap[field]\n",
    "\n",
    "            aggregated.append(best)\n",
    "        else:\n",
    "            # No final file: only snapshots. Pick the latest snapshot as the representative row.\n",
    "            snaps_sorted = sorted(\n",
    "                recs,\n",
    "                key=lambda r: (\n",
    "                    r.get(\"snapshot_step\") if r.get(\"snapshot_step\") is not None else -1,\n",
    "                    r.get(\"end_time\") or 0.0,\n",
    "                ),\n",
    "            )\n",
    "            aggregated.append(snaps_sorted[-1])\n",
    "\n",
    "    return aggregated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-stage Dataframe\n",
    "\n",
    "def build_stage_dataframe(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main entry point:\n",
    "      - loads config metadata,\n",
    "      - walks all log roots,\n",
    "      - collects StageMetrics from experiment_summary.json and individual stage JSONs,\n",
    "      - returns one big DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    cfg_meta = load_config_meta(log_roots)\n",
    "    cc_df = load_codecarbon_logs(log_roots)  # not yet used as fallback, but available\n",
    "\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        # 1) experiment_summary.json files (per run)\n",
    "        for summary_path in root.rglob(\"experiment_summary.json\"):\n",
    "            # Skip copies written into individual stage dirs:\n",
    "            # .../<root>/stages/<stage>/experiment_summary.json\n",
    "            parent = summary_path.parent\n",
    "            if parent.parent.name == \"stages\":\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(summary_path) as f:\n",
    "                    summary = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read experiment_summary at {summary_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if \"stages\" in summary:\n",
    "                all_records.extend(\n",
    "                    _collect_from_experiment_summary(summary_path, root, cluster, cfg_meta)\n",
    "                )\n",
    "            else:\n",
    "                # Some summaries might be in an older/global format; skip or handle specially.\n",
    "                pass\n",
    "\n",
    "\n",
    "        # 2) Standalone stage directories: often under root/stages/*, but also\n",
    "        for stage_dir in root.rglob(\"*\"):\n",
    "            if not stage_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Skip the container folder itself (we only want its children)\n",
    "            if stage_dir == root / \"stages\":\n",
    "                continue\n",
    "\n",
    "            # Heuristic: a \"stage dir\" is one that contains some StageMetrics JSON\n",
    "            has_stage_json = any(_is_stage_metrics_json(p) for p in stage_dir.glob(\"*.json\"))\n",
    "            if not has_stage_json:\n",
    "                continue\n",
    "\n",
    "            records = _collect_stage_jsons_in_dir(stage_dir, root, cluster, cfg_meta)\n",
    "            all_records.extend(records)\n",
    "\n",
    "\n",
    "    if not all_records:\n",
    "        return pd.DataFrame(columns=STAGE_DEFAULTS.keys())\n",
    "\n",
    "    stage_df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Optional: deduplicate (e.g., you might want to drop stage_json records\n",
    "    # that correspond exactly to summary records). For now, keep everything\n",
    "    # and let later analysis decide which to use.\n",
    "    return stage_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_dataframe_for_path(path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience helper to build a standardized stage DataFrame for a specific\n",
    "    log root or stage directory.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs\")\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    - build_stage_dataframe_for_path(\"/abs/path/to/runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    \"\"\"\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # If they passed a specific stage dir under .../stages/<stage_name>\n",
    "    if path.is_dir() and path.name != \"stages\" and path.parent.name == \"stages\":\n",
    "        # /.../<log_root>/stages/<stage_name>\n",
    "        # For /project/.../Energy/runpod2_logs/stages/sft_32b_to_1b_math_nosft\n",
    "        # we want log_root = /project/.../Energy/runpod2_logs\n",
    "        log_root = path.parent.parent  # == path.parents[1]\n",
    "        filter_prefix = str(path)\n",
    "    elif path.is_dir() and path.name == \"stages\":\n",
    "        # They pointed at the stages/ directory: restrict to that subtree\n",
    "        log_root = path.parent\n",
    "        filter_prefix = str(path)\n",
    "    else:\n",
    "        # Treat as a log root\n",
    "        log_root = path\n",
    "        filter_prefix = str(log_root)\n",
    "\n",
    "    df = build_stage_dataframe([log_root])\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # If they gave a root, no extra filtering\n",
    "    if filter_prefix == str(log_root):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    # Otherwise restrict to that specific stage subtree\n",
    "    stage_dirs = df[\"stage_dir\"].astype(str)\n",
    "    mask = stage_dirs.str.startswith(filter_prefix)\n",
    "    return df[mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose what you want to process\n",
    "# - Leave `paths` empty to use default LOG_ROOTS\n",
    "# - Or set it to one or more specific paths, e.g. a single stage dir\n",
    "paths = [\n",
    "    \"/home/klambert/projects/aip-craffel/klambert/Energy/runpod2_logs/\",\n",
    "    \"/home/klambert/projects/aip-craffel/klambert/Energy/logs\",\n",
    "    \"/home/klambert/projects/aip-craffel/klambert/Energy/trillium-logs\",    \n",
    "    ]\n",
    "output = \"stage_metrics.csv\"\n",
    "\n",
    "if paths:\n",
    "    dfs = [build_stage_dataframe_for_path(p) for p in paths]\n",
    "    df = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]\n",
    "else:\n",
    "    df = build_stage_dataframe(LOG_ROOTS)\n",
    "\n",
    "output_path = Path(output)\n",
    "file_exists = output_path.exists()\n",
    "\n",
    "display(df.head())\n",
    "df.to_csv(\n",
    "    output_path,\n",
    "    mode=\"a\" if file_exists else \"w\",   # append if exists, else write\n",
    "    header=not file_exists,            # write header only if new file\n",
    "    index=False,\n",
    ")\n",
    "print(f\"Saved {output} with {len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a465a0a",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49327384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de091677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded stage_metrics.csv with 22 rows.\n",
      "stage_df_all:   22 rows\n",
      "stage_df_clean: 18 rows (tokens_processed > 0 where available)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>root</th>\n",
       "      <th>cluster</th>\n",
       "      <th>stage_dir</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>stage_id</th>\n",
       "      <th>stage_name</th>\n",
       "      <th>source</th>\n",
       "      <th>is_snapshot</th>\n",
       "      <th>snapshot_step</th>\n",
       "      <th>...</th>\n",
       "      <th>total_codecarbon_energy_kwh</th>\n",
       "      <th>codecarbon_emissions_kg</th>\n",
       "      <th>codecarbon_cpu_energy_kwh</th>\n",
       "      <th>codecarbon_gpu_energy_kwh</th>\n",
       "      <th>codecarbon_ram_energy_kwh</th>\n",
       "      <th>energy_kwh</th>\n",
       "      <th>gpu_energy_kwh</th>\n",
       "      <th>cpu_energy_kwh</th>\n",
       "      <th>energy_j_per_token</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/6104653/klambert/Energy/runpod2_logs</td>\n",
       "      <td>runpod</td>\n",
       "      <td>/project/6104653/klambert/Energy/runpod2_logs/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sft_32b_to_13b_tulu_nosft</td>\n",
       "      <td>sft_32b_to_13b_tulu_nosft</td>\n",
       "      <td>sft_32b_to_13b_tulu_nosft</td>\n",
       "      <td>stage_json</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716492</td>\n",
       "      <td>0.121834</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.701760</td>\n",
       "      <td>0.010234</td>\n",
       "      <td>0.706382</td>\n",
       "      <td>0.701883</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.257970</td>\n",
       "      <td>2584.730090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/project/6104653/klambert/Energy/logs</td>\n",
       "      <td>killarney</td>\n",
       "      <td>/project/6104653/klambert/Energy/logs/stages/c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sft_olmo2_7b_to_1b</td>\n",
       "      <td>codeforces_cots_preprocess_codeforces_cots_pre...</td>\n",
       "      <td>codeforces_cots_preprocess_codeforces_cots_pre...</td>\n",
       "      <td>stage_json</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126436</td>\n",
       "      <td>978.473355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/project/6104653/klambert/Energy/logs</td>\n",
       "      <td>killarney</td>\n",
       "      <td>/project/6104653/klambert/Energy/logs/stages/k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kd_olmo2_32b_to_13b_nosft</td>\n",
       "      <td>kd_olmo2_32b_to_13b_nosft</td>\n",
       "      <td>kd_olmo2_32b_to_13b_nosft</td>\n",
       "      <td>snapshot</td>\n",
       "      <td>True</td>\n",
       "      <td>22200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>0.350545</td>\n",
       "      <td>0.007219</td>\n",
       "      <td>11.082666</td>\n",
       "      <td>11.031604</td>\n",
       "      <td>0.051062</td>\n",
       "      <td>1.259227</td>\n",
       "      <td>371.081407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/project/6104653/klambert/Energy/logs</td>\n",
       "      <td>killarney</td>\n",
       "      <td>/project/6104653/klambert/Energy/logs/stages/k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kd_olmo2_32b_to_1b</td>\n",
       "      <td>kd_olmo2_32b_to_1b</td>\n",
       "      <td>kd_olmo2_32b_to_1b</td>\n",
       "      <td>stage_json</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182769</td>\n",
       "      <td>0.031079</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>0.149456</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.180505</td>\n",
       "      <td>0.149366</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>0.565676</td>\n",
       "      <td>435.515455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/project/6104653/klambert/Energy/logs</td>\n",
       "      <td>killarney</td>\n",
       "      <td>/project/6104653/klambert/Energy/logs/stages/k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kd_olmo2_32b_to_1b_nosft</td>\n",
       "      <td>kd_olmo2_32b_to_1b_nosft</td>\n",
       "      <td>kd_olmo2_32b_to_1b_nosft</td>\n",
       "      <td>snapshot</td>\n",
       "      <td>True</td>\n",
       "      <td>50800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.210226</td>\n",
       "      <td>5.210226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258287</td>\n",
       "      <td>843.604915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            root    cluster  \\\n",
       "0  /project/6104653/klambert/Energy/runpod2_logs     runpod   \n",
       "4          /project/6104653/klambert/Energy/logs  killarney   \n",
       "5          /project/6104653/klambert/Energy/logs  killarney   \n",
       "6          /project/6104653/klambert/Energy/logs  killarney   \n",
       "7          /project/6104653/klambert/Energy/logs  killarney   \n",
       "\n",
       "                                           stage_dir experiment_id  \\\n",
       "0  /project/6104653/klambert/Energy/runpod2_logs/...           NaN   \n",
       "4  /project/6104653/klambert/Energy/logs/stages/c...           NaN   \n",
       "5  /project/6104653/klambert/Energy/logs/stages/k...           NaN   \n",
       "6  /project/6104653/klambert/Energy/logs/stages/k...           NaN   \n",
       "7  /project/6104653/klambert/Energy/logs/stages/k...           NaN   \n",
       "\n",
       "             experiment_name  \\\n",
       "0  sft_32b_to_13b_tulu_nosft   \n",
       "4         sft_olmo2_7b_to_1b   \n",
       "5  kd_olmo2_32b_to_13b_nosft   \n",
       "6         kd_olmo2_32b_to_1b   \n",
       "7   kd_olmo2_32b_to_1b_nosft   \n",
       "\n",
       "                                            stage_id  \\\n",
       "0                          sft_32b_to_13b_tulu_nosft   \n",
       "4  codeforces_cots_preprocess_codeforces_cots_pre...   \n",
       "5                          kd_olmo2_32b_to_13b_nosft   \n",
       "6                                 kd_olmo2_32b_to_1b   \n",
       "7                           kd_olmo2_32b_to_1b_nosft   \n",
       "\n",
       "                                          stage_name      source  is_snapshot  \\\n",
       "0                          sft_32b_to_13b_tulu_nosft  stage_json        False   \n",
       "4  codeforces_cots_preprocess_codeforces_cots_pre...  stage_json        False   \n",
       "5                          kd_olmo2_32b_to_13b_nosft    snapshot         True   \n",
       "6                                 kd_olmo2_32b_to_1b  stage_json        False   \n",
       "7                           kd_olmo2_32b_to_1b_nosft    snapshot         True   \n",
       "\n",
       "   snapshot_step  ... total_codecarbon_energy_kwh codecarbon_emissions_kg  \\\n",
       "0            NaN  ...                    0.716492                0.121834   \n",
       "4            NaN  ...                    0.002873                0.000488   \n",
       "5        22200.0  ...                    0.408826                0.000000   \n",
       "6            NaN  ...                    0.182769                0.031079   \n",
       "7        50800.0  ...                    0.000000                0.000000   \n",
       "\n",
       "  codecarbon_cpu_energy_kwh codecarbon_gpu_energy_kwh  \\\n",
       "0                  0.004499                  0.701760   \n",
       "4                       NaN                       NaN   \n",
       "5                  0.051062                  0.350545   \n",
       "6                  0.031140                  0.149456   \n",
       "7                  0.000000                  0.000000   \n",
       "\n",
       "  codecarbon_ram_energy_kwh energy_kwh  gpu_energy_kwh  cpu_energy_kwh  \\\n",
       "0                  0.010234   0.706382        0.701883        0.004499   \n",
       "4                       NaN   0.001675        0.001675        0.000000   \n",
       "5                  0.007219  11.082666       11.031604        0.051062   \n",
       "6                  0.002173   0.180505        0.149366        0.031140   \n",
       "7                  0.000000   5.210226        5.210226        0.000000   \n",
       "\n",
       "   energy_j_per_token  tokens_per_sec  \n",
       "0            0.257970     2584.730090  \n",
       "4            0.126436      978.473355  \n",
       "5            1.259227      371.081407  \n",
       "6            0.565676      435.515455  \n",
       "7            0.258287      843.604915  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the aggregated stage metrics CSV\n",
    "if \"output_path\" in globals():\n",
    "    stage_metrics_path = output_path\n",
    "else:\n",
    "    stage_metrics_path = Path(\"stage_metrics.csv\")\n",
    "\n",
    "stage_df_raw = pd.read_csv(stage_metrics_path)\n",
    "print(f\"Loaded {stage_metrics_path} with {len(stage_df_raw)} rows.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Ensure numeric dtypes for core metric columns\n",
    "# -------------------------------------------------------------------------\n",
    "numeric_cols = [\n",
    "    \"duration_seconds\",\n",
    "    \"tokens_processed\",\n",
    "    \"tokens_per_second\",\n",
    "    \"gpu_energy_joules\",\n",
    "    \"gpu_avg_power_watts\",\n",
    "    \"gpu_peak_power_watts\",\n",
    "    \"nvml_poll_interval_ms\",\n",
    "    \"cpu_energy_joules\",\n",
    "    \"total_energy_joules\",\n",
    "    \"total_energy_kwh\",\n",
    "    \"joules_per_token\",\n",
    "    \"kwh_total\",\n",
    "    \"total_codecarbon_energy_kwh\",\n",
    "    \"codecarbon_emissions_kg\",\n",
    "    \"codecarbon_cpu_energy_kwh\",\n",
    "    \"codecarbon_gpu_energy_kwh\",\n",
    "    \"codecarbon_ram_energy_kwh\",\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in stage_df_raw.columns:\n",
    "        stage_df_raw[col] = pd.to_numeric(stage_df_raw[col], errors=\"coerce\")\n",
    "\n",
    "# Keep a working copy\n",
    "stage_df_all = stage_df_raw.copy()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Canonical energy / throughput columns\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Best-effort total energy in kWh\n",
    "energy_kwh = None\n",
    "if \"total_energy_kwh\" in stage_df_all.columns:\n",
    "    energy_kwh = stage_df_all[\"total_energy_kwh\"].copy()\n",
    "if \"total_codecarbon_energy_kwh\" in stage_df_all.columns:\n",
    "    if energy_kwh is None:\n",
    "        energy_kwh = stage_df_all[\"total_codecarbon_energy_kwh\"].copy()\n",
    "    else:\n",
    "        energy_kwh = energy_kwh.fillna(stage_df_all[\"total_codecarbon_energy_kwh\"])\n",
    "if \"kwh_total\" in stage_df_all.columns:\n",
    "    if energy_kwh is None:\n",
    "        energy_kwh = stage_df_all[\"kwh_total\"].copy()\n",
    "    else:\n",
    "        energy_kwh = energy_kwh.fillna(stage_df_all[\"kwh_total\"])\n",
    "\n",
    "if energy_kwh is None:\n",
    "    stage_df_all[\"energy_kwh\"] = np.nan\n",
    "else:\n",
    "    stage_df_all[\"energy_kwh\"] = energy_kwh\n",
    "\n",
    "# GPU energy in kWh: prefer direct Joules, fall back to CodeCarbon\n",
    "stage_df_all[\"gpu_energy_kwh\"] = np.nan\n",
    "if \"gpu_energy_joules\" in stage_df_all.columns:\n",
    "    mask = stage_df_all[\"gpu_energy_joules\"].notna()\n",
    "    stage_df_all.loc[mask, \"gpu_energy_kwh\"] = (\n",
    "        stage_df_all.loc[mask, \"gpu_energy_joules\"] / 3.6e6\n",
    "    )\n",
    "if \"codecarbon_gpu_energy_kwh\" in stage_df_all.columns:\n",
    "    stage_df_all[\"gpu_energy_kwh\"] = stage_df_all[\"gpu_energy_kwh\"].fillna(\n",
    "        stage_df_all[\"codecarbon_gpu_energy_kwh\"]\n",
    "    )\n",
    "\n",
    "# CPU energy in kWh: prefer direct Joules, fall back to CodeCarbon\n",
    "stage_df_all[\"cpu_energy_kwh\"] = np.nan\n",
    "if \"cpu_energy_joules\" in stage_df_all.columns:\n",
    "    mask = stage_df_all[\"cpu_energy_joules\"].notna()\n",
    "    stage_df_all.loc[mask, \"cpu_energy_kwh\"] = (\n",
    "        stage_df_all.loc[mask, \"cpu_energy_joules\"] / 3.6e6\n",
    "    )\n",
    "if \"codecarbon_cpu_energy_kwh\" in stage_df_all.columns:\n",
    "    stage_df_all[\"cpu_energy_kwh\"] = stage_df_all[\"cpu_energy_kwh\"].fillna(\n",
    "        stage_df_all[\"codecarbon_cpu_energy_kwh\"]\n",
    "    )\n",
    "\n",
    "# Joules per token: prefer precomputed, else total_energy_joules / tokens_processed\n",
    "if \"joules_per_token\" in stage_df_all.columns:\n",
    "    stage_df_all[\"energy_j_per_token\"] = stage_df_all[\"joules_per_token\"]\n",
    "else:\n",
    "    stage_df_all[\"energy_j_per_token\"] = np.nan\n",
    "\n",
    "mask_need_jpt = stage_df_all[\"energy_j_per_token\"].isna()\n",
    "if \"total_energy_joules\" in stage_df_all.columns and \"tokens_processed\" in stage_df_all.columns:\n",
    "    denom = stage_df_all[\"tokens_processed\"].replace({0: np.nan})\n",
    "    stage_df_all.loc[mask_need_jpt, \"energy_j_per_token\"] = (\n",
    "        stage_df_all.loc[mask_need_jpt, \"total_energy_joules\"] / denom[mask_need_jpt]\n",
    "    )\n",
    "\n",
    "# Tokens per second: prefer precomputed, else tokens_processed / duration_seconds\n",
    "if \"tokens_per_second\" in stage_df_all.columns:\n",
    "    stage_df_all[\"tokens_per_sec\"] = stage_df_all[\"tokens_per_second\"]\n",
    "else:\n",
    "    stage_df_all[\"tokens_per_sec\"] = np.nan\n",
    "\n",
    "mask_need_tps = stage_df_all[\"tokens_per_sec\"].isna()\n",
    "if \"duration_seconds\" in stage_df_all.columns and \"tokens_processed\" in stage_df_all.columns:\n",
    "    dur = stage_df_all[\"duration_seconds\"].replace({0: np.nan})\n",
    "    stage_df_all.loc[mask_need_tps, \"tokens_per_sec\"] = (\n",
    "        stage_df_all.loc[mask_need_tps, \"tokens_processed\"] / dur[mask_need_tps]\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Filtered view: drop rows that will mess up per-token metrics\n",
    "# -------------------------------------------------------------------------\n",
    "if \"tokens_processed\" in stage_df_all.columns:\n",
    "    mask_valid_tokens = (\n",
    "        stage_df_all[\"tokens_processed\"].notna()\n",
    "        & (stage_df_all[\"tokens_processed\"] > 0)\n",
    "    )\n",
    "    stage_df_clean = stage_df_all[mask_valid_tokens].copy()\n",
    "else:\n",
    "    stage_df_clean = stage_df_all.copy()\n",
    "\n",
    "print(f\"stage_df_all:   {len(stage_df_all)} rows\")\n",
    "print(\n",
    "    f\"stage_df_clean: {len(stage_df_clean)} rows \"\n",
    "    \"(tokens_processed > 0 where available)\"\n",
    ")\n",
    "\n",
    "# Quick sanity check; this is the main DataFrame you'll use downstream\n",
    "display(stage_df_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a80a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
