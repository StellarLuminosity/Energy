{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd7e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5649a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_ROOTS = [\n",
    "    Path(\"logs\"),\n",
    "    Path(\"trillium-logs\"),\n",
    "    Path(\"runpod2_logs\"),\n",
    "]\n",
    "ROOT_CLUSTER = {\n",
    "    \"logs\": \"killarney\",\n",
    "    \"trillium-logs\": \"trillium\",\n",
    "    \"runpod2_logs\": \"runpod\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3936c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codecarbon Helper\n",
    "\n",
    "def load_codecarbon_logs(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CodeCarbon emissions.csv from each root into a single DataFrame.\n",
    "\n",
    "    Returns columns including:\n",
    "        root, cluster, project_name, experiment_id,\n",
    "        duration, cpu_energy, gpu_energy, ram_energy, energy_consumed, emissions, ...\n",
    "    \"\"\"\n",
    "    cc_rows = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cc_dir = root / \"codecarbon\"\n",
    "        if not cc_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Prefer the main emissions.csv; ignore .bak variants here\n",
    "        cc_path = cc_dir / \"emissions.csv\"\n",
    "        if not cc_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(cc_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read CodeCarbon CSV at {cc_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"root\"] = str(root)\n",
    "        df[\"cluster\"] = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        cc_rows.append(df)\n",
    "\n",
    "    if not cc_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cc_df = pd.concat(cc_rows, ignore_index=True)\n",
    "\n",
    "    # Normalize names we use often\n",
    "    cc_df.rename(\n",
    "        columns={\n",
    "            \"energy_consumed\": \"energy_consumed_kwh\",\n",
    "            \"cpu_energy\": \"cpu_energy_kwh\",\n",
    "            \"gpu_energy\": \"gpu_energy_kwh\",\n",
    "            \"ram_energy\": \"ram_energy_kwh\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return cc_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3f08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage Metrics Normalization\n",
    "\n",
    "STAGE_DEFAULTS: Dict[str, Any] = {\n",
    "    # identity / meta\n",
    "    \"root\": None,\n",
    "    \"cluster\": None,\n",
    "    \"stage_dir\": None,\n",
    "    \"experiment_id\": None,\n",
    "    \"experiment_name\": None,\n",
    "    \"stage_id\": None,\n",
    "    \"stage_name\": None,\n",
    "    \"source\": None,  # \"summary\", \"stage_json\", \"snapshot\", \"codecarbon_only\"\n",
    "\n",
    "    # snapshot info\n",
    "    \"is_snapshot\": False,\n",
    "    \"snapshot_step\": None,\n",
    "    \"snapshot_type\": None,\n",
    "    \"snapshot_time\": None,\n",
    "\n",
    "    # config metadata\n",
    "    \"total_energy_policy\": None,\n",
    "    \"pipeline\": None,\n",
    "    \"student_size\": None,\n",
    "    \"dataset_choice\": None,\n",
    "    \"kd_temperature\": None,\n",
    "    \"kd_alpha\": None,\n",
    "    \"sft_max_new_tokens\": None,\n",
    "\n",
    "    # timing / tokens\n",
    "    \"start_time\": None,\n",
    "    \"end_time\": None,\n",
    "    \"duration_seconds\": None,\n",
    "    \"tokens_processed\": None,\n",
    "    \"tokens_per_second\": None,\n",
    "\n",
    "    # GPU metrics\n",
    "    \"gpu_energy_joules\": None,\n",
    "    \"gpu_avg_power_watts\": None,\n",
    "    \"gpu_peak_power_watts\": None,\n",
    "    \"nvml_poll_interval_ms\": None,\n",
    "\n",
    "    # CPU + total\n",
    "    \"cpu_energy_joules\": None,\n",
    "    \"total_energy_joules\": None,\n",
    "    \"total_energy_kwh\": None,\n",
    "    \"joules_per_token\": None,\n",
    "    \"kwh_total\": None,\n",
    "\n",
    "    # CodeCarbon normalized\n",
    "    \"total_codecarbon_energy_kwh\": None,\n",
    "    \"codecarbon_emissions_kg\": None,\n",
    "    \"codecarbon_cpu_energy_kwh\": None,\n",
    "    \"codecarbon_gpu_energy_kwh\": None,\n",
    "    \"codecarbon_ram_energy_kwh\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_stage_metrics_dict(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize a StageMetrics-like dict (from stage JSON or experiment_summary)\n",
    "    into the canonical keys in STAGE_DEFAULTS (no root/cluster/stage_dir/source).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # Basic identifiers\n",
    "    out[\"stage_id\"] = raw.get(\"stage_id\")\n",
    "    out[\"stage_name\"] = raw.get(\"stage_name\")\n",
    "\n",
    "    # Timing / tokens\n",
    "    out[\"start_time\"] = raw.get(\"start_time\")\n",
    "    out[\"end_time\"] = raw.get(\"end_time\")\n",
    "    out[\"duration_seconds\"] = raw.get(\"duration_seconds\")\n",
    "    out[\"tokens_processed\"] = raw.get(\"tokens_processed\")\n",
    "    out[\"tokens_per_second\"] = raw.get(\"tokens_per_second\")\n",
    "\n",
    "    # GPU\n",
    "    out[\"gpu_energy_joules\"] = raw.get(\"gpu_energy_joules\")\n",
    "    out[\"gpu_avg_power_watts\"] = raw.get(\"gpu_avg_power_watts\")\n",
    "    out[\"gpu_peak_power_watts\"] = raw.get(\"gpu_peak_power_watts\")\n",
    "    out[\"nvml_poll_interval_ms\"] = raw.get(\"nvml_poll_interval_ms\")\n",
    "\n",
    "    # CPU\n",
    "    out[\"cpu_energy_joules\"] = raw.get(\"cpu_energy_joules\")\n",
    "\n",
    "    # CodeCarbon variants:\n",
    "    # new-style: total_codecarbon_energy_kwh\n",
    "    # old-style:  codecarbon_energy_kwh\n",
    "    cc_total = raw.get(\"total_codecarbon_energy_kwh\", None)\n",
    "    if cc_total is None:\n",
    "        cc_total = raw.get(\"codecarbon_energy_kwh\", None)\n",
    "    out[\"total_codecarbon_energy_kwh\"] = cc_total\n",
    "\n",
    "    out[\"codecarbon_emissions_kg\"] = raw.get(\"codecarbon_emissions_kg\")\n",
    "    out[\"codecarbon_cpu_energy_kwh\"] = raw.get(\"codecarbon_cpu_energy_kwh\")\n",
    "    out[\"codecarbon_gpu_energy_kwh\"] = raw.get(\"codecarbon_gpu_energy_kwh\")\n",
    "    out[\"codecarbon_ram_energy_kwh\"] = raw.get(\"codecarbon_ram_energy_kwh\")\n",
    "\n",
    "    # Totals / derived\n",
    "    out[\"total_energy_joules\"] = raw.get(\"total_energy_joules\")\n",
    "    out[\"total_energy_kwh\"] = raw.get(\"total_energy_kwh\")\n",
    "    out[\"joules_per_token\"] = raw.get(\"joules_per_token\")\n",
    "    out[\"kwh_total\"] = raw.get(\"kwh_total\")\n",
    "\n",
    "    # Snapshot info (may or may not be present)\n",
    "    out[\"is_snapshot\"] = bool(raw.get(\"snapshot\", False))\n",
    "    out[\"snapshot_step\"] = raw.get(\"snapshot_step\")\n",
    "    out[\"snapshot_type\"] = raw.get(\"snapshot_type\")\n",
    "    out[\"snapshot_time\"] = raw.get(\"snapshot_time\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80eee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Metadata extraction\n",
    "\n",
    "def _infer_pipeline_and_student(exp_name: str) -> (Optional[str], Optional[str]):\n",
    "    s = exp_name.lower()\n",
    "    pipeline = None\n",
    "    if s.startswith(\"kd_\"):\n",
    "        pipeline = \"kd\"\n",
    "    elif s.startswith(\"sft_\"):\n",
    "        pipeline = \"sft\"\n",
    "    elif \"true\" in s:\n",
    "        pipeline = \"true_sft\"\n",
    "\n",
    "    student_size = None\n",
    "    if \"to_1b\" in s:\n",
    "        student_size = \"1B\"\n",
    "    elif \"to_7b\" in s:\n",
    "        student_size = \"7B\"\n",
    "    elif \"to_13b\" in s or \"13b\" in s:\n",
    "        student_size = \"13B\"\n",
    "\n",
    "    return pipeline, student_size\n",
    "\n",
    "\n",
    "def load_config_meta(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan all config_*.json files and extract per-(root, stage_dir, stage_name) metadata:\n",
    "        experiment_name, total_energy_policy, pipeline, student_size, kd_temperature, kd_alpha,\n",
    "        sft_max_new_tokens, dataset_choice, etc.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        for cfg_path in root.rglob(\"config_*.json\"):\n",
    "            try:\n",
    "                with open(cfg_path) as f:\n",
    "                    cfg = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read config at {cfg_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            stage_name = cfg.get(\"stage_name\")\n",
    "            stage_id = cfg.get(\"stage_id\")\n",
    "\n",
    "            config = cfg.get(\"config\", {})\n",
    "            exp_cfg = config.get(\"experiment\", {})\n",
    "            data_cfg = config.get(\"data\", {})\n",
    "            train_cfg = config.get(\"training\", {})\n",
    "            kd_cfg = config.get(\"kd\", config.get(\"distillation\", {}))  # handle naming\n",
    "            energy_cfg = config.get(\"energy\", {})\n",
    "\n",
    "            exp_name = exp_cfg.get(\"name\", stage_name)\n",
    "            pipeline, student_size = _infer_pipeline_and_student(exp_name)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"root\": str(root),\n",
    "                    \"cluster\": cluster,\n",
    "                    \"stage_dir\": str(cfg_path.parent),\n",
    "                    \"stage_name\": stage_name,\n",
    "                    \"stage_id\": stage_id,\n",
    "                    \"experiment_name\": exp_name,\n",
    "                    \"total_energy_policy\": energy_cfg.get(\"total_energy_policy\"),\n",
    "                    \"pipeline\": pipeline,\n",
    "                    \"student_size\": student_size,\n",
    "                    \"dataset_choice\": data_cfg.get(\"dataset_choice\"),\n",
    "                    \"kd_temperature\": kd_cfg.get(\"temperature\"),\n",
    "                    \"kd_alpha\": kd_cfg.get(\"alpha\"),\n",
    "                    \"sft_max_new_tokens\": train_cfg.get(\"max_new_tokens\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage folder -> records\n",
    "\n",
    "def _collect_from_experiment_summary(\n",
    "    summary_path: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Given an experiment_summary.json, return a list of normalized stage records (source='summary').\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    exp_id = summary.get(\"experiment_id\")\n",
    "    exp_name = summary.get(\"experiment_name\")\n",
    "    stages = summary.get(\"stages\", {})\n",
    "\n",
    "    for stage_name, raw in stages.items():\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        # Default: parent of the summary (e.g., run_dir); overridden if config meta is found\n",
    "        base[\"stage_dir\"] = str(summary_path.parent)\n",
    "        base[\"experiment_id\"] = exp_id\n",
    "        base[\"experiment_name\"] = exp_name\n",
    "        base[\"source\"] = \"summary\"\n",
    "\n",
    "        # Normalize metrics\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # Attach config meta if available.\n",
    "        # Match by root + stage_name, then prefer the config's stage_dir.\n",
    "        m = cfg_meta[\n",
    "            (cfg_meta[\"root\"] == str(root))\n",
    "            & (cfg_meta[\"stage_name\"] == stage_name)\n",
    "        ]\n",
    "        if not m.empty:\n",
    "            meta_row = m.iloc[0].to_dict()\n",
    "\n",
    "            # Prefer the config's notion of the stage_dir (actual stage folder)\n",
    "            stage_dir_cfg = meta_row.get(\"stage_dir\")\n",
    "            if stage_dir_cfg:\n",
    "                base[\"stage_dir\"] = stage_dir_cfg\n",
    "\n",
    "            # Optionally override stage_id if missing\n",
    "            if base.get(\"stage_id\") is None and meta_row.get(\"stage_id\"):\n",
    "                base[\"stage_id\"] = meta_row[\"stage_id\"]\n",
    "\n",
    "            for k in [\n",
    "                \"total_energy_policy\",\n",
    "                \"pipeline\",\n",
    "                \"student_size\",\n",
    "                \"dataset_choice\",\n",
    "                \"kd_temperature\",\n",
    "                \"kd_alpha\",\n",
    "                \"sft_max_new_tokens\",\n",
    "            ]:\n",
    "                base[k] = meta_row.get(k)\n",
    "\n",
    "        records.append(base)\n",
    "\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def _is_stage_metrics_json(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: JSON files that look like StageMetrics but are not config/env/summary.\n",
    "    Includes snapshots.\n",
    "    \"\"\"\n",
    "    name = path.name\n",
    "    if not name.endswith(\".json\"):\n",
    "        return False\n",
    "    if name.startswith(\"config_\") or name.startswith(\"environment_\"):\n",
    "        return False\n",
    "    if name == \"experiment_summary.json\":\n",
    "        return False\n",
    "    # This will match stage.json and stage__step_*.json (snapshots)\n",
    "    return True\n",
    "\n",
    "\n",
    "def _collect_stage_jsons_in_dir(\n",
    "    stage_dir: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect stage records from standalone stage JSON files in a stage directory\n",
    "    (finals + snapshots), using source='stage_json' or 'snapshot'.\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Try to find associated experiment_name and config meta for this dir\n",
    "    m_dir = cfg_meta[\n",
    "        (cfg_meta[\"root\"] == str(root)) & (cfg_meta[\"stage_dir\"] == str(stage_dir))\n",
    "    ]\n",
    "    cfg_row = m_dir.iloc[0].to_dict() if not m_dir.empty else {}\n",
    "\n",
    "    for path in stage_dir.glob(\"*.json\"):\n",
    "        if not _is_stage_metrics_json(path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                raw = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read stage JSON at {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # ---- NEW: guard against non-dict JSONs (lists, etc.) ----\n",
    "        if isinstance(raw, list):\n",
    "            # Sometimes logs are saved as a single-element list; handle that.\n",
    "            if len(raw) == 1 and isinstance(raw[0], dict):\n",
    "                raw = raw[0]\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[INFO] Skipping JSON at {path} \"\n",
    "                    f\"(top-level list, not a StageMetrics dict)\"\n",
    "                )\n",
    "                continue\n",
    "        elif not isinstance(raw, dict):\n",
    "            print(\n",
    "                f\"[INFO] Skipping JSON at {path} \"\n",
    "                f\"(top-level {type(raw).__name__}, expected dict)\"\n",
    "            )\n",
    "            continue\n",
    "        # ---- END NEW ----\n",
    "\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        base[\"stage_dir\"] = str(stage_dir)\n",
    "        base[\"experiment_name\"] = cfg_row.get(\"experiment_name\")\n",
    "        base[\"source\"] = \"snapshot\" if raw.get(\"snapshot\") else \"stage_json\"\n",
    "\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # Config meta\n",
    "        for k in [\n",
    "            \"total_energy_policy\",\n",
    "            \"pipeline\",\n",
    "            \"student_size\",\n",
    "            \"dataset_choice\",\n",
    "            \"kd_temperature\",\n",
    "            \"kd_alpha\",\n",
    "            \"sft_max_new_tokens\",\n",
    "        ]:\n",
    "            if k in cfg_row:\n",
    "                base[k] = cfg_row[k]\n",
    "\n",
    "        records.append(base)\n",
    "\n",
    "    return records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aecf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-stage Dataframe\n",
    "\n",
    "def build_stage_dataframe(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main entry point:\n",
    "      - loads config metadata,\n",
    "      - walks all log roots,\n",
    "      - collects StageMetrics from experiment_summary.json and individual stage JSONs,\n",
    "      - returns one big DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    cfg_meta = load_config_meta(log_roots)\n",
    "    cc_df = load_codecarbon_logs(log_roots)  # not yet used as fallback, but available\n",
    "\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        # 1) experiment_summary.json files (per run)\n",
    "        for summary_path in root.rglob(\"experiment_summary.json\"):\n",
    "            # Skip copies written into individual stage dirs:\n",
    "            # .../<root>/stages/<stage>/experiment_summary.json\n",
    "            parent = summary_path.parent\n",
    "            if parent.parent.name == \"stages\":\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(summary_path) as f:\n",
    "                    summary = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read experiment_summary at {summary_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if \"stages\" in summary:\n",
    "                all_records.extend(\n",
    "                    _collect_from_experiment_summary(summary_path, root, cluster, cfg_meta)\n",
    "                )\n",
    "            else:\n",
    "                # Some summaries might be in an older/global format; skip or handle specially.\n",
    "                pass\n",
    "\n",
    "\n",
    "        # 2) Standalone stage directories: often under root/stages/*, but also\n",
    "        #    top-level experiment dirs like trillium-logs/sft_32b_to_1b_tulu_3500\n",
    "        for stage_dir in root.rglob(\"*\"):\n",
    "            if not stage_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Heuristic: a \"stage dir\" is one that contains some StageMetrics JSON\n",
    "            has_stage_json = any(_is_stage_metrics_json(p) for p in stage_dir.glob(\"*.json\"))\n",
    "            if not has_stage_json:\n",
    "                continue\n",
    "\n",
    "            # We already handled those covered by experiment_summary.json – but that's okay.\n",
    "            # The downstream logic can deduplicate or prefer summary over stage_json if needed.\n",
    "            records = _collect_stage_jsons_in_dir(stage_dir, root, cluster, cfg_meta)\n",
    "            all_records.extend(records)\n",
    "\n",
    "    if not all_records:\n",
    "        return pd.DataFrame(columns=STAGE_DEFAULTS.keys())\n",
    "\n",
    "    stage_df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Optional: deduplicate (e.g., you might want to drop stage_json records\n",
    "    # that correspond exactly to summary records). For now, keep everything\n",
    "    # and let later analysis decide which to use.\n",
    "    return stage_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_dataframe_for_path(path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience helper to build a standardized stage DataFrame for a specific\n",
    "    log root or stage directory.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs\")\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    - build_stage_dataframe_for_path(\"/abs/path/to/runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    \"\"\"\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # If they passed a specific stage dir under .../stages/<stage_name>\n",
    "    if path.is_dir() and path.name != \"stages\" and path.parent.name == \"stages\":\n",
    "        # /.../<log_root>/stages/<stage_name>\n",
    "        # For /project/.../Energy/runpod2_logs/stages/sft_32b_to_1b_math_nosft\n",
    "        # we want log_root = /project/.../Energy/runpod2_logs\n",
    "        log_root = path.parent.parent  # == path.parents[1]\n",
    "        filter_prefix = str(path)\n",
    "    elif path.is_dir() and path.name == \"stages\":\n",
    "        # They pointed at the stages/ directory: restrict to that subtree\n",
    "        log_root = path.parent\n",
    "        filter_prefix = str(path)\n",
    "    else:\n",
    "        # Treat as a log root\n",
    "        log_root = path\n",
    "        filter_prefix = str(log_root)\n",
    "\n",
    "    df = build_stage_dataframe([log_root])\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # If they gave a root, no extra filtering\n",
    "    if filter_prefix == str(log_root):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    # Otherwise restrict to that specific stage subtree\n",
    "    stage_dirs = df[\"stage_dir\"].astype(str)\n",
    "    mask = stage_dirs.str.startswith(filter_prefix)\n",
    "    return df[mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "391c1e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>root</th>\n",
       "      <th>cluster</th>\n",
       "      <th>stage_dir</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>stage_id</th>\n",
       "      <th>stage_name</th>\n",
       "      <th>source</th>\n",
       "      <th>is_snapshot</th>\n",
       "      <th>snapshot_step</th>\n",
       "      <th>...</th>\n",
       "      <th>cpu_energy_joules</th>\n",
       "      <th>total_energy_joules</th>\n",
       "      <th>total_energy_kwh</th>\n",
       "      <th>joules_per_token</th>\n",
       "      <th>kwh_total</th>\n",
       "      <th>total_codecarbon_energy_kwh</th>\n",
       "      <th>codecarbon_emissions_kg</th>\n",
       "      <th>codecarbon_cpu_energy_kwh</th>\n",
       "      <th>codecarbon_gpu_energy_kwh</th>\n",
       "      <th>codecarbon_ram_energy_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/project/6104653/klambert/Energy/runpod2_logs</td>\n",
       "      <td>runpod</td>\n",
       "      <td>/project/6104653/klambert/Energy/runpod2_logs/...</td>\n",
       "      <td>None</td>\n",
       "      <td>sft_32b_to_1b_math_nosft</td>\n",
       "      <td>sft_32b_to_1b_math_nosft</td>\n",
       "      <td>sft_32b_to_1b_math_nosft</td>\n",
       "      <td>stage_json</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>4377.360368</td>\n",
       "      <td>656638.919252</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.185152</td>\n",
       "      <td>0.031484</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.181163</td>\n",
       "      <td>0.002773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            root cluster  \\\n",
       "0  /project/6104653/klambert/Energy/runpod2_logs  runpod   \n",
       "\n",
       "                                           stage_dir experiment_id  \\\n",
       "0  /project/6104653/klambert/Energy/runpod2_logs/...          None   \n",
       "\n",
       "            experiment_name                  stage_id  \\\n",
       "0  sft_32b_to_1b_math_nosft  sft_32b_to_1b_math_nosft   \n",
       "\n",
       "                 stage_name      source  is_snapshot snapshot_step  ...  \\\n",
       "0  sft_32b_to_1b_math_nosft  stage_json        False          None  ...   \n",
       "\n",
       "  cpu_energy_joules total_energy_joules total_energy_kwh joules_per_token  \\\n",
       "0       4377.360368       656638.919252           0.1824              0.0   \n",
       "\n",
       "  kwh_total total_codecarbon_energy_kwh codecarbon_emissions_kg  \\\n",
       "0    0.1824                    0.185152                0.031484   \n",
       "\n",
       "  codecarbon_cpu_energy_kwh codecarbon_gpu_energy_kwh  \\\n",
       "0                  0.001216                  0.181163   \n",
       "\n",
       "   codecarbon_ram_energy_kwh  \n",
       "0                   0.002773  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sft_32b_to_1b_math_nosft.csv with 1 rows.\n"
     ]
    }
   ],
   "source": [
    "# Choose what you want to process\n",
    "# - Leave `paths` empty to use default LOG_ROOTS\n",
    "# - Or set it to one or more specific paths, e.g. a single stage dir\n",
    "paths = [\"/home/klambert/projects/aip-craffel/klambert/Energy/runpod2_logs/stages/sft_32b_to_1b_math_nosft\"]\n",
    "output = \"sft_32b_to_1b_math_nosft.csv\"\n",
    "\n",
    "if paths:\n",
    "    dfs = [build_stage_dataframe_for_path(p) for p in paths]\n",
    "    df = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]\n",
    "else:\n",
    "    df = build_stage_dataframe(LOG_ROOTS)\n",
    "\n",
    "display(df.head())\n",
    "df.to_csv(output, index=False)\n",
    "print(f\"Saved {output} with {len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a465a0a",
   "metadata": {},
   "source": [
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_stage_df(log_roots):\n",
    "    rows = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        for summary_path in root.rglob(\"experiment_summary.json\"):\n",
    "            with open(summary_path) as f:\n",
    "                summary = json.load(f)\n",
    "\n",
    "            cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "\n",
    "            exp_id = summary.get(\"experiment_id\")\n",
    "            exp_name = summary.get(\"experiment_name\")\n",
    "\n",
    "            # New-style: stages dict\n",
    "            stages = summary.get(\"stages\", {})\n",
    "\n",
    "            for stage_name, s in stages.items():\n",
    "                rows.append({\n",
    "                    \"cluster\": cluster,\n",
    "                    \"root\": str(root),\n",
    "                    \"exp_dir\": str(summary_path.parent),\n",
    "                    \"exp_id\": exp_id,\n",
    "                    \"exp_name\": exp_name,\n",
    "                    \"stage_name\": stage_name,\n",
    "\n",
    "                    # core metrics\n",
    "                    \"duration_s\": s.get(\"duration_seconds\"),\n",
    "                    \"tokens\": s.get(\"tokens_processed\"),\n",
    "                    \"total_energy_j\": s.get(\"total_energy_joules\"),\n",
    "                    \"total_energy_kwh\": s.get(\"total_energy_kwh\"),\n",
    "                    \"j_per_token\": s.get(\"joules_per_token\"),\n",
    "                    \"tps\": s.get(\"tokens_per_second\"),\n",
    "\n",
    "                    # gpu detail\n",
    "                    \"gpu_energy_j\": s.get(\"gpu_energy_joules\"),\n",
    "                    \"gpu_avg_power_w\": s.get(\"gpu_avg_power_watts\"),\n",
    "                    \"gpu_peak_power_w\": s.get(\"gpu_peak_power_watts\"),\n",
    "\n",
    "                    # cpu if present\n",
    "                    \"cpu_energy_j\": s.get(\"cpu_energy_joules\", 0.0),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "stage_df = load_stage_df(LOG_ROOTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee4e09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>root</th>\n",
       "      <th>exp_dir</th>\n",
       "      <th>exp_id</th>\n",
       "      <th>exp_name</th>\n",
       "      <th>stage_name</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>tokens</th>\n",
       "      <th>total_energy_j</th>\n",
       "      <th>total_energy_kwh</th>\n",
       "      <th>j_per_token</th>\n",
       "      <th>tps</th>\n",
       "      <th>gpu_energy_j</th>\n",
       "      <th>gpu_avg_power_w</th>\n",
       "      <th>gpu_peak_power_w</th>\n",
       "      <th>cpu_energy_j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>killarney</td>\n",
       "      <td>logs</td>\n",
       "      <td>logs/stages/codeforces_cots_preprocess_codefor...</td>\n",
       "      <td>codeforces_cots_preprocess_20260114_204758</td>\n",
       "      <td>codeforces_cots_preprocess</td>\n",
       "      <td>codeforces_cots_preprocess_codeforces_cots_pre...</td>\n",
       "      <td>48.728971</td>\n",
       "      <td>47680</td>\n",
       "      <td>6.028469e+03</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.126436</td>\n",
       "      <td>978.473355</td>\n",
       "      <td>6.028469e+03</td>\n",
       "      <td>123.714263</td>\n",
       "      <td>124.722</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>killarney</td>\n",
       "      <td>logs</td>\n",
       "      <td>logs/stages/kd_olmo2_32b_to_1b_adamw</td>\n",
       "      <td>kd_olmo2_32b_to_1b_20260118_104507</td>\n",
       "      <td>kd_olmo2_32b_to_1b</td>\n",
       "      <td>kd_olmo2_32b_to_1b</td>\n",
       "      <td>2637.674475</td>\n",
       "      <td>1148748</td>\n",
       "      <td>6.498188e+05</td>\n",
       "      <td>0.180505</td>\n",
       "      <td>0.565676</td>\n",
       "      <td>435.515455</td>\n",
       "      <td>5.377166e+05</td>\n",
       "      <td>203.860109</td>\n",
       "      <td>300.569</td>\n",
       "      <td>1.121022e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>killarney</td>\n",
       "      <td>logs</td>\n",
       "      <td>logs/stages/kd_olmo2_32b_to_7b_adamw</td>\n",
       "      <td>kd_olmo2_32b_to_7b_20260118_122055</td>\n",
       "      <td>kd_olmo2_32b_to_7b</td>\n",
       "      <td>kd_olmo2_32b_to_7b</td>\n",
       "      <td>3894.085561</td>\n",
       "      <td>992861</td>\n",
       "      <td>1.498956e+06</td>\n",
       "      <td>0.416377</td>\n",
       "      <td>1.509734</td>\n",
       "      <td>254.966406</td>\n",
       "      <td>1.333456e+06</td>\n",
       "      <td>342.431142</td>\n",
       "      <td>554.975</td>\n",
       "      <td>1.654999e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>killarney</td>\n",
       "      <td>logs</td>\n",
       "      <td>logs/stages/openr1_math_preprocess_dataset_ope...</td>\n",
       "      <td>openr1_math_preprocess_dataset_20260116_164107</td>\n",
       "      <td>openr1_math_preprocess_dataset</td>\n",
       "      <td>openr1_math_preprocess_dataset_openr1_math_pre...</td>\n",
       "      <td>77.804341</td>\n",
       "      <td>8000</td>\n",
       "      <td>8.130251e+03</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>1.016281</td>\n",
       "      <td>102.822026</td>\n",
       "      <td>8.130251e+03</td>\n",
       "      <td>104.496112</td>\n",
       "      <td>106.112</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>killarney</td>\n",
       "      <td>logs</td>\n",
       "      <td>logs/stages/tulu_preprocess</td>\n",
       "      <td>tulu_preprocess_dataset_20260108_114813</td>\n",
       "      <td>tulu_preprocess_dataset</td>\n",
       "      <td>tulu_preprocess_dataset</td>\n",
       "      <td>105.623756</td>\n",
       "      <td>194769</td>\n",
       "      <td>2.300193e+04</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.118099</td>\n",
       "      <td>1843.988587</td>\n",
       "      <td>2.300193e+04</td>\n",
       "      <td>217.772338</td>\n",
       "      <td>320.473</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>synthetic_tulu_generation_20260120_184906</td>\n",
       "      <td>synthetic_tulu_generation</td>\n",
       "      <td>synthetic_tulu_generation</td>\n",
       "      <td>47943.635401</td>\n",
       "      <td>5375119</td>\n",
       "      <td>2.065802e+07</td>\n",
       "      <td>5.738340</td>\n",
       "      <td>3.843268</td>\n",
       "      <td>112.113296</td>\n",
       "      <td>1.962278e+07</td>\n",
       "      <td>409.288623</td>\n",
       "      <td>586.806</td>\n",
       "      <td>1.035238e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs/sft_32b_to_1b_tulu_256_synthetic...</td>\n",
       "      <td>sft_32b_to_1b_tulu_256_20260125_113720</td>\n",
       "      <td>sft_32b_to_1b_tulu_256</td>\n",
       "      <td>sft_32b_to_1b_tulu_256</td>\n",
       "      <td>9903.494433</td>\n",
       "      <td>1432109</td>\n",
       "      <td>4.078968e+06</td>\n",
       "      <td>1.133047</td>\n",
       "      <td>2.848224</td>\n",
       "      <td>144.606433</td>\n",
       "      <td>3.868880e+06</td>\n",
       "      <td>390.658049</td>\n",
       "      <td>540.894</td>\n",
       "      <td>2.100877e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs/sft_32b_to_1b_tulu_3500</td>\n",
       "      <td>sft_32b_to_1b_tulu_3500_20260124_201253</td>\n",
       "      <td>sft_32b_to_1b_tulu_3500</td>\n",
       "      <td>sft_32b_to_1b_tulu_3500</td>\n",
       "      <td>813.011440</td>\n",
       "      <td>15034902</td>\n",
       "      <td>5.147457e+05</td>\n",
       "      <td>0.142985</td>\n",
       "      <td>0.034237</td>\n",
       "      <td>18492.854171</td>\n",
       "      <td>4.971349e+05</td>\n",
       "      <td>611.473496</td>\n",
       "      <td>657.203</td>\n",
       "      <td>1.761072e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs/stages/synthetic_codeforces</td>\n",
       "      <td>synthetic_codeforces_20260118_165320</td>\n",
       "      <td>synthetic_codeforces</td>\n",
       "      <td>synthetic_codeforces</td>\n",
       "      <td>40.807530</td>\n",
       "      <td>41375</td>\n",
       "      <td>5.025964e+03</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.121473</td>\n",
       "      <td>1013.906008</td>\n",
       "      <td>5.008924e+03</td>\n",
       "      <td>122.745100</td>\n",
       "      <td>123.826</td>\n",
       "      <td>1.703997e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs/stages/synthetic_codeforces_gene...</td>\n",
       "      <td>synthetic_codeforces_generation_20260120_185205</td>\n",
       "      <td>synthetic_codeforces_generation</td>\n",
       "      <td>synthetic_codeforces_generation</td>\n",
       "      <td>15118.032157</td>\n",
       "      <td>2399873</td>\n",
       "      <td>6.655575e+06</td>\n",
       "      <td>1.848771</td>\n",
       "      <td>2.773303</td>\n",
       "      <td>158.742419</td>\n",
       "      <td>6.332620e+06</td>\n",
       "      <td>418.878592</td>\n",
       "      <td>589.538</td>\n",
       "      <td>3.229554e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs/stages/synthetic_math</td>\n",
       "      <td>synthetic_math_20260118_164603</td>\n",
       "      <td>synthetic_math</td>\n",
       "      <td>synthetic_math</td>\n",
       "      <td>9.302697</td>\n",
       "      <td>7999</td>\n",
       "      <td>1.120552e+03</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.140087</td>\n",
       "      <td>859.858151</td>\n",
       "      <td>1.089487e+03</td>\n",
       "      <td>117.115139</td>\n",
       "      <td>119.591</td>\n",
       "      <td>3.106528e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs/stages/synthetic_math_generation</td>\n",
       "      <td>synthetic_math_generation_20260121_124102</td>\n",
       "      <td>synthetic_math_generation</td>\n",
       "      <td>synthetic_math_generation</td>\n",
       "      <td>67148.152377</td>\n",
       "      <td>6334944</td>\n",
       "      <td>2.804010e+07</td>\n",
       "      <td>7.788918</td>\n",
       "      <td>4.426259</td>\n",
       "      <td>94.342789</td>\n",
       "      <td>2.658926e+07</td>\n",
       "      <td>395.979055</td>\n",
       "      <td>564.827</td>\n",
       "      <td>1.450842e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>trillium</td>\n",
       "      <td>trillium-logs</td>\n",
       "      <td>trillium-logs/stages/synthetic_tulu_generation</td>\n",
       "      <td>synthetic_tulu_generation_20260120_184906</td>\n",
       "      <td>synthetic_tulu_generation</td>\n",
       "      <td>synthetic_tulu_generation</td>\n",
       "      <td>47943.635401</td>\n",
       "      <td>5375119</td>\n",
       "      <td>2.065802e+07</td>\n",
       "      <td>5.738340</td>\n",
       "      <td>3.843268</td>\n",
       "      <td>112.113296</td>\n",
       "      <td>1.962278e+07</td>\n",
       "      <td>409.288623</td>\n",
       "      <td>586.806</td>\n",
       "      <td>1.035238e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cluster           root  \\\n",
       "0   killarney           logs   \n",
       "1   killarney           logs   \n",
       "2   killarney           logs   \n",
       "3   killarney           logs   \n",
       "4   killarney           logs   \n",
       "5    trillium  trillium-logs   \n",
       "6    trillium  trillium-logs   \n",
       "7    trillium  trillium-logs   \n",
       "8    trillium  trillium-logs   \n",
       "9    trillium  trillium-logs   \n",
       "10   trillium  trillium-logs   \n",
       "11   trillium  trillium-logs   \n",
       "12   trillium  trillium-logs   \n",
       "\n",
       "                                              exp_dir  \\\n",
       "0   logs/stages/codeforces_cots_preprocess_codefor...   \n",
       "1                logs/stages/kd_olmo2_32b_to_1b_adamw   \n",
       "2                logs/stages/kd_olmo2_32b_to_7b_adamw   \n",
       "3   logs/stages/openr1_math_preprocess_dataset_ope...   \n",
       "4                         logs/stages/tulu_preprocess   \n",
       "5                                       trillium-logs   \n",
       "6   trillium-logs/sft_32b_to_1b_tulu_256_synthetic...   \n",
       "7               trillium-logs/sft_32b_to_1b_tulu_3500   \n",
       "8           trillium-logs/stages/synthetic_codeforces   \n",
       "9   trillium-logs/stages/synthetic_codeforces_gene...   \n",
       "10                trillium-logs/stages/synthetic_math   \n",
       "11     trillium-logs/stages/synthetic_math_generation   \n",
       "12     trillium-logs/stages/synthetic_tulu_generation   \n",
       "\n",
       "                                             exp_id  \\\n",
       "0        codeforces_cots_preprocess_20260114_204758   \n",
       "1                kd_olmo2_32b_to_1b_20260118_104507   \n",
       "2                kd_olmo2_32b_to_7b_20260118_122055   \n",
       "3    openr1_math_preprocess_dataset_20260116_164107   \n",
       "4           tulu_preprocess_dataset_20260108_114813   \n",
       "5         synthetic_tulu_generation_20260120_184906   \n",
       "6            sft_32b_to_1b_tulu_256_20260125_113720   \n",
       "7           sft_32b_to_1b_tulu_3500_20260124_201253   \n",
       "8              synthetic_codeforces_20260118_165320   \n",
       "9   synthetic_codeforces_generation_20260120_185205   \n",
       "10                   synthetic_math_20260118_164603   \n",
       "11        synthetic_math_generation_20260121_124102   \n",
       "12        synthetic_tulu_generation_20260120_184906   \n",
       "\n",
       "                           exp_name  \\\n",
       "0        codeforces_cots_preprocess   \n",
       "1                kd_olmo2_32b_to_1b   \n",
       "2                kd_olmo2_32b_to_7b   \n",
       "3    openr1_math_preprocess_dataset   \n",
       "4           tulu_preprocess_dataset   \n",
       "5         synthetic_tulu_generation   \n",
       "6            sft_32b_to_1b_tulu_256   \n",
       "7           sft_32b_to_1b_tulu_3500   \n",
       "8              synthetic_codeforces   \n",
       "9   synthetic_codeforces_generation   \n",
       "10                   synthetic_math   \n",
       "11        synthetic_math_generation   \n",
       "12        synthetic_tulu_generation   \n",
       "\n",
       "                                           stage_name    duration_s    tokens  \\\n",
       "0   codeforces_cots_preprocess_codeforces_cots_pre...     48.728971     47680   \n",
       "1                                  kd_olmo2_32b_to_1b   2637.674475   1148748   \n",
       "2                                  kd_olmo2_32b_to_7b   3894.085561    992861   \n",
       "3   openr1_math_preprocess_dataset_openr1_math_pre...     77.804341      8000   \n",
       "4                             tulu_preprocess_dataset    105.623756    194769   \n",
       "5                           synthetic_tulu_generation  47943.635401   5375119   \n",
       "6                              sft_32b_to_1b_tulu_256   9903.494433   1432109   \n",
       "7                             sft_32b_to_1b_tulu_3500    813.011440  15034902   \n",
       "8                                synthetic_codeforces     40.807530     41375   \n",
       "9                     synthetic_codeforces_generation  15118.032157   2399873   \n",
       "10                                     synthetic_math      9.302697      7999   \n",
       "11                          synthetic_math_generation  67148.152377   6334944   \n",
       "12                          synthetic_tulu_generation  47943.635401   5375119   \n",
       "\n",
       "    total_energy_j  total_energy_kwh  j_per_token           tps  gpu_energy_j  \\\n",
       "0     6.028469e+03          0.001675     0.126436    978.473355  6.028469e+03   \n",
       "1     6.498188e+05          0.180505     0.565676    435.515455  5.377166e+05   \n",
       "2     1.498956e+06          0.416377     1.509734    254.966406  1.333456e+06   \n",
       "3     8.130251e+03          0.002258     1.016281    102.822026  8.130251e+03   \n",
       "4     2.300193e+04          0.006389     0.118099   1843.988587  2.300193e+04   \n",
       "5     2.065802e+07          5.738340     3.843268    112.113296  1.962278e+07   \n",
       "6     4.078968e+06          1.133047     2.848224    144.606433  3.868880e+06   \n",
       "7     5.147457e+05          0.142985     0.034237  18492.854171  4.971349e+05   \n",
       "8     5.025964e+03          0.001396     0.121473   1013.906008  5.008924e+03   \n",
       "9     6.655575e+06          1.848771     2.773303    158.742419  6.332620e+06   \n",
       "10    1.120552e+03          0.000311     0.140087    859.858151  1.089487e+03   \n",
       "11    2.804010e+07          7.788918     4.426259     94.342789  2.658926e+07   \n",
       "12    2.065802e+07          5.738340     3.843268    112.113296  1.962278e+07   \n",
       "\n",
       "    gpu_avg_power_w  gpu_peak_power_w  cpu_energy_j  \n",
       "0        123.714263           124.722  0.000000e+00  \n",
       "1        203.860109           300.569  1.121022e+05  \n",
       "2        342.431142           554.975  1.654999e+05  \n",
       "3        104.496112           106.112  0.000000e+00  \n",
       "4        217.772338           320.473  0.000000e+00  \n",
       "5        409.288623           586.806  1.035238e+06  \n",
       "6        390.658049           540.894  2.100877e+05  \n",
       "7        611.473496           657.203  1.761072e+04  \n",
       "8        122.745100           123.826  1.703997e+01  \n",
       "9        418.878592           589.538  3.229554e+05  \n",
       "10       117.115139           119.591  3.106528e+01  \n",
       "11       395.979055           564.827  1.450842e+06  \n",
       "12       409.288623           586.806  1.035238e+06  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd24098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Logs into dataframe\n",
    "import json, glob, pathlib\n",
    "import pandas as pd\n",
    "\n",
    "def load_all_experiment_summaries(root=\".\"):\n",
    "    rows = []\n",
    "    for path in glob.glob(f\"{root}/**/experiment_summary.json\", recursive=True):\n",
    "        with open(path) as f:\n",
    "            summary = json.load(f)\n",
    "        exp_id = summary[\"experiment_id\"]\n",
    "        exp_name = summary[\"experiment_name\"]\n",
    "        for stage_name, s in summary[\"stages\"].items():\n",
    "            rows.append({\n",
    "                \"exp_id\": exp_id,\n",
    "                \"exp_name\": exp_name,\n",
    "                \"stage_name\": stage_name,\n",
    "\n",
    "                # core metrics\n",
    "                \"duration_s\": s[\"duration_seconds\"],\n",
    "                \"tokens\": s[\"tokens_processed\"],\n",
    "                \"total_energy_j\": s[\"total_energy_joules\"],\n",
    "                \"total_energy_kwh\": s[\"total_energy_kwh\"],\n",
    "                \"j_per_token\": s[\"joules_per_token\"],\n",
    "                \"tps\": s[\"tokens_per_second\"],\n",
    "\n",
    "                # gpu detail\n",
    "                \"gpu_energy_j\": s[\"gpu_energy_joules\"],\n",
    "                \"gpu_avg_power_w\": s[\"gpu_avg_power_watts\"],\n",
    "                \"gpu_peak_power_w\": s[\"gpu_peak_power_watts\"],\n",
    "\n",
    "                # cpu if available\n",
    "                \"cpu_energy_j\": s.get(\"cpu_energy_joules\", 0.0),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "stage_df = load_all_experiment_summaries(\"/path/to/your/log/root\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ec94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each stage with a role\n",
    "def infer_stage_role(stage_name: str) -> str:\n",
    "    s = stage_name.lower()\n",
    "    if \"preprocess\" in s:\n",
    "        return \"teacher_preprocess\"\n",
    "    if \"synthetic\" in s or \"generation\" in s:\n",
    "        return \"teacher_generation\"\n",
    "    if \"logprob\" in s or \"logit\" in s or \"cache\" in s:\n",
    "        return \"teacher_logit_cache\"\n",
    "    if \"eval\" in s or \"benchmark\" in s:\n",
    "        return \"eval\"\n",
    "    # default: main training stage\n",
    "    return \"student_train\"\n",
    "\n",
    "stage_df[\"stage_role\"] = stage_df[\"stage_name\"].apply(infer_stage_role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e36ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach Metadata\n",
    "def load_config_metadata(root=\".\"):\n",
    "    rows = []\n",
    "    for path in glob.glob(f\"{root}/**/config_*.json\", recursive=True):\n",
    "        with open(path) as f:\n",
    "            cfg = json.load(f)\n",
    "        stage_name = cfg[\"stage_name\"]\n",
    "        exp_cfg = cfg[\"config\"][\"experiment\"]\n",
    "        data_cfg = cfg[\"config\"][\"data\"]\n",
    "        train_cfg = cfg[\"config\"].get(\"training\", {})\n",
    "        kd_cfg = cfg[\"config\"].get(\"kd\", {})\n",
    "\n",
    "        exp_name = exp_cfg[\"name\"]\n",
    "        # heuristic: parse pipeline + student size from exp_name\n",
    "        # e.g. \"kd_olmo2_32b_to_13b_nosft\"\n",
    "        s = exp_name.lower()\n",
    "        if s.startswith(\"kd_\"):\n",
    "            pipeline = \"kd\"\n",
    "        elif s.startswith(\"sft_\"):\n",
    "            pipeline = \"sft\"\n",
    "        elif \"synthetic\" in s:\n",
    "            pipeline = \"synthetic_sft\"\n",
    "        else:\n",
    "            pipeline = \"other\"\n",
    "\n",
    "        # crude parse of student size from name; you can refine\n",
    "        student_size = None\n",
    "        if \"to_1b\" in s:\n",
    "            student_size = \"1B\"\n",
    "        elif \"to_7b\" in s:\n",
    "            student_size = \"7B\"\n",
    "        elif \"to_13b\" in s or \"13b\" in s:\n",
    "            student_size = \"13B\"\n",
    "\n",
    "        rows.append({\n",
    "            \"exp_name\": exp_name,\n",
    "            \"stage_name\": stage_name,\n",
    "            \"pipeline\": pipeline,\n",
    "            \"student_size\": student_size,\n",
    "            # KD hyperparams if present\n",
    "            \"kd_temperature\": kd_cfg.get(\"temperature\"),\n",
    "            \"kd_alpha\": kd_cfg.get(\"alpha\"),\n",
    "            # SFT hyperparams\n",
    "            \"sft_max_new_tokens\": train_cfg.get(\"max_new_tokens\"),\n",
    "            \"dataset_choice\": data_cfg.get(\"dataset_choice\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cfg_meta = load_config_metadata(\"/path/to/your/log/root\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda04316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metadata into stage_df\n",
    "stage_df = stage_df.merge(\n",
    "    cfg_meta,\n",
    "    on=[\"exp_name\", \"stage_name\"],\n",
    "    how=\"left\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06479e3",
   "metadata": {},
   "source": [
    "## Experiment-Level Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ce015",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_roles = {\"teacher_preprocess\", \"teacher_generation\", \"teacher_logit_cache\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics across stages\n",
    "\n",
    "agg_cols = [\"total_energy_kwh\", \"total_energy_joules\", \"tokens\", \"duration_s\"]\n",
    "\n",
    "# Teacher-only totals\n",
    "teacher = (\n",
    "    stage_df[stage_df[\"stage_role\"].isin(teacher_roles)]\n",
    "    .groupby([\"exp_id\", \"exp_name\"])\n",
    "    [agg_cols]\n",
    "    .sum()\n",
    "    .add_prefix(\"teacher_\")\n",
    ")\n",
    "\n",
    "# Student-only totals (everything else, including eval)\n",
    "student = (\n",
    "    stage_df[~stage_df[\"stage_role\"].isin(teacher_roles)]\n",
    "    .groupby([\"exp_id\", \"exp_name\"])\n",
    "    [agg_cols]\n",
    "    .sum()\n",
    "    .add_prefix(\"student_\")\n",
    ")\n",
    "\n",
    "# Combine into run-level df\n",
    "run_df = teacher.join(student, how=\"outer\").reset_index()\n",
    "\n",
    "# Fill NaNs where experiments don't have teacher stages, etc.\n",
    "run_df = run_df.fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348aef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive key per-run metrics\n",
    "\n",
    "# Total pipeline energy\n",
    "run_df[\"total_energy_kwh\"] = (\n",
    "    run_df[\"teacher_total_energy_kwh\"] + run_df[\"student_total_energy_kwh\"]\n",
    ")\n",
    "run_df[\"total_energy_j\"] = (\n",
    "    run_df[\"teacher_total_energy_joules\"] + run_df[\"student_total_energy_joules\"]\n",
    ")\n",
    "\n",
    "# Total tokens (for per-token normalization)\n",
    "run_df[\"total_tokens\"] = run_df[\"teacher_tokens\"] + run_df[\"student_tokens\"]\n",
    "\n",
    "# Per-token metrics\n",
    "run_df[\"student_j_per_token\"] = run_df[\"student_total_energy_joules\"] / run_df[\"student_tokens\"]\n",
    "run_df[\"teacher_j_per_token\"] = run_df[\"teacher_total_energy_joules\"] / run_df[\"teacher_tokens\"]\n",
    "run_df[\"overall_j_per_token\"] = run_df[\"total_energy_j\"] / run_df[\"total_tokens\"]\n",
    "\n",
    "# Throughput and avg power\n",
    "run_df[\"total_duration_s\"] = run_df[\"teacher_duration_s\"] + run_df[\"student_duration_s\"]\n",
    "run_df[\"overall_tps\"] = run_df[\"total_tokens\"] / run_df[\"total_duration_s\"]\n",
    "run_df[\"overall_avg_power_w\"] = run_df[\"total_energy_j\"] / run_df[\"total_duration_s\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group\n",
    "# Pull a single representative row (pipeline, student_size, etc.) per experiment\n",
    "meta_per_exp = (\n",
    "    stage_df\n",
    "    .groupby([\"exp_id\", \"exp_name\"])\n",
    "    .agg({\n",
    "        \"pipeline\": \"first\",\n",
    "        \"student_size\": \"first\",\n",
    "        \"dataset_choice\": \"first\",\n",
    "        \"kd_temperature\": \"first\",\n",
    "        \"kd_alpha\": \"first\",\n",
    "        \"sft_max_new_tokens\": \"first\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "run_df = run_df.merge(meta_per_exp, on=[\"exp_id\", \"exp_name\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc144e",
   "metadata": {},
   "source": [
    "## Results & Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee66d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which runs to include in the \"core 3x3\", e.g. main config only\n",
    "core = run_df.query(\"pipeline in ['sft', 'kd', 'synthetic_sft'] and student_size in ['1B','7B','13B']\")\n",
    "\n",
    "# If you have multiple seeds per cell, pick median energy or best-quality run, etc.\n",
    "# Example: median by energy\n",
    "core_med = (\n",
    "    core\n",
    "    .groupby([\"pipeline\", \"student_size\"])\n",
    "    .agg({\n",
    "        \"total_energy_kwh\": \"median\",\n",
    "        \"overall_avg_power_w\": \"median\",\n",
    "        \"overall_j_per_token\": \"median\",\n",
    "        \"student_j_per_token\": \"median\",\n",
    "        \"overall_tps\": \"median\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot into grids for tables\n",
    "energy_grid = core_med.pivot(index=\"pipeline\", columns=\"student_size\", values=\"total_energy_kwh\")\n",
    "jpt_grid    = core_med.pivot(index=\"pipeline\", columns=\"student_size\", values=\"overall_j_per_token\")\n",
    "power_grid  = core_med.pivot(index=\"pipeline\", columns=\"student_size\", values=\"overall_avg_power_w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare against baseline SFT by normalizing\n",
    "# Merge SFT baseline onto other pipelines for same student_size\n",
    "sft_baseline = core_med[core_med[\"pipeline\"] == \"sft\"][[\"student_size\", \"total_energy_kwh\"]].rename(columns={\"total_energy_kwh\": \"sft_energy_kwh\"})\n",
    "\n",
    "core_med = core_med.merge(sft_baseline, on=\"student_size\", how=\"left\")\n",
    "core_med[\"energy_vs_sft\"] = core_med[\"total_energy_kwh\"] / core_med[\"sft_energy_kwh\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01599316",
   "metadata": {},
   "source": [
    "### Pareto Frontiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook into benchmarks\n",
    "# model_id = maybe exp_name or a path to weights, mapped back to exp_name\n",
    "bench_df = pd.DataFrame([\n",
    "    # one row per (experiment, benchmark)\n",
    "    # exp_name, benchmark, score\n",
    "])\n",
    "\n",
    "# Option 1: a single scalar per model (e.g. MT-Bench-101 or composite)\n",
    "quality = (\n",
    "    bench_df\n",
    "    .groupby(\"exp_name\")\n",
    "    .agg({\"score\": \"mean\"})  # or a weighted combo\n",
    "    .rename(columns={\"score\": \"quality_score\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "run_q = run_df.merge(quality, on=\"exp_name\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For teacher-ignored vs teacher-included\n",
    "run_q[\"energy_teacher_ignored\"] = run_q[\"student_total_energy_kwh\"]\n",
    "run_q[\"energy_teacher_included\"] = run_q[\"total_energy_kwh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17038c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_front(df, energy_col=\"total_energy_kwh\", quality_col=\"quality_score\"):\n",
    "    # smaller energy is better, higher quality is better\n",
    "    points = df.sort_values(energy_col).reset_index(drop=True)\n",
    "    best_q = -float(\"inf\")\n",
    "    mask = []\n",
    "    for _, row in points.iterrows():\n",
    "        if row[quality_col] > best_q:\n",
    "            best_q = row[quality_col]\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return points[mask]\n",
    "\n",
    "frontier = pareto_front(run_q.query(\"pipeline == 'kd'\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5013b5b",
   "metadata": {},
   "source": [
    "### Stage-wise breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb471bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum energy per role × pipeline × student size\n",
    "stage_energy = (\n",
    "    stage_df\n",
    "    .groupby([\"pipeline\", \"student_size\", \"stage_role\"])\n",
    "    [\"total_energy_kwh\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd294934",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd = run_q.query(\"pipeline == 'kd'\")\n",
    "\n",
    "# Example: line plots of energy vs quality for different temperatures\n",
    "# One panel per τ, x-axis alpha, y-axis quality / energy, etc.\n",
    "\n",
    "# For a simple table:\n",
    "kd_summary = (\n",
    "    kd.groupby([\"student_size\", \"kd_temperature\", \"kd_alpha\"])\n",
    "    .agg({\n",
    "        \"total_energy_kwh\": \"median\",\n",
    "        \"overall_j_per_token\": \"median\",\n",
    "        \"quality_score\": \"median\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abad757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft = run_q.query(\"pipeline == 'sft'\")\n",
    "\n",
    "sft_summary = (\n",
    "    sft.groupby([\"student_size\", \"sft_max_new_tokens\", \"dataset_choice\"])\n",
    "    .agg({\n",
    "        \"total_energy_kwh\": \"median\",\n",
    "        \"overall_j_per_token\": \"median\",\n",
    "        \"quality_score\": \"median\",\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca26f35",
   "metadata": {},
   "source": [
    "## Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose core_med has columns:\n",
    "# pipeline, student_size, total_energy_kwh, overall_j_per_token, overall_avg_power_w\n",
    "\n",
    "# Pivot to a 3×3 grid of total kWh\n",
    "energy_grid = core_med.pivot(index=\"pipeline\", columns=\"student_size\", values=\"total_energy_kwh\")\n",
    "\n",
    "latex_table = energy_grid.to_latex(\n",
    "    index=True,\n",
    "    float_format=\"%.3f\",\n",
    "    caption=\"Total energy (kWh) for each pipeline and student size.\",\n",
    "    label=\"tab:energy_core_grid\",\n",
    "    escape=False,\n",
    "    bold_rows=False,\n",
    "    column_format=\"lccc\",  # adjust as needed\n",
    "    longtable=False,\n",
    "    multicolumn=True,\n",
    "    multicolumn_format='c',\n",
    "    na_rep=\"--\",\n",
    "    bold_header=True if hasattr(energy_grid, 'style') else False,  # optional\n",
    "    buf=None,\n",
    "    header=True,\n",
    "    show_dimensions=False\n",
    ")\n",
    "\n",
    "with open(\"tables/energy_core_grid.tex\", \"w\") as f:\n",
    "    f.write(latex_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e40189",
   "metadata": {},
   "source": [
    "In Latex:\n",
    "```\n",
    "\\begin{table}[t]\n",
    "    \\centering\n",
    "    \\input{tables/energy_core_grid.tex}\n",
    "    \\vspace{-0.5em}\n",
    "\\end{table}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfac57",
   "metadata": {},
   "source": [
    "```\n",
    "\\begin{table}[t]\n",
    "  \\centering\n",
    "  \\small\n",
    "  \\setlength{\\tabcolsep}{4pt}\n",
    "  \\input{tables/energy_core_grid.tex}\n",
    "  \\caption{Total energy (kWh) per pipeline and student size. Lower is better.}\n",
    "  \\label{tab:energy_core_grid}\n",
    "\\end{table}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52eee34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
