{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5649a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_ROOTS = [\n",
    "    Path(\"logs\"),\n",
    "    Path(\"trillium-logs\"),\n",
    "    Path(\"runpod2_logs\"),\n",
    "]\n",
    "ROOT_CLUSTER = {\n",
    "    \"logs\": \"killarney\",\n",
    "    \"trillium-logs\": \"trillium\",\n",
    "    \"runpod2_logs\": \"runpod\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3936c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codecarbon Helper\n",
    "\n",
    "def load_codecarbon_logs(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CodeCarbon emissions.csv from each root into a single DataFrame.\n",
    "\n",
    "    Returns columns including:\n",
    "        root, cluster, project_name, experiment_id,\n",
    "        duration, cpu_energy, gpu_energy, ram_energy, energy_consumed, emissions, ...\n",
    "    \"\"\"\n",
    "    cc_rows = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cc_dir = root / \"codecarbon\"\n",
    "        if not cc_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Prefer the main emissions.csv; ignore .bak variants here\n",
    "        cc_path = cc_dir / \"emissions.csv\"\n",
    "        if not cc_path.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(cc_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read CodeCarbon CSV at {cc_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"root\"] = str(root)\n",
    "        df[\"cluster\"] = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        cc_rows.append(df)\n",
    "\n",
    "    if not cc_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    cc_df = pd.concat(cc_rows, ignore_index=True)\n",
    "\n",
    "    # Normalize names we use often\n",
    "    cc_df.rename(\n",
    "        columns={\n",
    "            \"energy_consumed\": \"energy_consumed_kwh\",\n",
    "            \"cpu_energy\": \"cpu_energy_kwh\",\n",
    "            \"gpu_energy\": \"gpu_energy_kwh\",\n",
    "            \"ram_energy\": \"ram_energy_kwh\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return cc_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage Metrics Normalization\n",
    "\n",
    "STAGE_DEFAULTS: Dict[str, Any] = {\n",
    "    # identity / meta\n",
    "    \"root\": None,\n",
    "    \"cluster\": None,\n",
    "    \"stage_dir\": None,\n",
    "    \"experiment_id\": None,\n",
    "    \"experiment_name\": None,\n",
    "    \"stage_id\": None,\n",
    "    \"stage_name\": None,\n",
    "    \"source\": None,  # \"summary\", \"stage_json\", \"snapshot\", \"codecarbon_only\"\n",
    "\n",
    "    # snapshot info\n",
    "    \"is_snapshot\": False,\n",
    "    \"snapshot_step\": None,\n",
    "    \"snapshot_type\": None,\n",
    "    \"snapshot_time\": None,\n",
    "\n",
    "    # config metadata\n",
    "    \"total_energy_policy\": None,\n",
    "    \"pipeline\": None,\n",
    "    \"student_size\": None,\n",
    "    \"dataset_choice\": None,\n",
    "    \"kd_temperature\": None,\n",
    "    \"kd_alpha\": None,\n",
    "    \"sft_max_new_tokens\": None,\n",
    "\n",
    "    # timing / tokens\n",
    "    \"start_time\": None,\n",
    "    \"end_time\": None,\n",
    "    \"duration_seconds\": None,\n",
    "    \"tokens_processed\": None,\n",
    "    \"tokens_per_second\": None,\n",
    "\n",
    "    # GPU metrics\n",
    "    \"gpu_energy_joules\": None,\n",
    "    \"gpu_avg_power_watts\": None,\n",
    "    \"gpu_peak_power_watts\": None,\n",
    "    \"nvml_poll_interval_ms\": None,\n",
    "\n",
    "    # CPU + total\n",
    "    \"cpu_energy_joules\": None,\n",
    "    \"total_energy_joules\": None,\n",
    "    \"total_energy_kwh\": None,\n",
    "    \"joules_per_token\": None,\n",
    "    \"kwh_total\": None,\n",
    "\n",
    "    # CodeCarbon normalized\n",
    "    \"total_codecarbon_energy_kwh\": None,\n",
    "    \"codecarbon_emissions_kg\": None,\n",
    "    \"codecarbon_cpu_energy_kwh\": None,\n",
    "    \"codecarbon_gpu_energy_kwh\": None,\n",
    "    \"codecarbon_ram_energy_kwh\": None,\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_stage_metrics_dict(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize a StageMetrics-like dict (from stage JSON or experiment_summary)\n",
    "    into the canonical keys in STAGE_DEFAULTS (no root/cluster/stage_dir/source).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    # Basic identifiers\n",
    "    out[\"stage_id\"] = raw.get(\"stage_id\")\n",
    "    out[\"stage_name\"] = raw.get(\"stage_name\")\n",
    "\n",
    "    # Timing / tokens\n",
    "    out[\"start_time\"] = raw.get(\"start_time\")\n",
    "    out[\"end_time\"] = raw.get(\"end_time\")\n",
    "    out[\"duration_seconds\"] = raw.get(\"duration_seconds\")\n",
    "    out[\"tokens_processed\"] = raw.get(\"tokens_processed\")\n",
    "    out[\"tokens_per_second\"] = raw.get(\"tokens_per_second\")\n",
    "\n",
    "    # GPU\n",
    "    out[\"gpu_energy_joules\"] = raw.get(\"gpu_energy_joules\")\n",
    "    out[\"gpu_avg_power_watts\"] = raw.get(\"gpu_avg_power_watts\")\n",
    "    out[\"gpu_peak_power_watts\"] = raw.get(\"gpu_peak_power_watts\")\n",
    "    out[\"nvml_poll_interval_ms\"] = raw.get(\"nvml_poll_interval_ms\")\n",
    "\n",
    "    # CPU\n",
    "    out[\"cpu_energy_joules\"] = raw.get(\"cpu_energy_joules\")\n",
    "\n",
    "    # CodeCarbon variants:\n",
    "    # new-style: total_codecarbon_energy_kwh\n",
    "    # old-style:  codecarbon_energy_kwh\n",
    "    cc_total = raw.get(\"total_codecarbon_energy_kwh\", None)\n",
    "    if cc_total is None:\n",
    "        cc_total = raw.get(\"codecarbon_energy_kwh\", None)\n",
    "    out[\"total_codecarbon_energy_kwh\"] = cc_total\n",
    "\n",
    "    out[\"codecarbon_emissions_kg\"] = raw.get(\"codecarbon_emissions_kg\")\n",
    "    out[\"codecarbon_cpu_energy_kwh\"] = raw.get(\"codecarbon_cpu_energy_kwh\")\n",
    "    out[\"codecarbon_gpu_energy_kwh\"] = raw.get(\"codecarbon_gpu_energy_kwh\")\n",
    "    out[\"codecarbon_ram_energy_kwh\"] = raw.get(\"codecarbon_ram_energy_kwh\")\n",
    "\n",
    "    # Totals / derived\n",
    "    out[\"total_energy_joules\"] = raw.get(\"total_energy_joules\")\n",
    "    out[\"total_energy_kwh\"] = raw.get(\"total_energy_kwh\")\n",
    "    out[\"joules_per_token\"] = raw.get(\"joules_per_token\")\n",
    "    out[\"kwh_total\"] = raw.get(\"kwh_total\")\n",
    "\n",
    "    # Snapshot info (may or may not be present)\n",
    "    out[\"is_snapshot\"] = bool(raw.get(\"snapshot\", False))\n",
    "    out[\"snapshot_step\"] = raw.get(\"snapshot_step\")\n",
    "    out[\"snapshot_type\"] = raw.get(\"snapshot_type\")\n",
    "    out[\"snapshot_time\"] = raw.get(\"snapshot_time\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80eee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Metadata extraction\n",
    "\n",
    "def _infer_pipeline_and_student(exp_name: str) -> (Optional[str], Optional[str]):\n",
    "    s = exp_name.lower()\n",
    "    pipeline = None\n",
    "    if s.startswith(\"kd_\"):\n",
    "        pipeline = \"kd\"\n",
    "    elif s.startswith(\"sft_\"):\n",
    "        pipeline = \"sft\"\n",
    "    elif \"true\" in s:\n",
    "        pipeline = \"true_sft\"\n",
    "\n",
    "    student_size = None\n",
    "    if \"to_1b\" in s:\n",
    "        student_size = \"1B\"\n",
    "    elif \"to_7b\" in s:\n",
    "        student_size = \"7B\"\n",
    "    elif \"to_13b\" in s or \"13b\" in s:\n",
    "        student_size = \"13B\"\n",
    "\n",
    "    return pipeline, student_size\n",
    "\n",
    "\n",
    "def load_config_meta(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan all config_*.json files and extract per-(root, stage_dir, stage_name) metadata:\n",
    "        experiment_name, total_energy_policy, pipeline, student_size, kd_temperature, kd_alpha,\n",
    "        sft_max_new_tokens, dataset_choice, etc.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        for cfg_path in root.rglob(\"config_*.json\"):\n",
    "            try:\n",
    "                with open(cfg_path) as f:\n",
    "                    cfg = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read config at {cfg_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            stage_name = cfg.get(\"stage_name\")\n",
    "            stage_id = cfg.get(\"stage_id\")\n",
    "\n",
    "            config = cfg.get(\"config\", {})\n",
    "            exp_cfg = config.get(\"experiment\", {})\n",
    "            data_cfg = config.get(\"data\", {})\n",
    "            train_cfg = config.get(\"training\", {})\n",
    "            kd_cfg = config.get(\"kd\", config.get(\"distillation\", {}))  # handle naming\n",
    "            energy_cfg = config.get(\"energy\", {})\n",
    "\n",
    "            exp_name = exp_cfg.get(\"name\", stage_name)\n",
    "            pipeline, student_size = _infer_pipeline_and_student(exp_name)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"root\": str(root),\n",
    "                    \"cluster\": cluster,\n",
    "                    \"stage_dir\": str(cfg_path.parent),\n",
    "                    \"stage_name\": stage_name,\n",
    "                    \"stage_id\": stage_id,\n",
    "                    \"experiment_name\": exp_name,\n",
    "                    \"total_energy_policy\": energy_cfg.get(\"total_energy_policy\"),\n",
    "                    \"pipeline\": pipeline,\n",
    "                    \"student_size\": student_size,\n",
    "                    \"dataset_choice\": data_cfg.get(\"dataset_choice\"),\n",
    "                    \"kd_temperature\": kd_cfg.get(\"temperature\"),\n",
    "                    \"kd_alpha\": kd_cfg.get(\"alpha\"),\n",
    "                    \"sft_max_new_tokens\": train_cfg.get(\"max_new_tokens\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage folder -> records\n",
    "\n",
    "def _collect_from_experiment_summary(\n",
    "    summary_path: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Given an experiment_summary.json, return a list of normalized stage records (source='summary').\n",
    "    \"\"\"\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    with open(summary_path) as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    exp_id = summary.get(\"experiment_id\")\n",
    "    exp_name = summary.get(\"experiment_name\")\n",
    "    stages = summary.get(\"stages\", {})\n",
    "\n",
    "    for stage_name, raw in stages.items():\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        # Default: parent of the summary (e.g., run_dir); overridden if config meta is found\n",
    "        base[\"stage_dir\"] = str(summary_path.parent)\n",
    "        base[\"experiment_id\"] = exp_id\n",
    "        base[\"experiment_name\"] = exp_name\n",
    "        base[\"source\"] = \"summary\"\n",
    "\n",
    "        # Normalize metrics\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # Attach config meta if available.\n",
    "        # Match by root + stage_name, then prefer the config's stage_dir.\n",
    "        m = cfg_meta[\n",
    "            (cfg_meta[\"root\"] == str(root))\n",
    "            & (cfg_meta[\"stage_name\"] == stage_name)\n",
    "        ]\n",
    "        if not m.empty:\n",
    "            meta_row = m.iloc[0].to_dict()\n",
    "\n",
    "            # Prefer the config's notion of the stage_dir (actual stage folder)\n",
    "            stage_dir_cfg = meta_row.get(\"stage_dir\")\n",
    "            if stage_dir_cfg:\n",
    "                base[\"stage_dir\"] = stage_dir_cfg\n",
    "\n",
    "            # Optionally override stage_id if missing\n",
    "            if base.get(\"stage_id\") is None and meta_row.get(\"stage_id\"):\n",
    "                base[\"stage_id\"] = meta_row[\"stage_id\"]\n",
    "\n",
    "            for k in [\n",
    "                \"total_energy_policy\",\n",
    "                \"pipeline\",\n",
    "                \"student_size\",\n",
    "                \"dataset_choice\",\n",
    "                \"kd_temperature\",\n",
    "                \"kd_alpha\",\n",
    "                \"sft_max_new_tokens\",\n",
    "            ]:\n",
    "                base[k] = meta_row.get(k)\n",
    "\n",
    "        records.append(base)\n",
    "\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def _is_stage_metrics_json(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: JSON files that look like StageMetrics but are not config/env/summary.\n",
    "    Includes snapshots.\n",
    "    \"\"\"\n",
    "    name = path.name\n",
    "    if not name.endswith(\".json\"):\n",
    "        return False\n",
    "    if name.startswith(\"config_\") or name.startswith(\"environment_\"):\n",
    "        return False\n",
    "    if name == \"experiment_summary.json\":\n",
    "        return False\n",
    "    # This will match stage.json and stage__step_*.json (snapshots)\n",
    "    return True\n",
    "\n",
    "\n",
    "def _collect_stage_jsons_in_dir(\n",
    "    stage_dir: Path,\n",
    "    root: Path,\n",
    "    cluster: str,\n",
    "    cfg_meta: pd.DataFrame,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collect stage records from standalone stage JSON files in a stage directory,\n",
    "    aggregate snapshots, and return ONE row per logical stage.\n",
    "\n",
    "    Rules:\n",
    "      - If a final stage JSON exists (source='stage_json', not a snapshot),\n",
    "        use that as the base row.\n",
    "      - If only snapshots exist, pick the latest snapshot (by snapshot_step, then end_time).\n",
    "      - For stages with both final and snapshots, final wins; we can still\n",
    "        use the last snapshot to fill missing fields if needed.\n",
    "    \"\"\"\n",
    "    # Match config for this directory (pipeline, student_size, etc.)\n",
    "    m_dir = cfg_meta[\n",
    "        (cfg_meta[\"root\"] == str(root)) & (cfg_meta[\"stage_dir\"] == str(stage_dir))\n",
    "    ]\n",
    "    cfg_row = m_dir.iloc[0].to_dict() if not m_dir.empty else {}\n",
    "\n",
    "    stage_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for path in stage_dir.glob(\"*.json\"):\n",
    "        if not _is_stage_metrics_json(path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(path) as f:\n",
    "                raw = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read stage JSON at {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Skip JSONs that aren't dicts (or single-element list of dict)\n",
    "        if isinstance(raw, list):\n",
    "            if len(raw) == 1 and isinstance(raw[0], dict):\n",
    "                raw = raw[0]\n",
    "            else:\n",
    "                print(\n",
    "                    f\"[INFO] Skipping JSON at {path} \"\n",
    "                    f\"(top-level list, not a StageMetrics dict)\"\n",
    "                )\n",
    "                continue\n",
    "        elif not isinstance(raw, dict):\n",
    "            print(\n",
    "                f\"[INFO] Skipping JSON at {path} \"\n",
    "                f\"(top-level {type(raw).__name__}, expected dict)\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        base = dict(STAGE_DEFAULTS)\n",
    "        base[\"root\"] = str(root)\n",
    "        base[\"cluster\"] = cluster\n",
    "        base[\"stage_dir\"] = str(stage_dir)\n",
    "        base[\"experiment_name\"] = cfg_row.get(\"experiment_name\")\n",
    "        base[\"source\"] = \"snapshot\" if raw.get(\"snapshot\") else \"stage_json\"\n",
    "\n",
    "        # Normalize StageMetrics-style dict into our standard fields\n",
    "        norm = _normalize_stage_metrics_dict(raw)\n",
    "        base.update(norm)\n",
    "\n",
    "        # If JSON didn't carry stage_name, fall back to folder name\n",
    "        if not base.get(\"stage_name\"):\n",
    "            base[\"stage_name\"] = stage_dir.name\n",
    "\n",
    "        # Attach config meta\n",
    "        for k in [\n",
    "            \"total_energy_policy\",\n",
    "            \"pipeline\",\n",
    "            \"student_size\",\n",
    "            \"dataset_choice\",\n",
    "            \"kd_temperature\",\n",
    "            \"kd_alpha\",\n",
    "            \"sft_max_new_tokens\",\n",
    "        ]:\n",
    "            base[k] = cfg_row.get(k)\n",
    "\n",
    "        stage_records.append(base)\n",
    "\n",
    "    if not stage_records:\n",
    "        return []\n",
    "\n",
    "    # --- Aggregate to ONE row per logical stage in this directory ---\n",
    "\n",
    "    by_stage: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for rec in stage_records:\n",
    "        key = rec.get(\"stage_id\") or rec[\"stage_name\"]\n",
    "        by_stage.setdefault(key, []).append(rec)\n",
    "\n",
    "    aggregated: List[Dict[str, Any]] = []\n",
    "\n",
    "    for key, recs in by_stage.items():\n",
    "        finals = [\n",
    "            r\n",
    "            for r in recs\n",
    "            if r.get(\"source\") != \"snapshot\" and not r.get(\"is_snapshot\", False)\n",
    "        ]\n",
    "        snapshots = [r for r in recs if r.get(\"source\") == \"snapshot\"]\n",
    "\n",
    "        if finals:\n",
    "            # Prefer the final metrics JSON; if multiple, take the one with the latest end_time.\n",
    "            best = max(finals, key=lambda r: (r.get(\"end_time\") or 0.0))\n",
    "\n",
    "            # Optional: use the latest snapshot as a fallback for missing fields.\n",
    "            if snapshots:\n",
    "                snaps_sorted = sorted(\n",
    "                    snapshots,\n",
    "                    key=lambda r: (\n",
    "                        r.get(\"snapshot_step\") if r.get(\"snapshot_step\") is not None else -1,\n",
    "                        r.get(\"end_time\") or 0.0,\n",
    "                    ),\n",
    "                )\n",
    "                last_snap = snaps_sorted[-1]\n",
    "                for field in STAGE_DEFAULTS.keys():\n",
    "                    if best.get(field) in (None, 0) and last_snap.get(field) not in (None, 0):\n",
    "                        best[field] = last_snap[field]\n",
    "\n",
    "            aggregated.append(best)\n",
    "        else:\n",
    "            # No final file: only snapshots. Pick the latest snapshot as the representative row.\n",
    "            snaps_sorted = sorted(\n",
    "                recs,\n",
    "                key=lambda r: (\n",
    "                    r.get(\"snapshot_step\") if r.get(\"snapshot_step\") is not None else -1,\n",
    "                    r.get(\"end_time\") or 0.0,\n",
    "                ),\n",
    "            )\n",
    "            aggregated.append(snaps_sorted[-1])\n",
    "\n",
    "    return aggregated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aecf649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-stage Dataframe\n",
    "\n",
    "def build_stage_dataframe(log_roots: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main entry point:\n",
    "      - loads config metadata,\n",
    "      - walks all log roots,\n",
    "      - collects StageMetrics from experiment_summary.json and individual stage JSONs,\n",
    "      - returns one big DataFrame with standardized columns.\n",
    "    \"\"\"\n",
    "    cfg_meta = load_config_meta(log_roots)\n",
    "    cc_df = load_codecarbon_logs(log_roots)  # not yet used as fallback, but available\n",
    "\n",
    "    all_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for root in log_roots:\n",
    "        cluster = ROOT_CLUSTER.get(root.name, root.name)\n",
    "        if not root.exists():\n",
    "            continue\n",
    "\n",
    "        # 1) experiment_summary.json files (per run)\n",
    "        for summary_path in root.rglob(\"experiment_summary.json\"):\n",
    "            # Skip copies written into individual stage dirs:\n",
    "            # .../<root>/stages/<stage>/experiment_summary.json\n",
    "            parent = summary_path.parent\n",
    "            if parent.parent.name == \"stages\":\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with open(summary_path) as f:\n",
    "                    summary = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read experiment_summary at {summary_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if \"stages\" in summary:\n",
    "                all_records.extend(\n",
    "                    _collect_from_experiment_summary(summary_path, root, cluster, cfg_meta)\n",
    "                )\n",
    "            else:\n",
    "                # Some summaries might be in an older/global format; skip or handle specially.\n",
    "                pass\n",
    "\n",
    "\n",
    "        # 2) Standalone stage directories: often under root/stages/*, but also\n",
    "        for stage_dir in root.rglob(\"*\"):\n",
    "            if not stage_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Skip the container folder itself (we only want its children)\n",
    "            if stage_dir == root / \"stages\":\n",
    "                continue\n",
    "\n",
    "            # Heuristic: a \"stage dir\" is one that contains some StageMetrics JSON\n",
    "            has_stage_json = any(_is_stage_metrics_json(p) for p in stage_dir.glob(\"*.json\"))\n",
    "            if not has_stage_json:\n",
    "                continue\n",
    "\n",
    "            records = _collect_stage_jsons_in_dir(stage_dir, root, cluster, cfg_meta)\n",
    "            all_records.extend(records)\n",
    "\n",
    "\n",
    "    if not all_records:\n",
    "        return pd.DataFrame(columns=STAGE_DEFAULTS.keys())\n",
    "\n",
    "    stage_df = pd.DataFrame(all_records)\n",
    "\n",
    "    # Optional: deduplicate (e.g., you might want to drop stage_json records\n",
    "    # that correspond exactly to summary records). For now, keep everything\n",
    "    # and let later analysis decide which to use.\n",
    "    return stage_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage_dataframe_for_path(path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience helper to build a standardized stage DataFrame for a specific\n",
    "    log root or stage directory.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs\")\n",
    "    - build_stage_dataframe_for_path(\"runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    - build_stage_dataframe_for_path(\"/abs/path/to/runpod2_logs/stages/sft_32b_to_13b_tulu_nosft\")\n",
    "    \"\"\"\n",
    "    path = Path(path).resolve()\n",
    "\n",
    "    # If they passed a specific stage dir under .../stages/<stage_name>\n",
    "    if path.is_dir() and path.name != \"stages\" and path.parent.name == \"stages\":\n",
    "        # /.../<log_root>/stages/<stage_name>\n",
    "        # For /project/.../Energy/runpod2_logs/stages/sft_32b_to_1b_math_nosft\n",
    "        # we want log_root = /project/.../Energy/runpod2_logs\n",
    "        log_root = path.parent.parent  # == path.parents[1]\n",
    "        filter_prefix = str(path)\n",
    "    elif path.is_dir() and path.name == \"stages\":\n",
    "        # They pointed at the stages/ directory: restrict to that subtree\n",
    "        log_root = path.parent\n",
    "        filter_prefix = str(path)\n",
    "    else:\n",
    "        # Treat as a log root\n",
    "        log_root = path\n",
    "        filter_prefix = str(log_root)\n",
    "\n",
    "    df = build_stage_dataframe([log_root])\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # If they gave a root, no extra filtering\n",
    "    if filter_prefix == str(log_root):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    # Otherwise restrict to that specific stage subtree\n",
    "    stage_dirs = df[\"stage_dir\"].astype(str)\n",
    "    mask = stage_dirs.str.startswith(filter_prefix)\n",
    "    return df[mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c1e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose what you want to process\n",
    "# - Leave `paths` empty to use default LOG_ROOTS\n",
    "# - Or set it to one or more specific paths, e.g. a single stage dir\n",
    "paths = [\n",
    "    \"/home/klambert/projects/aip-craffel/klambert/Energy/runpod2_logs/\",\n",
    "    \"/home/klambert/projects/aip-craffel/klambert/Energy/logs\",\n",
    "    \"/home/klambert/projects/aip-craffel/klambert/Energy/trillium-logs\",    \n",
    "    ]\n",
    "output = \"stage_metrics.csv\"\n",
    "\n",
    "if paths:\n",
    "    dfs = [build_stage_dataframe_for_path(p) for p in paths]\n",
    "    df = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]\n",
    "else:\n",
    "    df = build_stage_dataframe(LOG_ROOTS)\n",
    "\n",
    "output_path = Path(output)\n",
    "file_exists = output_path.exists()\n",
    "\n",
    "display(df.head())\n",
    "df.to_csv(\n",
    "    output_path,\n",
    "    mode=\"a\" if file_exists else \"w\",   # append if exists, else write\n",
    "    header=not file_exists,            # write header only if new file\n",
    "    index=False,\n",
    ")\n",
    "print(f\"Saved {output} with {len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a465a0a",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de091677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the aggregated stage metrics CSV\n",
    "if \"output_path\" in globals():\n",
    "    stage_metrics_path = output_path\n",
    "else:\n",
    "    stage_metrics_path = Path(\"stage_metrics.csv\")\n",
    "\n",
    "stage_df_raw = pd.read_csv(stage_metrics_path)\n",
    "print(f\"\\n=== Loaded aggregated stage metrics ===\")\n",
    "print(f\"Path: {stage_metrics_path}\")\n",
    "print(f\"Rows: {len(stage_df_raw)}\")\n",
    "print(f\"Columns: {len(stage_df_raw.columns)}\")\n",
    "print(\"First few columns:\", list(stage_df_raw.columns[:10]))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Ensure numeric dtypes for core metric columns\n",
    "# -------------------------------------------------------------------------\n",
    "numeric_cols = [\n",
    "    \"duration_seconds\",\n",
    "    \"tokens_processed\",\n",
    "    \"tokens_per_second\",\n",
    "    \"gpu_energy_joules\",\n",
    "    \"gpu_avg_power_watts\",\n",
    "    \"gpu_peak_power_watts\",\n",
    "    \"nvml_poll_interval_ms\",\n",
    "    \"cpu_energy_joules\",\n",
    "    \"total_energy_joules\",\n",
    "    \"total_energy_kwh\",\n",
    "    \"joules_per_token\",\n",
    "    \"kwh_total\",\n",
    "    \"total_codecarbon_energy_kwh\",\n",
    "    \"codecarbon_emissions_kg\",\n",
    "    \"codecarbon_cpu_energy_kwh\",\n",
    "    \"codecarbon_gpu_energy_kwh\",\n",
    "    \"codecarbon_ram_energy_kwh\",\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in stage_df_raw.columns:\n",
    "        stage_df_raw[col] = pd.to_numeric(stage_df_raw[col], errors=\"coerce\")\n",
    "\n",
    "stage_df_all = stage_df_raw.copy()\n",
    "\n",
    "print(\"\\n=== Non-null counts for numeric columns ===\")\n",
    "print(stage_df_all[numeric_cols].notna().sum().sort_values())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Canonical energy / throughput columns\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Best-effort total energy in kWh\n",
    "energy_kwh = None\n",
    "energy_sources_used = []\n",
    "\n",
    "if \"total_energy_kwh\" in stage_df_all.columns:\n",
    "    energy_kwh = stage_df_all[\"total_energy_kwh\"].copy()\n",
    "    energy_sources_used.append(\"total_energy_kwh\")\n",
    "\n",
    "if \"total_codecarbon_energy_kwh\" in stage_df_all.columns:\n",
    "    if energy_kwh is None:\n",
    "        energy_kwh = stage_df_all[\"total_codecarbon_energy_kwh\"].copy()\n",
    "    else:\n",
    "        missing_mask = energy_kwh.isna() & stage_df_all[\"total_codecarbon_energy_kwh\"].notna()\n",
    "        energy_kwh = energy_kwh.fillna(stage_df_all[\"total_codecarbon_energy_kwh\"])\n",
    "        if missing_mask.any():\n",
    "            print(\n",
    "                f\"[INFO] Filled energy_kwh from total_codecarbon_energy_kwh \"\n",
    "                f\"for {missing_mask.sum()} rows\"\n",
    "            )\n",
    "    energy_sources_used.append(\"total_codecarbon_energy_kwh\")\n",
    "\n",
    "if \"kwh_total\" in stage_df_all.columns:\n",
    "    if energy_kwh is None:\n",
    "        energy_kwh = stage_df_all[\"kwh_total\"].copy()\n",
    "    else:\n",
    "        missing_mask = energy_kwh.isna() & stage_df_all[\"kwh_total\"].notna()\n",
    "        energy_kwh = energy_kwh.fillna(stage_df_all[\"kwh_total\"])\n",
    "        if missing_mask.any():\n",
    "            print(\n",
    "                f\"[INFO] Filled energy_kwh from kwh_total \"\n",
    "                f\"for {missing_mask.sum()} rows\"\n",
    "            )\n",
    "    energy_sources_used.append(\"kwh_total\")\n",
    "\n",
    "if energy_kwh is None:\n",
    "    stage_df_all[\"energy_kwh\"] = np.nan\n",
    "    print(\"[WARN] Could not construct energy_kwh from any known columns.\")\n",
    "else:\n",
    "    stage_df_all[\"energy_kwh\"] = energy_kwh\n",
    "\n",
    "print(\"\\n=== energy_kwh summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"energy_kwh\"].notna().sum())\n",
    "print(\"Rows with NaN energy_kwh:\", stage_df_all[\"energy_kwh\"].isna().sum())\n",
    "if energy_sources_used:\n",
    "    print(\"Sources considered for energy_kwh:\", \", \".join(energy_sources_used))\n",
    "\n",
    "# GPU energy in kWh: prefer direct Joules, fall back to CodeCarbon\n",
    "stage_df_all[\"gpu_energy_kwh\"] = np.nan\n",
    "if \"gpu_energy_joules\" in stage_df_all.columns:\n",
    "    mask_gpu_j = stage_df_all[\"gpu_energy_joules\"].notna()\n",
    "    stage_df_all.loc[mask_gpu_j, \"gpu_energy_kwh\"] = (\n",
    "        stage_df_all.loc[mask_gpu_j, \"gpu_energy_joules\"] / 3.6e6\n",
    "    )\n",
    "    print(\n",
    "        f\"[INFO] Converted gpu_energy_joules -> gpu_energy_kwh \"\n",
    "        f\"for {mask_gpu_j.sum()} rows\"\n",
    "    )\n",
    "\n",
    "if \"codecarbon_gpu_energy_kwh\" in stage_df_all.columns:\n",
    "    mask_fill = stage_df_all[\"gpu_energy_kwh\"].isna() & stage_df_all[\"codecarbon_gpu_energy_kwh\"].notna()\n",
    "    stage_df_all.loc[mask_fill, \"gpu_energy_kwh\"] = stage_df_all.loc[\n",
    "        mask_fill, \"codecarbon_gpu_energy_kwh\"\n",
    "    ]\n",
    "    print(\n",
    "        f\"[INFO] Filled gpu_energy_kwh from codecarbon_gpu_energy_kwh \"\n",
    "        f\"for {mask_fill.sum()} rows\"\n",
    "    )\n",
    "\n",
    "# CPU energy in kWh: prefer direct Joules, fall back to CodeCarbon\n",
    "stage_df_all[\"cpu_energy_kwh\"] = np.nan\n",
    "if \"cpu_energy_joules\" in stage_df_all.columns:\n",
    "    mask_cpu_j = stage_df_all[\"cpu_energy_joules\"].notna()\n",
    "    stage_df_all.loc[mask_cpu_j, \"cpu_energy_kwh\"] = (\n",
    "        stage_df_all.loc[mask_cpu_j, \"cpu_energy_joules\"] / 3.6e6\n",
    "    )\n",
    "    print(\n",
    "        f\"[INFO] Converted cpu_energy_joules -> cpu_energy_kwh \"\n",
    "        f\"for {mask_cpu_j.sum()} rows\"\n",
    "    )\n",
    "\n",
    "if \"codecarbon_cpu_energy_kwh\" in stage_df_all.columns:\n",
    "    mask_fill = stage_df_all[\"cpu_energy_kwh\"].isna() & stage_df_all[\"codecarbon_cpu_energy_kwh\"].notna()\n",
    "    stage_df_all.loc[mask_fill, \"cpu_energy_kwh\"] = stage_df_all.loc[\n",
    "        mask_fill, \"codecarbon_cpu_energy_kwh\"\n",
    "    ]\n",
    "    print(\n",
    "        f\"[INFO] Filled cpu_energy_kwh from codecarbon_cpu_energy_kwh \"\n",
    "        f\"for {mask_fill.sum()} rows\"\n",
    "    )\n",
    "\n",
    "# Joules per token: prefer precomputed, else total_energy_joules / tokens_processed\n",
    "if \"joules_per_token\" in stage_df_all.columns:\n",
    "    stage_df_all[\"energy_j_per_token\"] = stage_df_all[\"joules_per_token\"]\n",
    "else:\n",
    "    stage_df_all[\"energy_j_per_token\"] = np.nan\n",
    "\n",
    "mask_need_jpt = stage_df_all[\"energy_j_per_token\"].isna()\n",
    "if \"total_energy_joules\" in stage_df_all.columns and \"tokens_processed\" in stage_df_all.columns:\n",
    "    denom = stage_df_all[\"tokens_processed\"].replace({0: np.nan})\n",
    "    jpt_mask = mask_need_jpt & stage_df_all[\"total_energy_joules\"].notna() & denom.notna()\n",
    "    stage_df_all.loc[jpt_mask, \"energy_j_per_token\"] = (\n",
    "        stage_df_all.loc[jpt_mask, \"total_energy_joules\"] / denom[jpt_mask]\n",
    "    )\n",
    "    if jpt_mask.any():\n",
    "        print(\n",
    "            f\"[INFO] Computed energy_j_per_token as total_energy_joules/tokens_processed \"\n",
    "            f\"for {jpt_mask.sum()} rows\"\n",
    "        )\n",
    "\n",
    "print(\"\\n=== energy_j_per_token summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"energy_j_per_token\"].notna().sum())\n",
    "\n",
    "# Tokens per second: prefer precomputed, else tokens_processed / duration_seconds\n",
    "if \"tokens_per_second\" in stage_df_all.columns:\n",
    "    stage_df_all[\"tokens_per_sec\"] = stage_df_all[\"tokens_per_second\"]\n",
    "else:\n",
    "    stage_df_all[\"tokens_per_sec\"] = np.nan\n",
    "\n",
    "mask_need_tps = stage_df_all[\"tokens_per_sec\"].isna()\n",
    "if \"duration_seconds\" in stage_df_all.columns and \"tokens_processed\" in stage_df_all.columns:\n",
    "    dur = stage_df_all[\"duration_seconds\"].replace({0: np.nan})\n",
    "    tps_mask = mask_need_tps & dur.notna() & stage_df_all[\"tokens_processed\"].notna()\n",
    "    stage_df_all.loc[tps_mask, \"tokens_per_sec\"] = (\n",
    "        stage_df_all.loc[tps_mask, \"tokens_processed\"] / dur[tps_mask]\n",
    "    )\n",
    "    if tps_mask.any():\n",
    "        print(\n",
    "            f\"[INFO] Computed tokens_per_sec as tokens_processed/duration_seconds \"\n",
    "            f\"for {tps_mask.sum()} rows\"\n",
    "        )\n",
    "\n",
    "print(\"\\n=== tokens_per_sec summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"tokens_per_sec\"].notna().sum())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Filtered view: drop rows that will mess up per-token metrics\n",
    "# -------------------------------------------------------------------------\n",
    "if \"tokens_processed\" in stage_df_all.columns:\n",
    "    mask_valid_tokens = (\n",
    "        stage_df_all[\"tokens_processed\"].notna()\n",
    "        & (stage_df_all[\"tokens_processed\"] > 0)\n",
    "    )\n",
    "    stage_df_clean = stage_df_all[mask_valid_tokens].copy()\n",
    "else:\n",
    "    stage_df_clean = stage_df_all.copy()\n",
    "\n",
    "print(\"\\n=== Clean vs all rows ===\")\n",
    "print(f\"stage_df_all:   {len(stage_df_all)} rows\")\n",
    "print(\n",
    "    f\"stage_df_clean: {len(stage_df_clean)} rows \"\n",
    "    \"(tokens_processed > 0 where available)\"\n",
    ")\n",
    "\n",
    "display(stage_df_clean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315a80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Tag stages, runs, and cells (Step 2)\n",
    "\n",
    "def _normalize_str(s: pd.Series, to_lower: bool = True) -> pd.Series:\n",
    "    \"\"\"Basic string normalization helper.\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    # convert literal \"nan\" (created by astype(str)) back to real NaN\n",
    "    s = s.replace({\"nan\": np.nan})\n",
    "    if to_lower:\n",
    "        s = s.str.lower()\n",
    "    return s\n",
    "\n",
    "# Normalize key identifier columns for easier grouping\n",
    "for col in [\"pipeline\", \"student_size\", \"dataset_choice\", \"stage_name\", \"experiment_name\", \"source\"]:\n",
    "    if col in stage_df_all.columns:\n",
    "        # pipeline & dataset_choice we want consistently lowercased\n",
    "        to_lower = col in [\"pipeline\", \"dataset_choice\", \"stage_name\", \"source\"]\n",
    "        stage_df_all[col] = _normalize_str(stage_df_all[col], to_lower=to_lower)\n",
    "\n",
    "print(\"\\n=== Identifier column samples ===\")\n",
    "for col in [\"pipeline\", \"student_size\", \"dataset_choice\", \"stage_name\"]:\n",
    "    if col in stage_df_all.columns:\n",
    "        print(f\"{col}: {stage_df_all[col].dropna().unique()[:5]}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Stage role inference\n",
    "# -------------------------------------------------------------------------\n",
    "def infer_stage_role(row: pd.Series) -> str:\n",
    "    name = str(row.get(\"stage_name\", \"\") or \"\").lower()\n",
    "    pipeline = str(row.get(\"pipeline\", \"\") or \"\").lower()\n",
    "    source = str(row.get(\"source\", \"\") or \"\").lower()\n",
    "\n",
    "    # Synthetic / teacher-side work\n",
    "    if \"synthetic\" in name:\n",
    "        if \"generation\" in name:\n",
    "            return \"teacher_generation\"\n",
    "        return \"teacher_processing\"\n",
    "\n",
    "    # Dataset preprocessing\n",
    "    if \"preprocess\" in name:\n",
    "        return \"data_preprocess\"\n",
    "\n",
    "    # Evaluation stages (future-proofed for eval names)\n",
    "    if (\n",
    "        \"eval\" in name\n",
    "        or \"gsm8k\" in name\n",
    "        or \"mmlu\" in name\n",
    "        or \"alpaca\" in name\n",
    "        or \"ifeval\" in name\n",
    "    ):\n",
    "        return \"evaluation\"\n",
    "\n",
    "    # Main training pipelines\n",
    "    if pipeline in {\"sft\", \"kd\", \"dpo\"}:\n",
    "        # Distinguish between summary and per-stage json if you want\n",
    "        if source == \"summary\":\n",
    "            return \"train_summary\"\n",
    "        return \"student_train\"\n",
    "\n",
    "    # Fallbacks\n",
    "    if source == \"summary\":\n",
    "        return \"summary_only\"\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "stage_df_all[\"stage_role\"] = stage_df_all.apply(infer_stage_role, axis=1)\n",
    "\n",
    "print(\"\\n=== stage_role value counts ===\")\n",
    "print(stage_df_all[\"stage_role\"].value_counts())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Run ID: group stages belonging to the same logical experiment\n",
    "# -------------------------------------------------------------------------\n",
    "if \"experiment_name\" in stage_df_all.columns:\n",
    "    stage_df_all[\"run_id\"] = stage_df_all[\"experiment_name\"].fillna(\n",
    "        stage_df_all[\"stage_name\"]\n",
    "    )\n",
    "else:\n",
    "    stage_df_all[\"run_id\"] = stage_df_all[\"stage_name\"]\n",
    "\n",
    "print(\"\\nUnique run_id count:\", stage_df_all[\"run_id\"].nunique())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3×3 grid cell ID: pipeline × student_size × dataset_choice\n",
    "# -------------------------------------------------------------------------\n",
    "required_for_cell = [\"pipeline\", \"student_size\", \"dataset_choice\"]\n",
    "for col in required_for_cell:\n",
    "    if col in stage_df_all.columns:\n",
    "        stage_df_all[col] = stage_df_all[col].replace(\"\", np.nan)\n",
    "\n",
    "if all(col in stage_df_all.columns for col in required_for_cell):\n",
    "    missing_any = stage_df_all[required_for_cell].isna().any(axis=1)\n",
    "    stage_df_all[\"cell_id\"] = np.where(\n",
    "        missing_any,\n",
    "        np.nan,\n",
    "        (\n",
    "            stage_df_all[\"pipeline\"].str.lower()\n",
    "            + \"_\"\n",
    "            + stage_df_all[\"student_size\"].astype(str)\n",
    "            + \"_\"\n",
    "            + stage_df_all[\"dataset_choice\"].str.lower()\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    stage_df_all[\"cell_id\"] = np.nan\n",
    "\n",
    "print(\"\\n=== cell_id summary ===\")\n",
    "print(\"Non-null cell_id rows:\", stage_df_all[\"cell_id\"].notna().sum())\n",
    "print(\"Unique cell_ids:\", stage_df_all[\"cell_id\"].dropna().nunique())\n",
    "print(stage_df_all[\"cell_id\"].dropna().unique())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Keep stage_df_clean in sync with new columns\n",
    "# -------------------------------------------------------------------------\n",
    "if \"tokens_processed\" in stage_df_all.columns:\n",
    "    valid = stage_df_all[\"tokens_processed\"].notna() & (stage_df_all[\"tokens_processed\"] > 0)\n",
    "    stage_df_clean = stage_df_all[valid].copy()\n",
    "else:\n",
    "    stage_df_clean = stage_df_all.copy()\n",
    "\n",
    "print(\"\\n=== Preview with tagging columns ===\")\n",
    "display(\n",
    "    stage_df_all[\n",
    "        [\n",
    "            \"stage_name\",\n",
    "            \"pipeline\",\n",
    "            \"student_size\",\n",
    "            \"dataset_choice\",\n",
    "            \"stage_role\",\n",
    "            \"run_id\",\n",
    "            \"cell_id\",\n",
    "            \"energy_kwh\",\n",
    "            \"energy_j_per_token\",\n",
    "            \"tokens_per_sec\",\n",
    "        ]\n",
    "    ].head(15)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25328066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Aggregate pipeline-level metrics (Step 3)\n",
    "\n",
    "print(\"\\n\\n=== Step 3: Aggregating pipeline-level (cell) metrics ===\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Effective energy in Joules for aggregation\n",
    "# -------------------------------------------------------------------------\n",
    "if \"total_energy_joules\" in stage_df_all.columns:\n",
    "    stage_df_all[\"energy_joules_eff\"] = stage_df_all[\"total_energy_joules\"]\n",
    "    # Fill missing with kWh-based estimate if available\n",
    "    if \"energy_kwh\" in stage_df_all.columns:\n",
    "        mask_fill_j = stage_df_all[\"energy_joules_eff\"].isna() & stage_df_all[\"energy_kwh\"].notna()\n",
    "        stage_df_all.loc[mask_fill_j, \"energy_joules_eff\"] = (\n",
    "            stage_df_all.loc[mask_fill_j, \"energy_kwh\"] * 3.6e6\n",
    "        )\n",
    "        if mask_fill_j.any():\n",
    "            print(\n",
    "                f\"[INFO] Filled energy_joules_eff from energy_kwh for \"\n",
    "                f\"{mask_fill_j.sum()} rows\"\n",
    "            )\n",
    "else:\n",
    "    if \"energy_kwh\" in stage_df_all.columns:\n",
    "        stage_df_all[\"energy_joules_eff\"] = stage_df_all[\"energy_kwh\"] * 3.6e6\n",
    "        print(\"[INFO] Constructed energy_joules_eff from energy_kwh for all rows.\")\n",
    "    else:\n",
    "        stage_df_all[\"energy_joules_eff\"] = np.nan\n",
    "        print(\"[WARN] No total_energy_joules or energy_kwh; energy_joules_eff is NaN.\")\n",
    "\n",
    "print(\"\\n=== energy_joules_eff summary ===\")\n",
    "print(\"Non-null rows:\", stage_df_all[\"energy_joules_eff\"].notna().sum())\n",
    "print(\"Rows with NaN energy_joules_eff:\", stage_df_all[\"energy_joules_eff\"].isna().sum())\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Stage-role groupings for aggregation\n",
    "# -------------------------------------------------------------------------\n",
    "STUDENT_ROLES = {\"student_train\", \"train_summary\"}\n",
    "TEACHER_ROLES = {\"teacher_generation\", \"teacher_processing\", \"data_preprocess\"}\n",
    "EVAL_ROLES = {\"evaluation\"}\n",
    "\n",
    "def _safe_div(num, denom):\n",
    "    \"\"\"Division with protection against zero / NaN denom.\"\"\"\n",
    "    if denom is None or pd.isna(denom) or denom == 0:\n",
    "        return np.nan\n",
    "    return num / denom\n",
    "\n",
    "def aggregate_cell_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate stage-level metrics into pipeline-level (cell-level) metrics.\n",
    "\n",
    "    Grouping keys:\n",
    "        pipeline, student_size, dataset_choice, cell_id, run_id\n",
    "\n",
    "    For each group, we compute:\n",
    "        - student_tokens / teacher_tokens\n",
    "        - student_duration_s / teacher_duration_s\n",
    "        - student_kwh / teacher_kwh / eval_kwh / other_kwh / total_kwh_all\n",
    "        - GPU/CPU kWh for student stages\n",
    "        - total / student / teacher energy in Joules\n",
    "        - tokens_per_sec_student\n",
    "        - energy_j_per_token_total\n",
    "        - energy_j_per_token_student\n",
    "    \"\"\"\n",
    "    group_cols = [\"pipeline\", \"student_size\", \"dataset_choice\", \"cell_id\", \"run_id\"]\n",
    "    group_cols = [c for c in group_cols if c in df.columns]\n",
    "\n",
    "    print(\"\\n[DEBUG] Aggregating with group columns:\", group_cols)\n",
    "    grouped = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for key, rows in grouped:\n",
    "        # key can be a scalar or tuple depending on number of group_cols\n",
    "        if not isinstance(key, tuple):\n",
    "            key = (key,)\n",
    "        record = dict(zip(group_cols, key))\n",
    "\n",
    "        # Split by stage_role\n",
    "        student_rows = rows[rows[\"stage_role\"].isin(STUDENT_ROLES)]\n",
    "        teacher_rows = rows[rows[\"stage_role\"].isin(TEACHER_ROLES)]\n",
    "        eval_rows = rows[rows[\"stage_role\"].isin(EVAL_ROLES)]\n",
    "        other_rows = rows[\n",
    "            ~rows[\"stage_role\"].isin(STUDENT_ROLES | TEACHER_ROLES | EVAL_ROLES)\n",
    "        ]\n",
    "\n",
    "        # Tokens\n",
    "        student_tokens = (\n",
    "            student_rows[\"tokens_processed\"].sum()\n",
    "            if \"tokens_processed\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "        teacher_tokens = (\n",
    "            teacher_rows[\"tokens_processed\"].sum()\n",
    "            if \"tokens_processed\" in teacher_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        # Durations\n",
    "        student_dur = (\n",
    "            student_rows[\"duration_seconds\"].sum()\n",
    "            if \"duration_seconds\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "        teacher_dur = (\n",
    "            teacher_rows[\"duration_seconds\"].sum()\n",
    "            if \"duration_seconds\" in teacher_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        # Energy in kWh\n",
    "        student_kwh = student_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in student_rows.columns else np.nan\n",
    "        teacher_kwh = teacher_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in teacher_rows.columns else np.nan\n",
    "        eval_kwh = eval_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in eval_rows.columns else np.nan\n",
    "        other_kwh = other_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in other_rows.columns else np.nan\n",
    "        total_kwh_all = rows[\"energy_kwh\"].sum() if \"energy_kwh\" in rows.columns else np.nan\n",
    "\n",
    "        # Energy in Joules (effective)\n",
    "        student_j = student_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in student_rows.columns else np.nan\n",
    "        teacher_j = teacher_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in teacher_rows.columns else np.nan\n",
    "        eval_j = eval_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in eval_rows.columns else np.nan\n",
    "        other_j = other_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in other_rows.columns else np.nan\n",
    "        total_j_all = rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in rows.columns else np.nan\n",
    "\n",
    "        # GPU / CPU energy (student-only for now; you can expand to teacher/eval later)\n",
    "        student_gpu_kwh = (\n",
    "            student_rows[\"gpu_energy_kwh\"].sum()\n",
    "            if \"gpu_energy_kwh\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "        student_cpu_kwh = (\n",
    "            student_rows[\"cpu_energy_kwh\"].sum()\n",
    "            if \"cpu_energy_kwh\" in student_rows.columns\n",
    "            else np.nan\n",
    "        )\n",
    "\n",
    "        # Tokens per second (student)\n",
    "        tokens_per_sec_student = _safe_div(student_tokens, student_dur)\n",
    "\n",
    "        # J/token:\n",
    "        #  - total: teacher + student + eval + other, divided by student tokens\n",
    "        #  - student-only: student energy divided by student tokens\n",
    "        energy_j_per_token_total = _safe_div(total_j_all, student_tokens)\n",
    "        energy_j_per_token_student = _safe_div(student_j, student_tokens)\n",
    "\n",
    "        record.update(\n",
    "            dict(\n",
    "                student_tokens=student_tokens,\n",
    "                teacher_tokens=teacher_tokens,\n",
    "                student_duration_s=student_dur,\n",
    "                teacher_duration_s=teacher_dur,\n",
    "                student_kwh=student_kwh,\n",
    "                teacher_kwh=teacher_kwh,\n",
    "                eval_kwh=eval_kwh,\n",
    "                other_kwh=other_kwh,\n",
    "                total_kwh_all=total_kwh_all,\n",
    "                student_energy_joules=student_j,\n",
    "                teacher_energy_joules=teacher_j,\n",
    "                eval_energy_joules=eval_j,\n",
    "                other_energy_joules=other_j,\n",
    "                total_energy_joules_all=total_j_all,\n",
    "                student_gpu_kwh=student_gpu_kwh,\n",
    "                student_cpu_kwh=student_cpu_kwh,\n",
    "                tokens_per_sec_student=tokens_per_sec_student,\n",
    "                energy_j_per_token_total=energy_j_per_token_total,\n",
    "                energy_j_per_token_student=energy_j_per_token_student,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    agg_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    print(f\"[INFO] Aggregated into {len(agg_df)} rows (one per {group_cols} group).\")\n",
    "    return agg_df\n",
    "\n",
    "cell_metrics_df = aggregate_cell_metrics(stage_df_all)\n",
    "\n",
    "print(\"\\n=== cell_metrics_df basic summary ===\")\n",
    "print(\"Rows:\", len(cell_metrics_df))\n",
    "print(\"Distinct cells (cell_id):\", cell_metrics_df[\"cell_id\"].dropna().nunique() if \"cell_id\" in cell_metrics_df.columns else \"N/A\")\n",
    "print(\"Distinct run_id:\", cell_metrics_df[\"run_id\"].nunique() if \"run_id\" in cell_metrics_df.columns else \"N/A\")\n",
    "\n",
    "print(\"\\n=== Sample of aggregated pipeline-level metrics ===\")\n",
    "cols_to_show = [\n",
    "    \"pipeline\",\n",
    "    \"student_size\",\n",
    "    \"dataset_choice\",\n",
    "    \"cell_id\",\n",
    "    \"run_id\",\n",
    "    \"student_tokens\",\n",
    "    \"student_duration_s\",\n",
    "    \"total_kwh_all\",\n",
    "    \"student_kwh\",\n",
    "    \"teacher_kwh\",\n",
    "    \"eval_kwh\",\n",
    "    \"tokens_per_sec_student\",\n",
    "    \"energy_j_per_token_total\",\n",
    "    \"energy_j_per_token_student\",\n",
    "]\n",
    "cols_to_show = [c for c in cols_to_show if c in cell_metrics_df.columns]\n",
    "display(cell_metrics_df[cols_to_show].head(15))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Optional hook: evaluation metrics (to be added later)\n",
    "# -------------------------------------------------------------------------\n",
    "print(\n",
    "    \"\\n[NOTE] Evaluation metrics (GSM8K, MMLU, AlpacaEval, etc.) \"\n",
    "    \"will be merged into cell_metrics_df in a later step once we \"\n",
    "    \"have a structured eval_metrics CSV.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Iterable\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Normalization helpers\n",
    "# -------------------------------------------------------------------------\n",
    "def _norm_str(x: object) -> str:\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return \"\"\n",
    "    return str(x).strip()\n",
    "\n",
    "def _norm_key(x: object) -> str:\n",
    "    return _norm_str(x).lower()\n",
    "\n",
    "def _as_list(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (list, tuple, set)):\n",
    "        return list(x)\n",
    "    return [x]\n",
    "\n",
    "def _ci_mask(series: pd.Series, values: list[str]) -> pd.Series:\n",
    "    vals = {_norm_key(v) for v in values}\n",
    "    return series.astype(str).str.strip().str.lower().isin(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a44932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Ensure required id columns exist\n",
    "# --------------------------\n",
    "def ensure_identifiers(stage_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = stage_df.copy()\n",
    "\n",
    "    # Normalize some key cols if present\n",
    "    for c in [\"pipeline\", \"dataset_choice\", \"stage_name\", \"source\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "            df.loc[df[c].str.lower() == \"nan\", c] = np.nan\n",
    "            if c in {\"pipeline\", \"dataset_choice\", \"stage_name\", \"source\"}:\n",
    "                df[c] = df[c].str.lower()\n",
    "\n",
    "    # run_id\n",
    "    if \"run_id\" not in df.columns:\n",
    "        if \"experiment_name\" in df.columns:\n",
    "            df[\"run_id\"] = df[\"experiment_name\"].fillna(df.get(\"stage_name\"))\n",
    "        else:\n",
    "            df[\"run_id\"] = df.get(\"stage_name\")\n",
    "\n",
    "    # cell_id\n",
    "    if \"cell_id\" not in df.columns:\n",
    "        needed = [\"pipeline\", \"student_size\", \"dataset_choice\"]\n",
    "        if all(c in df.columns for c in needed):\n",
    "            miss = df[needed].isna().any(axis=1)\n",
    "            df[\"cell_id\"] = np.where(\n",
    "                miss,\n",
    "                np.nan,\n",
    "                df[\"pipeline\"].astype(str).str.lower()\n",
    "                + \"_\"\n",
    "                + df[\"student_size\"].astype(str)\n",
    "                + \"_\"\n",
    "                + df[\"dataset_choice\"].astype(str).str.lower()\n",
    "            )\n",
    "        else:\n",
    "            df[\"cell_id\"] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _safe_div(num, denom):\n",
    "    if denom is None or pd.isna(denom) or denom == 0:\n",
    "        return np.nan\n",
    "    return num / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3850df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Stage -> run/cell aggregation\n",
    "# --------------------------\n",
    "STUDENT_ROLES = {\"student_train\", \"train_summary\"}\n",
    "TEACHER_ROLES = {\"teacher_generation\", \"teacher_processing\", \"data_preprocess\"}\n",
    "EVAL_ROLES = {\"evaluation\"}\n",
    "\n",
    "def aggregate_cell_metrics(stage_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate stage-level rows into run-level/cell-level rows.\n",
    "    Output is grouped by: pipeline, student_size, dataset_choice, cell_id, run_id\n",
    "    \"\"\"\n",
    "    df = ensure_identifiers(stage_df)\n",
    "\n",
    "    # stage_role must exist\n",
    "    if \"stage_role\" not in df.columns:\n",
    "        raise ValueError(\"aggregate_cell_metrics: stage_role missing. Run retag_stage_roles(...) first.\")\n",
    "\n",
    "    group_cols = [c for c in [\"pipeline\",\"student_size\",\"dataset_choice\",\"cell_id\",\"run_id\"] if c in df.columns]\n",
    "    grouped = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "    recs = []\n",
    "    for key, rows in grouped:\n",
    "        if not isinstance(key, tuple):\n",
    "            key = (key,)\n",
    "        rec = dict(zip(group_cols, key))\n",
    "\n",
    "        student_rows = rows[rows[\"stage_role\"].isin(STUDENT_ROLES)]\n",
    "        teacher_rows = rows[rows[\"stage_role\"].isin(TEACHER_ROLES)]\n",
    "        eval_rows    = rows[rows[\"stage_role\"].isin(EVAL_ROLES)]\n",
    "        other_rows   = rows[~rows[\"stage_role\"].isin(STUDENT_ROLES | TEACHER_ROLES | EVAL_ROLES)]\n",
    "\n",
    "        # Tokens/duration\n",
    "        student_tokens = student_rows[\"tokens_processed\"].sum() if \"tokens_processed\" in rows.columns else np.nan\n",
    "        student_dur    = student_rows[\"duration_seconds\"].sum() if \"duration_seconds\" in rows.columns else np.nan\n",
    "\n",
    "        # Energy kWh\n",
    "        student_kwh = student_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in rows.columns else np.nan\n",
    "        teacher_kwh = teacher_rows[\"energy_kwh\"].sum() if \"energy_kwh\" in rows.columns else np.nan\n",
    "        eval_kwh    = eval_rows[\"energy_kwh\"].sum()    if \"energy_kwh\" in rows.columns else np.nan\n",
    "        other_kwh   = other_rows[\"energy_kwh\"].sum()   if \"energy_kwh\" in rows.columns else np.nan\n",
    "        total_kwh_all = rows[\"energy_kwh\"].sum()       if \"energy_kwh\" in rows.columns else np.nan\n",
    "\n",
    "        # Energy Joules (effective)\n",
    "        student_j = student_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in rows.columns else np.nan\n",
    "        teacher_j = teacher_rows[\"energy_joules_eff\"].sum() if \"energy_joules_eff\" in rows.columns else np.nan\n",
    "        eval_j    = eval_rows[\"energy_joules_eff\"].sum()    if \"energy_joules_eff\" in rows.columns else np.nan\n",
    "        other_j   = other_rows[\"energy_joules_eff\"].sum()   if \"energy_joules_eff\" in rows.columns else np.nan\n",
    "        total_j_all = rows[\"energy_joules_eff\"].sum()       if \"energy_joules_eff\" in rows.columns else np.nan\n",
    "\n",
    "        # GPU/CPU (student-only by default)\n",
    "        student_gpu_kwh = student_rows[\"gpu_energy_kwh\"].sum() if \"gpu_energy_kwh\" in rows.columns else np.nan\n",
    "        student_cpu_kwh = student_rows[\"cpu_energy_kwh\"].sum() if \"cpu_energy_kwh\" in rows.columns else np.nan\n",
    "\n",
    "        tokens_per_sec_student = _safe_div(student_tokens, student_dur)\n",
    "        energy_j_per_token_total = _safe_div(total_j_all, student_tokens)\n",
    "        energy_j_per_token_student = _safe_div(student_j, student_tokens)\n",
    "\n",
    "        rec.update({\n",
    "            \"student_tokens\": student_tokens,\n",
    "            \"student_duration_s\": student_dur,\n",
    "            \"student_kwh\": student_kwh,\n",
    "            \"teacher_kwh\": teacher_kwh,\n",
    "            \"eval_kwh\": eval_kwh,\n",
    "            \"other_kwh\": other_kwh,\n",
    "            \"total_kwh_all\": total_kwh_all,\n",
    "            \"student_energy_joules\": student_j,\n",
    "            \"teacher_energy_joules\": teacher_j,\n",
    "            \"eval_energy_joules\": eval_j,\n",
    "            \"other_energy_joules\": other_j,\n",
    "            \"total_energy_joules_all\": total_j_all,\n",
    "            \"student_gpu_kwh\": student_gpu_kwh,\n",
    "            \"student_cpu_kwh\": student_cpu_kwh,\n",
    "            \"tokens_per_sec_student\": tokens_per_sec_student,\n",
    "            \"energy_j_per_token_total\": energy_j_per_token_total,\n",
    "            \"energy_j_per_token_student\": energy_j_per_token_student,\n",
    "        })\n",
    "        recs.append(rec)\n",
    "\n",
    "    return pd.DataFrame.from_records(recs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f89aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Patch A: case-insensitive filter_cells (fixes 7B vs 7b issues)\n",
    "# -------------------------------------------------------------------------\n",
    "def filter_cells(\n",
    "    df: pd.DataFrame,\n",
    "    pipeline: str | list[str] | None = None,\n",
    "    student_size: str | list[str] | None = None,\n",
    "    dataset_choice: str | list[str] | None = None,\n",
    "    run_id: str | list[str] | None = None,\n",
    "    cell_id: str | list[str] | None = None,\n",
    "    *,\n",
    "    case_insensitive: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    pipeline = _as_list(pipeline)\n",
    "    student_size = _as_list(student_size)\n",
    "    dataset_choice = _as_list(dataset_choice)\n",
    "    run_id = _as_list(run_id)\n",
    "    cell_id = _as_list(cell_id)\n",
    "\n",
    "    def _apply(col: str, vals: list[str] | None):\n",
    "        nonlocal out\n",
    "        if vals is None or col not in out.columns:\n",
    "            return\n",
    "        if case_insensitive:\n",
    "            out = out[_ci_mask(out[col], vals)]\n",
    "        else:\n",
    "            out = out[out[col].isin(vals)]\n",
    "\n",
    "    _apply(\"pipeline\", pipeline)\n",
    "    _apply(\"student_size\", student_size)\n",
    "    _apply(\"dataset_choice\", dataset_choice)\n",
    "    _apply(\"run_id\", run_id)\n",
    "    _apply(\"cell_id\", cell_id)\n",
    "\n",
    "    print(f\"[filter_cells] -> {len(out)} rows (from {len(df)} input rows)\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Patch B: stronger stage_role inference (teacher-side KD + caching patterns)\n",
    "# -------------------------------------------------------------------------\n",
    "STAGE_ROLE_OVERRIDES: dict[str, str] = {\n",
    "    # Example manual overrides:\n",
    "    # \"logit_caching\": \"teacher_processing\",\n",
    "}\n",
    "\n",
    "STAGE_ROLE_RULES: list[tuple[str, list[str]]] = [\n",
    "    (\"teacher_generation\", [r\"synthetic.*generation\", r\"teacher.*generation\"]),\n",
    "    (\"teacher_processing\", [r\"logit\", r\"cache\", r\"teacher.*forward\", r\"teacher.*infer\", r\"distill.*teacher\"]),\n",
    "    (\"data_preprocess\", [r\"preprocess\", r\"tokeni[sz]e\", r\"build_.*dataset\", r\"dataset_.*prep\"]),\n",
    "    (\"evaluation\", [r\"\\beval\\b\", r\"gsm8k\", r\"mmlu\", r\"alpaca\", r\"ifeval\", r\"mt[-_]?bench\"]),\n",
    "]\n",
    "\n",
    "def infer_stage_role_v2(row: pd.Series) -> str:\n",
    "    name = _norm_key(row.get(\"stage_name\", \"\"))\n",
    "    pipeline = _norm_key(row.get(\"pipeline\", \"\"))\n",
    "    source = _norm_key(row.get(\"source\", \"\"))\n",
    "\n",
    "    if name in STAGE_ROLE_OVERRIDES:\n",
    "        return STAGE_ROLE_OVERRIDES[name]\n",
    "\n",
    "    for role, pats in STAGE_ROLE_RULES:\n",
    "        for pat in pats:\n",
    "            if re.search(pat, name):\n",
    "                return role\n",
    "\n",
    "    if pipeline in {\"sft\", \"kd\", \"dpo\", \"true_sft\", \"synthetic_sft\"}:\n",
    "        if source == \"summary\":\n",
    "            return \"train_summary\"\n",
    "        return \"student_train\"\n",
    "\n",
    "    if source == \"summary\":\n",
    "        return \"summary_only\"\n",
    "    return \"other\"\n",
    "\n",
    "def retag_stage_roles(stage_df: pd.DataFrame, *, inplace: bool = False) -> pd.DataFrame:\n",
    "    df = stage_df if inplace else stage_df.copy()\n",
    "    df[\"stage_role\"] = df.apply(infer_stage_role_v2, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------\n",
    "# run/cell -> core grid (9-row style)\n",
    "# --------------------------\n",
    "def build_core_grid(\n",
    "    cell_metrics_df: pd.DataFrame,\n",
    "    primary_dataset: str | None = None,\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Collapse run-level rows into one row per (pipeline, student_size, dataset_choice).\n",
    "    This is what you use for the 3×3 table.\n",
    "    \"\"\"\n",
    "    df = cell_metrics_df.copy()\n",
    "\n",
    "    if primary_dataset is not None and \"dataset_choice\" in df.columns:\n",
    "        before = len(df)\n",
    "        df = df[df[\"dataset_choice\"].astype(str).str.lower() == str(primary_dataset).lower()]\n",
    "        if verbose:\n",
    "            print(f\"[build_core_grid] dataset_choice={primary_dataset}: {before} -> {len(df)} rows\")\n",
    "\n",
    "    group_cols = [c for c in [\"pipeline\",\"student_size\",\"dataset_choice\"] if c in df.columns]\n",
    "    grouped = df.groupby(group_cols, dropna=False)\n",
    "\n",
    "    out = []\n",
    "    for key, rows in grouped:\n",
    "        if not isinstance(key, tuple):\n",
    "            key = (key,)\n",
    "        rec = dict(zip(group_cols, key))\n",
    "\n",
    "        tokens = rows[\"student_tokens\"].sum()\n",
    "        dur_s  = rows[\"student_duration_s\"].sum()\n",
    "\n",
    "        # Sum energies across runs\n",
    "        rec[\"runs_in_cell\"] = len(rows)\n",
    "        rec[\"total_student_tokens\"] = tokens\n",
    "        rec[\"total_student_duration_s\"] = dur_s\n",
    "\n",
    "        rec[\"student_kwh\"] = rows[\"student_kwh\"].sum()\n",
    "        rec[\"teacher_kwh\"] = rows[\"teacher_kwh\"].sum()\n",
    "        rec[\"eval_kwh\"]    = rows[\"eval_kwh\"].sum()\n",
    "        rec[\"other_kwh\"]   = rows[\"other_kwh\"].sum()\n",
    "        rec[\"total_kwh_all\"] = rows[\"total_kwh_all\"].sum()\n",
    "\n",
    "        rec[\"student_energy_joules\"] = rows[\"student_energy_joules\"].sum()\n",
    "        rec[\"teacher_energy_joules\"] = rows[\"teacher_energy_joules\"].sum()\n",
    "        rec[\"eval_energy_joules\"]    = rows[\"eval_energy_joules\"].sum()\n",
    "        rec[\"other_energy_joules\"]   = rows[\"other_energy_joules\"].sum()\n",
    "        rec[\"total_energy_joules_all\"] = rows[\"total_energy_joules_all\"].sum()\n",
    "\n",
    "        rec[\"student_gpu_kwh\"] = rows[\"student_gpu_kwh\"].sum()\n",
    "        rec[\"student_cpu_kwh\"] = rows[\"student_cpu_kwh\"].sum()\n",
    "\n",
    "        # Recompute derived metrics at the aggregated level\n",
    "        tps = _safe_div(tokens, dur_s)\n",
    "        jpt_total = _safe_div(rec[\"total_energy_joules_all\"], tokens)\n",
    "        jpt_student = _safe_div(rec[\"student_energy_joules\"], tokens)\n",
    "\n",
    "        # Provide both naming styles (plain + _agg)\n",
    "        rec[\"tokens_per_sec_student\"] = tps\n",
    "        rec[\"tokens_per_sec_student_agg\"] = tps\n",
    "\n",
    "        rec[\"energy_j_per_token_total\"] = jpt_total\n",
    "        rec[\"energy_j_per_token_total_agg\"] = jpt_total\n",
    "\n",
    "        rec[\"energy_j_per_token_student\"] = jpt_student\n",
    "        rec[\"energy_j_per_token_student_agg\"] = jpt_student\n",
    "\n",
    "        out.append(rec)\n",
    "\n",
    "    grid = pd.DataFrame(out)\n",
    "    if verbose:\n",
    "        print(f\"[build_core_grid] rows={len(grid)} groups={group_cols}\")\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Patch C: optional allocation of shared teacher stages to multiple student cells (model amortization)\n",
    "# -------------------------------------------------------------------------\n",
    "def allocate_shared_teacher_costs(\n",
    "    stage_df: pd.DataFrame,\n",
    "    allocation_rules: list[dict],\n",
    "    *,\n",
    "    energy_cols: Iterable[str] = (\n",
    "        \"energy_kwh\",\n",
    "        \"energy_joules_eff\",\n",
    "        \"gpu_energy_kwh\",\n",
    "        \"cpu_energy_kwh\",\n",
    "        \"duration_seconds\",\n",
    "        \"tokens_processed\",\n",
    "    ),\n",
    ") -> pd.DataFrame:\n",
    "    if not allocation_rules:\n",
    "        return stage_df\n",
    "\n",
    "    base = stage_df.copy()\n",
    "    if \"stage_role\" not in base.columns:\n",
    "        raise ValueError(\"allocate_shared_teacher_costs expects 'stage_role' (run retag_stage_roles first).\")\n",
    "\n",
    "    allocated_rows = []\n",
    "    for rule in allocation_rules:\n",
    "        pat = rule.get(\"match_stage_name_regex\")\n",
    "        if not pat:\n",
    "            continue\n",
    "\n",
    "        role_match = rule.get(\"match_stage_role\")\n",
    "        target_pipes = rule.get(\"target_pipelines\", [])\n",
    "        target_sizes = rule.get(\"target_student_sizes\", [])\n",
    "        target_ds = rule.get(\"target_dataset_choice\", None)\n",
    "        mode = rule.get(\"mode\", \"amortize\")\n",
    "        weight_override = rule.get(\"weight\", None)\n",
    "\n",
    "        mask = base[\"stage_name\"].astype(str).str.lower().str.contains(pat.lower(), regex=True, na=False)\n",
    "        if role_match is not None:\n",
    "            mask = mask & (base[\"stage_role\"] == role_match)\n",
    "\n",
    "        matched = base[mask].copy()\n",
    "        if matched.empty:\n",
    "            print(f\"[allocate_shared_teacher_costs] No matches for rule pattern: {pat!r}\")\n",
    "            continue\n",
    "\n",
    "        n_targets = max(1, len(target_pipes) * len(target_sizes))\n",
    "        if weight_override is not None:\n",
    "            weight = float(weight_override)\n",
    "        else:\n",
    "            weight = 1.0 if mode == \"full\" else 1.0 / float(n_targets)\n",
    "\n",
    "        for _, r in matched.iterrows():\n",
    "            for p in target_pipes:\n",
    "                for s in target_sizes:\n",
    "                    rr = r.copy()\n",
    "                    rr[\"pipeline\"] = p\n",
    "                    rr[\"student_size\"] = s\n",
    "                    if target_ds is not None:\n",
    "                        rr[\"dataset_choice\"] = target_ds\n",
    "                    rr[\"allocation_weight\"] = weight\n",
    "                    rr[\"allocated_from_stage_name\"] = r.get(\"stage_name\", None)\n",
    "                    rr[\"is_allocated\"] = True\n",
    "\n",
    "                    for c in energy_cols:\n",
    "                        if c in rr.index and pd.notna(rr[c]):\n",
    "                            rr[c] = rr[c] * weight\n",
    "\n",
    "                    allocated_rows.append(rr)\n",
    "\n",
    "        print(\n",
    "            f\"[allocate_shared_teacher_costs] Matched {len(matched)} stage rows for {pat!r}; \"\n",
    "            f\"created {len(matched) * n_targets} allocated rows (mode={mode}, weight={weight:g}).\"\n",
    "        )\n",
    "\n",
    "    if not allocated_rows:\n",
    "        return base\n",
    "\n",
    "    alloc_df = pd.DataFrame(allocated_rows)\n",
    "    out = pd.concat([base, alloc_df], ignore_index=True)\n",
    "    out[\"is_allocated\"] = out.get(\"is_allocated\", False).fillna(False)\n",
    "    return out\n",
    "\n",
    "def rebuild_aggregates(stage_df: pd.DataFrame, *, teacher_alloc_rules: list[dict] | None = None):\n",
    "    \"\"\"\n",
    "    stage_df -> retag roles -> optional teacher allocation -> cell_metrics_df -> core_grid_df\n",
    "    Returns: (stage_df2, cell_metrics_df2, core_grid_df2)\n",
    "    \"\"\"\n",
    "    df2 = retag_stage_roles(stage_df, inplace=False)\n",
    "    if teacher_alloc_rules:\n",
    "        df2 = allocate_shared_teacher_costs(df2, teacher_alloc_rules)\n",
    "\n",
    "    cell_df2 = aggregate_cell_metrics(df2)\n",
    "    core_df2 = build_core_grid(cell_df2, primary_dataset=None, verbose=False)\n",
    "    return df2, cell_df2, core_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0661d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Patch D: nicer totals + fractions (train-only vs all)\n",
    "# -------------------------------------------------------------------------\n",
    "def add_total_variants(core_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = core_df.copy()\n",
    "    for col in [\"student_kwh\", \"teacher_kwh\", \"eval_kwh\", \"other_kwh\", \"total_kwh_all\"]:\n",
    "        if col not in out.columns:\n",
    "            out[col] = np.nan\n",
    "\n",
    "    out[\"total_kwh_train_only\"] = out[\"student_kwh\"] + out[\"teacher_kwh\"]\n",
    "    out[\"teacher_frac_train_only\"] = out[\"teacher_kwh\"] / out[\"total_kwh_train_only\"].replace({0: np.nan})\n",
    "    out[\"eval_frac_of_total\"] = out[\"eval_kwh\"] / out[\"total_kwh_all\"].replace({0: np.nan})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d667a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Patch E: improved Pareto plot with actual student-size colors + legend\n",
    "# -------------------------------------------------------------------------\n",
    "PIPELINE_DISPLAY = {\"sft\": \"SFT\", \"kd\": \"KD\", \"true_sft\": \"Synthetic SFT\", \"synthetic_sft\": \"Synthetic SFT\"}\n",
    "PIPELINE_MARKERS = {\"sft\": \"o\", \"kd\": \"s\", \"true_sft\": \"D\", \"synthetic_sft\": \"D\"}\n",
    "\n",
    "def plot_energy_quality_pareto_v2(\n",
    "    df: pd.DataFrame,\n",
    "    energy_col: str,\n",
    "    quality_col: str,\n",
    "    *,\n",
    "    title: str | None = None,\n",
    "    pipeline_col: str = \"pipeline\",\n",
    "    student_size_col: str = \"student_size\",\n",
    "    label_col: str | None = None,\n",
    "    savepath: str | Path | None = None,\n",
    "    figsize=(7, 5),\n",
    "    annotate_pareto: bool = False,\n",
    "):\n",
    "    required = [energy_col, quality_col, \"pareto_optimal\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[plot_energy_quality_pareto_v2] Missing columns: {missing}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    student_vals = sorted(df[student_size_col].dropna().unique(), key=lambda x: _norm_key(x))\n",
    "    palette = plt.rcParams[\"axes.prop_cycle\"].by_key().get(\"color\", []) or [\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\"]\n",
    "    color_map = {s: palette[i % len(palette)] for i, s in enumerate(student_vals)}\n",
    "\n",
    "    for (p, s), sub in df.groupby([pipeline_col, student_size_col], dropna=False):\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        marker = PIPELINE_MARKERS.get(_norm_key(p), \"o\")\n",
    "        color = color_map.get(s, None)\n",
    "\n",
    "        non_p = sub[~sub[\"pareto_optimal\"].fillna(False)]\n",
    "        par = sub[sub[\"pareto_optimal\"].fillna(False)]\n",
    "\n",
    "        ax.scatter(non_p[energy_col], non_p[quality_col], marker=marker, c=color, s=40, alpha=0.45, edgecolors=\"none\")\n",
    "        ax.scatter(\n",
    "            par[energy_col], par[quality_col],\n",
    "            marker=marker, c=color, s=80, alpha=0.95,\n",
    "            edgecolors=\"black\", linewidths=1.2,\n",
    "            label=f\"{PIPELINE_DISPLAY.get(_norm_key(p), p)} • {s}\",\n",
    "        )\n",
    "\n",
    "        if annotate_pareto and label_col and label_col in par.columns:\n",
    "            for _, r in par.iterrows():\n",
    "                ax.annotate(str(r[label_col]), (r[energy_col], r[quality_col]), xytext=(4,4),\n",
    "                            textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(energy_col)\n",
    "    ax.set_ylabel(quality_col)\n",
    "    ax.set_title(title or f\"Energy vs Quality ({energy_col} vs {quality_col})\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if labels:\n",
    "        ax.legend(loc=\"best\", fontsize=8, frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savepath is not None:\n",
    "        savepath = Path(savepath)\n",
    "        savepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(savepath, bbox_inches=\"tight\")\n",
    "        print(f\"[plot_energy_quality_pareto_v2] Saved: {savepath}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Pareto frontier\n",
    "# --------------------------\n",
    "def mark_pareto_frontier(df: pd.DataFrame, energy_col: str, quality_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimization in energy_col, maximization in quality_col.\n",
    "    Marks pareto_optimal True/False.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"pareto_optimal\"] = False\n",
    "\n",
    "    sub = out[[energy_col, quality_col]].copy()\n",
    "    sub = sub.dropna()\n",
    "    idxs = sub.index.to_list()\n",
    "\n",
    "    e = sub[energy_col].to_numpy()\n",
    "    q = sub[quality_col].to_numpy()\n",
    "\n",
    "    pareto = np.ones(len(sub), dtype=bool)\n",
    "    for i in range(len(sub)):\n",
    "        if not pareto[i]:\n",
    "            continue\n",
    "        dominates = (e <= e[i]) & (q >= q[i]) & ((e < e[i]) | (q > q[i]))\n",
    "        if np.any(dominates):\n",
    "            pareto[i] = False\n",
    "\n",
    "    out.loc[idxs, \"pareto_optimal\"] = pareto\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda16cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Eval merge helper\n",
    "# --------------------------\n",
    "def merge_eval_metrics(base_df: pd.DataFrame, eval_df: pd.DataFrame, on=\"run_id\") -> pd.DataFrame:\n",
    "    if eval_df is None or len(eval_df) == 0:\n",
    "        return base_df\n",
    "    return base_df.merge(eval_df, on=on, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Patch F: headline 9-row grid builder + LaTeX/CSV export wrapper\n",
    "# -------------------------------------------------------------------------\n",
    "def make_headline_grid(\n",
    "    core_df: pd.DataFrame,\n",
    "    *,\n",
    "    dataset_choice: str | None = \"tulu\",\n",
    "    pipelines: list[str] | None = None,\n",
    "    student_sizes: list[str] | None = None,\n",
    "    include_eval: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    df = core_df.copy()\n",
    "    if dataset_choice is not None and \"dataset_choice\" in df.columns:\n",
    "        df = df[df[\"dataset_choice\"].astype(str).str.lower() == dataset_choice.lower()]\n",
    "\n",
    "    pipelines = pipelines or [\"sft\", \"kd\", \"true_sft\"]\n",
    "    student_sizes = student_sizes or [\"1B\", \"7B\", \"13B\"]\n",
    "    df = filter_cells(df, pipeline=pipelines, student_size=student_sizes, case_insensitive=True)\n",
    "\n",
    "    df = add_total_variants(df)\n",
    "    df[\"headline_total_kwh\"] = df[\"total_kwh_all\"] if include_eval else df[\"total_kwh_train_only\"]\n",
    "\n",
    "    want = [\n",
    "        \"pipeline\",\"student_size\",\"dataset_choice\",\n",
    "        \"total_student_tokens\",\n",
    "        \"headline_total_kwh\",\n",
    "        \"total_kwh_train_only\",\n",
    "        \"teacher_kwh\",\"teacher_frac_train_only\",\n",
    "        \"tokens_per_sec_student_agg\",\n",
    "        \"energy_j_per_token_total_agg\",\"energy_j_per_token_student_agg\",\n",
    "        # optional eval cols if merged\n",
    "        \"gsm8k_acc\",\"mmlu_acc\",\"alpacaeval_winrate\",\"ifeval_score\",\n",
    "    ]\n",
    "    want = [c for c in want if c in df.columns]\n",
    "    df = df[want].copy()\n",
    "\n",
    "    if \"pipeline\" in df.columns:\n",
    "        df[\"pipeline\"] = df[\"pipeline\"].map(lambda p: PIPELINE_DISPLAY.get(_norm_key(p), p))\n",
    "    return df\n",
    "\n",
    "def export_headline_tables(\n",
    "    core_df: pd.DataFrame,\n",
    "    *,\n",
    "    out_dir: str | Path = \"tables\",\n",
    "    dataset_choice: str | None = \"tulu\",\n",
    "    include_eval: bool = True,\n",
    "    basename: str = \"energy_grid\",\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tbl = make_headline_grid(core_df, dataset_choice=dataset_choice, include_eval=include_eval)\n",
    "\n",
    "    # save CSV\n",
    "    csv_path = out_dir / f\"{basename}.csv\"\n",
    "    tbl.to_csv(csv_path, index=False)\n",
    "    print(f\"[export_headline_tables] Wrote CSV: {csv_path}\")\n",
    "\n",
    "    # save LaTeX (uses your existing helper if present)\n",
    "    if \"export_table_to_latex_and_csv\" in globals():\n",
    "        export_table_to_latex_and_csv(\n",
    "            tbl,\n",
    "            out_path_prefix=str(out_dir / basename),\n",
    "            latex_caption=f\"Energy/throughput summary ({dataset_choice})\",\n",
    "            latex_label=f\"tab:{basename}\",\n",
    "            float_format=\"%.4g\",\n",
    "        )\n",
    "    else:\n",
    "        tex_path = out_dir / f\"{basename}.tex\"\n",
    "        tex_path.write_text(tbl.to_latex(index=False, float_format=\"%.4g\"), encoding=\"utf-8\")\n",
    "        print(f\"[export_headline_tables] Wrote LaTeX: {tex_path}\")\n",
    "\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Optional convenience loaders\n",
    "# -------------------------------------------------------------------------\n",
    "def load_eval_metrics_csv(path: str | Path = \"eval_metrics.csv\") -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"[load_eval_metrics_csv] Not found: {path} (returning empty df)\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"[load_eval_metrics_csv] Loaded {len(df)} rows from {path}\")\n",
    "    return df\n",
    "\n",
    "def load_overrides_csv(path: str | Path = \"overrides.csv\") -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"[load_overrides_csv] Not found: {path} (returning empty df)\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"[load_overrides_csv] Loaded {len(df)} rows from {path}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35834165",
   "metadata": {},
   "source": [
    "---\n",
    "## Example Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdef976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: amortize synthetic tulu generation across 3 student sizes for Synthetic SFT\n",
    "teacher_alloc_rules = [\n",
    "    dict(\n",
    "        match_stage_name_regex=r\"synthetic_tulu_generation\",\n",
    "        match_stage_role=\"teacher_generation\",\n",
    "        target_pipelines=[\"true_sft\"],\n",
    "        target_student_sizes=[\"1B\", \"7B\", \"13B\"],\n",
    "        target_dataset_choice=\"tulu\",\n",
    "        mode=\"amortize\",  # change to \"full\" for worst-case accounting\n",
    "    )\n",
    "]\n",
    "\n",
    "stage_df_v2, cell_metrics_df_v2, core_grid_df_v2 = rebuild_aggregates(\n",
    "    stage_df_all,\n",
    "    teacher_alloc_rules=teacher_alloc_rules,\n",
    ")\n",
    "\n",
    "core_grid_df_v2 = add_total_variants(core_grid_df_v2)\n",
    "display(core_grid_df_v2.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge evaluation metrics (once I have them)\n",
    "eval_df = load_eval_metrics_csv(\"eval_metrics.csv\")\n",
    "\n",
    "# Option A: merge on run_id (best if eval is per-run)\n",
    "cell_with_eval = merge_eval_metrics(cell_metrics_df_v2, eval_df, on=\"run_id\")\n",
    "\n",
    "# Option B: merge on pipeline/student_size/dataset_choice (if eval is per cell)\n",
    "# cell_with_eval = merge_eval_metrics(core_grid_df_v2, eval_df, on=[\"pipeline\",\"student_size\",\"dataset_choice\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29748a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the headline 9-row table (Latex + CSV)\n",
    "headline_tbl = export_headline_tables(\n",
    "    core_grid_df_v2,\n",
    "    out_dir=\"tables\",\n",
    "    dataset_choice=\"tulu\",   # swap to \"math\", \"codeforces\", etc.\n",
    "    include_eval=True,\n",
    "    basename=\"energy_grid_tulu\",\n",
    ")\n",
    "\n",
    "display(headline_tbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Frontier\n",
    "# Mark pareto points (run-level or cell-level)\n",
    "pareto_df = mark_pareto_frontier(\n",
    "    cell_with_eval,  # or core_grid_df_v2 if it has quality metrics\n",
    "    energy_col=\"total_kwh_all\",\n",
    "    quality_col=\"gsm8k_acc\",\n",
    ")\n",
    "\n",
    "plot_energy_quality_pareto_v2(\n",
    "    pareto_df,\n",
    "    energy_col=\"total_kwh_all\",\n",
    "    quality_col=\"gsm8k_acc\",\n",
    "    title=\"GSM8K: Energy vs Quality (Pareto highlighted)\",\n",
    "    label_col=\"run_id\",  # optional\n",
    "    savepath=\"figures/pareto_gsm8k_kwh.pdf\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
